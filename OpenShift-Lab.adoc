= Manage your Container with Red Hat OpenShift 
Dennis Deitermann <dennis@redhat.com>, Daniel Messer <dmesser@redhat.com>, Lutz Lange <llange@redhat.com>
:iconsdir: /etc/asciidoc/images/icons
:numbered:
:icons:
:toc: left
:tabsize: 8

== OpenShift Introduction Lab

=== What is OpenShift ?

OpenShift Online is Red Hat’s public cloud application development and hosting platform that automates the provisioning, management and scaling of applications so that you can focus on writing the code for your business, startup, or big idea.

Here is a Videos explaining OpenShift: 
https://youtu.be/D_Lj0rObunI[Introduction into OpenShift] +
//https://www.youtube.com/watch?v=aZ40GobvA1c[What is PaaS?]

Official documentation for https://docs.openshift.com/container-platform/3.5/welcome/index.html[OpenShift Container Platform]

==== Overview

OpenShift v3 is a layered system designed to expose underlying Docker-formatted container image and Kubernetes concepts as accurately as possible, with a focus on easy composition of applications by a developer. For example, install Ruby, push code, and add MySQL.

Unlike OpenShift v2, more flexibility of configuration is exposed after creation in all aspects of the model. The concept of an application as a separate object is removed in favor of more flexible composition of "services", allowing two web containers to reuse a database or expose a database directly to the edge of the network.

==== What are the Layers?

The Docker service provides the abstraction for packaging and creating Linux-based, lightweight container images. Kubernetes provides the cluster management and orchestrates containers on multiple hosts.

OpenShift Container Platform adds:

* Source code management, builds, and deployments for developers

* Managing and promoting images at scale as they flow through your system

* Application management at scale

* Team and user tracking for organizing a large developer organization

image::http://www.rhpet.de/pictures/OpenShift-Architecture.png[OpenShift Architecture]

==== Core Concepts

The following topics provide high-level, architectural information on core concepts and objects you will encounter when using OpenShift Container Platform. Many of these objects come from Kubernetes, which is extended by OpenShift Container Platform to provide a more feature-rich development lifecycle platform.

* https://docs.openshift.com/container-platform/3.5/architecture/core_concepts/containers_and_images.html#architecture-core-concepts-containers-and-images[Containers and images] are the building blocks for deploying your applications.

* https://docs.openshift.com/container-platform/3.5/architecture/core_concepts/pods_and_services.html[Pods and services] allow for containers to communicate with each other and proxy connections.

* https://docs.openshift.com/container-platform/3.5/architecture/core_concepts/projects_and_users.html[Projects and users] provide the space and means for communities to organize and manage their content together.

* https://docs.openshift.com/container-platform/3.5/architecture/core_concepts/builds_and_image_streams.html[Builds and image streams] allow you to build working images and react to new images.

* https://docs.openshift.com/container-platform/3.5/architecture/core_concepts/deployments.html[Deployments] add expanded support for the software development and deployment lifecycle.

* https://docs.openshift.com/container-platform/3.5/architecture/core_concepts/routes.html[Routes] announce your service to the world.

* https://docs.openshift.com/container-platform/3.5/architecture/core_concepts/templates.html[Templates] allow for many objects to be created at once based on customized parameters.

Click on the links above to get more information about the respective topic.

=== How to access the Lab Environment

First login to the ssh gateway with the user `rhpet`:

To obtain you gateway ip adress https://www.opentlc.com/pc-status/index.php[login here].

Login into the status page with your user `luXX` and your password `Frank_XX_furt`. Please use the lab number on http://192.168.103.200/dokuwiki/doku.php[this side] in the lower left as `XX`.

----
[user@localhost ~]$ ssh rhpet@<your gateway VM ip adress>

The authenticity of host '85.190.180.158 (85.190.180.158)' can't be established.
ECDSA key fingerprint is SHA256:bsDGeuXiG1zpM3RlsN+RlaAPRaDSi6Y/sJoBP2IXNqU.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '85.190.180.158' (ECDSA) to the list of known hosts.
rhpet@85.190.180.158's password:
Last login: Sun Mar 13 16:28:23 2016 from ipbcc3d2c2.dynamic.kabel-deutschland.de
[rhpet@gw ~]$
----

You will find the password for the rhpet user in the http://192.168.103.200/dokuwiki/doku.php?id=managing_rhv_with_ansible_playground[Lab description in the Dokuwikki].

Then get the power of root:
----
[rhpet@gw ~]$ su -
----
The root pw is `r3dh4t1!`

For HTTP & HTTPS connections we need to configure a Proxy in your Webbrowser. We tested it with Firefox.
Please go to `Settings` → `Advanced` → `Network` → `Settings`

image::http://www.rhpet.de/pictures/Firefox-Proxy.png[Firefox Proxy configuration]

Please use your gateway IP adress, port 80 and check the checkbox at "Use this proxy server for all protocols".

Please add `192.168.103.200` to the `No Proxy for:` field. When you don't do this, you cannot reach the Dokuwiki anymore.

Username for the Proxy is: `admin` +
Password for the Proxy is: `r3dh4t1!`


=== Lab Reference

[cols="3*", options="header"]
|===
| VM Name| FQDN | IP
| SSH Gateway, DNS & Proxy Server | gw.example.com | 192.168.0.250, Ports 22&80 are open
| Master | master.example.com | 192.168.0.100, Port 8443 is open
| Infranode | infranode.example.com | 192.168.0.101
| App Node 1 | node1.example.com | 192.168.0.102
| App Node 2 | node2.example.com | 192.168.0.103
| App Node 3 | node3.example.com | 192.168.0.104
|===

[cols="3*", options="header"]
|===
| Name | Password | Role
| luXX | ask the instructor | ssh user to connect to the gateway VM
| root | r3dh4t1! | root user for all VMs
| admin | r3dh4t1! | OSCP & CloudForms Administrator & Auth user for the Proxy
| marina | r3dh4t1! | Developer/User
| andrew  | r3dh4t1! | Developer/User
|=== 

=== Introduction into the CLI of OpenShift

With the OpenShift Container Platform command line interface (CLI), you can create applications and manage OpenShift Container Platform projects from a terminal. The CLI is ideal in situations where you are:

* Working directly with project source code.

* Scripting OpenShift Container Platform operations.

* Restricted by bandwidth resources and cannot use the web console.

The CLI is available using the `oc` command:
----
$ oc <command>
----

==== Basic Setup and Login

The `oc login` command is the best way to initially set up the CLI, and it serves as the entry point for most users. The interactive flow helps you establish a session to an OpenShift Container Platform server with the provided credentials. The information is automatically saved in a CLI configuration file that is then used for subsequent commands.

Login into the master host and the login into OpenShift as admin user:
----
[root@gw ~]# ssh master
Last login: Thu Jun  8 10:10:12 2017 from 192.168.0.250
----
 
----
[root@master ~]# oc login https://master.example.com:8443

Authentication required for https://master.example.com:8443 (openshift)
Username: admin
Password: r3dh4t1!
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
    kube-system
    logging
    management-infra
    openshift
  * openshift-infra

Using project "openshift-infra".
----

You can log out of CLI using the `oc logout` command. But we don't do this now.

==== Projects

A project in OpenShift Container Platform contains multiple objects to make up a logical application.

Most oc commands run in the context of a project. The `oc login` selects a default project during initial setup to be used with subsequent commands. Use the following command to display the project currently in use:

----
[root@master ~]# oc project
----

If you have access to multiple projects, use the following syntax to switch to a particular project by specifying the project name:
----
[root@master ~]# oc project default

Now using project "default" on server "https://master.example.com:8443".
----

The `oc status` command shows a high level overview of the project currently in use, with its components and their relationships, as shown in the following example:
----
[root@master ~]# oc status

In project default on server https://master.example.com:8443

https://docker-registry-default.cloudapps.example.com (passthrough) to pod port 5000-tcp (svc/docker-registry)
  dc/docker-registry deploys docker.io/openshift3/ose-docker-registry:v3.5.5.8
    deployment #1 deployed 3 weeks ago - 1 pod

svc/kubernetes - 172.30.0.1 ports 443, 53->8053, 53->8053

https://registry-console-default.cloudapps.example.com (passthrough) to pod port registry-console (svc/registry-console)
  dc/registry-console deploys registry.access.redhat.com/openshift3/registry-console:3.5
    deployment #1 deployed 2 days ago - 1 pod

svc/router - 172.30.49.219 ports 80, 443, 1936
  dc/router deploys docker.io/openshift3/ose-haproxy-router:v3.5.5.8
    deployment #1 deployed 3 weeks ago - 1 pod
----

If you want to learn more about the `oc` command, please look at the following documentation: +
https://docs.openshift.com/container-platform/3.5/cli_reference/basic_cli_operations.html[Developer CLI Operations] +
https://docs.openshift.com/container-platform/3.5/cli_reference/admin_cli_operations.html[Administrator CLI Operations]

=== Verify Your OpenShift Environment

Login into the master host:
----
[root@gw ~]# ssh master
----

Make sure that oc is in the default project
----
[root@gw ~]# oc project default
Now using project "default" on server "https://master.example.com:8443".
----

Run oc get nodes to check the status of your hosts:
----
[root@master ~]# oc get nodes
NAME                    STATUS                     AGE
infranode.example.com   Ready                      28d
master.example.com      Ready,SchedulingDisabled   28d
node1.example.com       Ready                      28d
node2.example.com       Ready                      28d
node3.example.com       Ready                      28d
----

Check if the installer has deployed the router and the registry containers:
----
[root@master ~]# oc get pods
NAME                       READY     STATUS    RESTARTS   AGE
docker-registry-1-26xs7    1/1       Running   9          28d
registry-console-1-tbwwj   1/1       Running   5          8d
router-1-xq3r6             1/1       Running   12         28d
----

Use your browser to connect to the OpenShift web console at https://master.example.com:8443/[https://master.example.com:8443/] and accept the untrusted Certificate.
Please don't login this time. We will do this a little bit later.

=== Configure OpenShift

In this section, you check the labels and do some intial configuration.

=== Pods

OpenShift leverages the Kubernetes concept of a pod, which is one or more containers deployed together on one host, and the smallest compute unit that can be defined, deployed, and managed.

Pods are the rough equivalent of OpenShift v2 gears, with containers the rough equivalent of v2 cartridge instances. Each pod is allocated its own internal IP address, therefore owning its entire port space, and containers within pods can share their local storage and networking.

Pods have a lifecycle; they are defined, then they are assigned to run on a node, then they run until their container(s) exit or they are removed for some other reason. Pods, depending on policy and exit code, may be removed after exiting, or may be retained in order to enable access to the logs of their containers.

OpenShift treats pods as largely immutable; changes cannot be made to a pod definition while it is running. OpenShift implements changes by terminating an existing pod and recreating it with modified configuration, base image(s), or both. Pods are also treated as expendable, and do not maintain state when recreated. Therefore pods should usually be managed by higher-level controllers, rather than directly by users.

=== Labels

Labels are used to organize, group, or select API objects. For example, pods are "tagged" with labels, and then services use label selectors to identify the pods they proxy to. This makes it possible for services to reference groups of pods, even treating pods with potentially different containers as related entities.

Most objects can include labels in their metadata. So labels can be used to group arbitrarily-related objects; for example, all of the pods, services, replication controllers, and deployment configurations of a particular application can be grouped.

Labels are simple key/value pairs, as in the following example:
----
labels:
  key1: value1
  key2: value2
----

Consider:

* A pod consisting of an *nginx* container, with the label *role=webserver*.

* A pod consisting of an *Apache httpd* container, with the same label *role=webserver*.

A service or replication controller that is defined to use pods with the *role=webserver* label treats both of these pods as part of the same group.

=== Set Regions and Zones

We have already labeled your nodes.

Check the labels of the nodes:
----
[root@master ~]# oc get nodes --show-labels
NAME                    STATUS                     AGE       LABELS
infranode.example.com   Ready                      22d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=infranode.example.com,region=infra,zone=infranodes
master.example.com      Ready,SchedulingDisabled   22d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master.example.com,region=master
node1.example.com       Ready                      22d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node1.example.com,region=primary,zone=east
node2.example.com       Ready                      22d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node2.example.com,region=primary,zone=west
node3.example.com       Ready                      22d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node3.example.com,region=primary,zone=north
----

You now have a running OpenShift environment across five hosts with one master and four nodes, divided into three regions: master, infra and primary.

Check that registry and router are running on the infranode:
----
[root@master ~]# oc get pods -o wide
NAME                       READY     STATUS    RESTARTS   AGE       IP              NODE
docker-registry-1-26xs7    1/1       Running   5          22d       10.128.0.10     infranode.example.com
registry-console-1-tbwwj   1/1       Running   1          2d        10.128.0.11     infranode.example.com
router-1-xq3r6             1/1       Running   7          22d       192.168.0.101   infranode.example.com
----

As you can see, all infrastructure pods are running on the infranode, because we configured a default node selector for this.
Please look https://blog.openshift.com/deploying-applications-to-specific-nodes/[here] for more information.

=== [ Optional ] Check Registry

In this lab scenario, infranode is the target for both the registry and the default router.

To check the URL of the docker registry run `oc status`:
----
[root@master ~]# oc status
In project default on server https://master.example.com:8443

https://docker-registry-default.cloudapps.example.com (passthrough) to pod port 5000-tcp (svc/docker-registry)
  dc/docker-registry deploys docker.io/openshift3/ose-docker-registry:v3.5.5.8 
    deployment #1 deployed 3 weeks ago - 1 pod

svc/kubernetes - 172.30.0.1 ports 443, 53->8053, 53->8053

https://registry-console-default.cloudapps.example.com (passthrough) to pod port registry-console (svc/registry-console)
  dc/registry-console deploys registry.access.redhat.com/openshift3/registry-console:3.5 
    deployment #1 deployed 2 days ago - 1 pod

svc/router - 172.30.49.219 ports 80, 443, 1936
  dc/router deploys docker.io/openshift3/ose-haproxy-router:v3.5.5.8 
    deployment #1 deployed 3 weeks ago - 1 pod
----

Test the status of the registry with the curl command to communicate with the registry’s service port, `curl -v https://registry-console-default.cloudapps.example.com --insecure`.
----
[root@master ~]# curl -v https://registry-console-default.cloudapps.example.com --insecure | grep "Red Hat Container Registry"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* About to connect() to registry-console-default.cloudapps.example.com port 443 (#0)
*   Trying 192.168.0.101...
* Connected to registry-console-default.cloudapps.example.com (192.168.0.101) port 443 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
* skipping SSL peer certificate verification
* SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
* Server certificate:
* 	subject: CN=registry-console-1-tbwwj
* 	start date: Jun 08 11:03:26 2017 GMT
* 	expire date: Mai 15 11:03:27 2117 GMT
* 	common name: registry-console-1-tbwwj
* 	issuer: CN=registry-console-1-tbwwj
> GET / HTTP/1.1
> User-Agent: curl/7.29.0
> Host: registry-console-default.cloudapps.example.com
> Accept: */*
> 
< HTTP/1.1 200 OK
< Content-Security-Policy: default-src 'self' 'unsafe-inline'; connect-src 'self' ws: wss:
< Transfer-Encoding: chunked
< Cache-Control: no-cache, no-store
< 
{ [data not shown]
var environment = {"page":{"title":"Red Hat Container Registry","connect":true},"hostname":"registry-console-1-tbwwj","os-release":{"NAME":"Red Hat Container Registry","ID":"registry","PRETTY_NAME":"Red Hat Container Registry"},"OAuth":{"URL":"https://master.example.com:8443//oauth/authorize?client_id=cockpit-oauth-client&response_type=token","ErrorParam":null,"TokenParam":null}};
100 42229    0 42229    0     0   212k      0 --:--:-- --:--:-- --:--:--  213k
* Connection #0 to host registry-console-default.cloudapps.example.com left intact
----

==== Set Regions and Zones

We have already labeled your nodes.

Check the labels of the nodes:
----
[root@master ~]# oc get nodes --show-labels
infranode.example.com   Ready                      28d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=infranode.example.com,region=infra,zone=infranodes
master.example.com      Ready,SchedulingDisabled   28d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master.example.com,region=master
node1.example.com       Ready                      28d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node1.example.com,region=primary,zone=east
node2.example.com       Ready                      28d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node2.example.com,region=primary,zone=west
node3.example.com       Ready                      28d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node3.example.com,region=primary,zone=north
----

You now have a running OpenShift Container Platform environment across four hosts with one master and three nodes, divided into two regions: infra and primary.

Check that registry, registry-console and router are running on the infranode:
----
[root@master ~]# oc get pods -o wide -n default
NAME                       READY     STATUS    RESTARTS   AGE       IP              NODE
docker-registry-1-26xs7    1/1       Running   9          28d       10.128.0.19     infranode.example.com
registry-console-1-tbwwj   1/1       Running   5          8d        10.128.0.18     infranode.example.com
router-1-xq3r6             1/1       Running   12         28d       192.168.0.101   infranode.example.com
----

As you can see, all infrastructure pods are running on the infranode, because we configured a default node selector for this.
Please look https://blog.openshift.com/deploying-applications-to-specific-nodes/[here] for more information.

=== [ Optional ] Resource Management Lab

In this lab, you learn how to manage OpenShift Container Platform resources.

* *Manage Users, Projects, and Quotas*
+
In this section, you create projects and test the use of quotas and limits.

* *Create Services and Routes*
+
In this section, you manually create services and routes for pods and review the changes to a service when scaling an application.

* *Explore Containers*
+
In this section, you run commands within active pods and explore the `docker-registry` and `Default Router` containers.

// This setion is missing below 
//* *Create Persistent Volume for Registry*
// In this section, you create a persistent volume for your registry, attach it to `deploymentConfiguration`, and redeploy the registry.

==== Manage Users, Projects, and Quotas

===== Create Project

On the master host, run `oadm` to create and assign the administrative user `andrew` to a project:

----
[root@master ~]# oadm new-project resourcemanagement --display-name="Resources Management" \
--description="This is the project we use to learn about resource management" \
--admin=andrew  --node-selector='region=primary'
----

[NOTE]
`andrew` can create his own project with the `oc new-project` command, an option you will experiment with later in this course. Note that defining the `--node-selector` is optional.

==== View Resources in Web Console

Now have a look at the web console.

. Open your web browser and go to https://master.example.com:8443[https://master.example.com:8443]
+
[NOTE]
====
The web console could take up to 90 seconds to become available after a restart of the master.
====

. When prompted, type the username and password, as follows:
** *Username*: `andrew`
** *Password*: `r3dh4t1!`

. In the web console, click the *Resources Management* project.
+
[NOTE]
The project is empty because it has no apps. You change that as part of this lab. 
+
[NOTE]
.An error occured getting metrics
====
We are using self signed certificates here, this is why your browser can't contact our metrics stack. Click "OpenMetricsURL and accept the certificate in your Browser or ignore the error for now.
====

===== Apply Quota to Project

A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per project. It can limit the quantity of objects that can be created in a project by type, as well as the total amount of compute resources and storage that may be consumed by resources in that project.

. Create a quota definition file:
+
----
[root@master ~]# cat << EOF > quota.json
{
  "apiVersion": "v1",
  "kind": "ResourceQuota",
  "metadata": {
    "name": "test-quota"
  },
  "spec": {
    "hard": {
      "memory": "512Mi",
      "cpu": "20",
      "pods": "3",
      "services": "5",
      "replicationcontrollers":"5",
      "resourcequotas":"1"
    }
  }
}
EOF
----

. On the master host, do the following:
.. Run `oc create` to apply the file you just created:
+
----
[root@master ~]# oc create -f quota.json --namespace=resourcemanagement
----

.. Verify that the quota exists:
+
----
[root@master ~]# oc get quota -n resourcemanagement 
NAME         AGE
test-quota   11s
----

.. Verify the limits and examine the usage:
+
[tabsize=8]
----
[root@master ~]# oc describe quota test-quota -n resourcemanagement
Name:			test-quota
Namespace:		resourcemanagement
Resource		Used	Hard
--------		----	----
cpu			0	20
memory			0	512Mi
pods			0	3
replicationcontrollers	0	5
resourcequotas		1	1
services		0	5
----
+

. On the web console, click the *Resource Management* project.

. Click the *Resources* tab

. Click *Quota* for information about the quota set.

==== Apply Limit Ranges to Project

For quotas to be effective, you must create _limit ranges_. They allocate the maximum, minimum, and default memory and CPU at both the pod and container level. Deployments to projects with a quota set will fail, if there are no default limits set for containers and pods. Pod and Containers with no limits are called unbound and are forbidden to run in quota projects.

. Create the `limits.json` file:
+
----
[root@master ~]# cat << EOF > limits.json
{
    "kind": "LimitRange",
    "apiVersion": "v1",
    "metadata": {
        "name": "limits",
        "creationTimestamp": null
    },
    "spec": {
        "limits": [
            {
                "type": "Pod",
                "max": {
                    "cpu": "500m",
                    "memory": "750Mi"
                },
                "min": {
                    "cpu": "10m",
                    "memory": "5Mi"
                }
            },
            {
                "type": "Container",
                "max": {
                    "cpu": "500m",
                    "memory": "750Mi"
                },
                "min": {
                    "cpu": "10m",
                    "memory": "5Mi"
                },
                "default": {
                    "cpu": "100m",
                    "memory": "100Mi"
                }
            }
        ]
    }
}
EOF
----

. On the master host, run `oc create` against the `limits.json` file and the
 `resourcemanagement` project:
+
----
[root@master ~]# oc create -f limits.json --namespace=resourcemanagement
----

. Review your limit ranges:
+
//image:Review-Ressource-limits.png[]
----
[root@master ~]# oc describe limitranges limits -n resourcemanagement
Name:		limits
Namespace:	resourcemanagement
Type		Resource	Min	Max	Default Request	Default Limit	Max Limit/Request Ratio
----		--------	---	---	---------------	-------------	-----------------------
Pod		cpu		10m	500m	-		-		-
Pod		memory		5Mi	750Mi	-		-		-
Container	cpu		10m	500m	100m		100m		-
Container	memory		5Mi	750Mi	100Mi		100Mi		-
----

==== Test Quota and Limit Settings

NOTE: You are running commands as the Linux users `andrew` and `root` in a lab environment. As a user it is unusual to use the `oc` command directly on the master. It is common to install `oc` on your workstation or notebook. You can get the Upstream OpenShift client tools for your operating system link:https://github.com/openshift/origin/releases/tag/v1.5.1[here]. And the OpenShift Client tools with support are found on the link:https://access.redhat.com/downloads/content/290[Red Hat Customer Portal].

The following lab will be done on the command line interface.

. Authenticate to OpenShift Container Platform and choose your project:

.. Connect to the shell of the OpenShift Container Platform master according to the procedure you followed
 previously.

.. When prompted, type the username and password:
** *Username*: `andrew`
** *Password*: `r3dh4t1!`
+
----
[root@master ~]# su - andrew
[andrew@master ~]$ oc login https://master.example.com:8443 -u andrew
----

* The output is as follows:
+
----
Login successful.

You have one project on this server: "resourcemanagement"

Using project "resourcemanagement".
Welcome! See 'oc help' to get started.
----
+
NOTE: This lab shows you the manual, step-by-step method of creating each object. This is done only for educational purpose. There are easier ways to create deployments and all the required objects. The most powerful way to create apps on OpenShift is the `oc new-app` command, which is covered later in this lab.

. Create the `hello-pod.json` pod definition file:
+
----
[andrew@master ~]$ cat <<EOF > hello-pod.json
{
  "kind": "Pod",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-openshift",
    "creationTimestamp": null,
    "labels": {
      "name": "hello-openshift"
    }
  },
  "spec": {
    "containers": [
      {
        "name": "hello-openshift",
        "image": "openshift/hello-openshift:v1.5.1",
        "ports": [
          {
            "containerPort": 8080,
            "protocol": "TCP"
          }
        ],
        "resources": {
        },
        "terminationMessagePath": "/dev/termination-log",
        "imagePullPolicy": "IfNotPresent",
        "capabilities": {},
        "securityContext": {
          "capabilities": {},
          "privileged": false
        }
      }
    ],
    "restartPolicy": "Always",
    "dnsPolicy": "ClusterFirst",
    "serviceAccount": ""
  },
  "status": {}
}
EOF
----

===== Run Pod

Here, you create a simple pod without a _route_ or _service_:

Create and verify the `hello-openshift` pod:
----
[andrew@master ~]$ oc create -f hello-pod.json

pod "hello-openshift" created
----
Wait a few seconds until the pod is up and running. (~40 seconds are needed)
----
[andrew@master ~]$ oc get pods

NAME              READY     STATUS    RESTARTS   AGE
hello-openshift   1/1       Running   0          41s
----

Run `oc describe` for details on your pod:
----
[andrew@master ~]$ oc describe pod hello-openshift

Name:			hello-openshift
Namespace:		resourcemanagement
Security Policy:	restricted
Node:			node2.example.com/192.168.0.103
Start Time:		Tue, 25 Apr 2017 19:15:01 -0400
Labels:			name=hello-openshift
Status:			Running
IP:			10.130.0.2
Controllers:		<none>
Containers:
  hello-openshift:
    Container ID:	docker://2674481be26d544323fa637c1cc5ba36a5eaafd4707f7735b2620045c495cb07
    Image:		openshift/hello-openshift:v1.5.1
    Image ID:		docker-pullable://docker.io/openshift/hello-openshift@sha256:7ce9d7b0c83a3abef41e0db590c5aa39fb05793315c60fd907f2c609997caf11
    Port:		8080/TCP
    Limits:
      cpu:	100m
      memory:	100Mi
    Requests:
      cpu:		100m
      memory:		100Mi
    State:		Running
      Started:		Tue, 25 Apr 2017 19:15:39 -0400
    Ready:		True
    Restart Count:	0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ylt00 (ro)
    Environment Variables:	<none>
Conditions:
  Type		Status
  Initialized 	True
  Ready 	True
  PodScheduled 	True
Volumes:
  default-token-ylt00:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	default-token-ylt00
QoS Class:	Guaranteed
Tolerations:	<none>
Events:
  FirstSeen	LastSeen	Count	From				SubobjectPath			Type		Reason		Message
  ---------	--------	-----	----				-------------			--------	------		-------
  2m		2m		1	{default-scheduler }						Normal		Scheduled	Successfully assigned hello-openshift to node2.example.com
  1m		1m		1	{kubelet node2.example.com}	spec.containers{hello-openshift}	Normal		Pulling		pulling image "openshift/hello-openshift:v1.5.1"
  1m		1m		1	{kubelet node2.example.com}	spec.containers{hello-openshift}	Normal		Pulled		Successfully pulled image "openshift/hello-openshift:v1.5.1"
  1m		1m		1	{kubelet node2.example.com}	spec.containers{hello-openshift}	Normal		Created		Created container with docker id 2674481be26d; Security:[seccomp=unconfined]
  1m		1m		1	{kubelet node2.example.com}	spec.containers{hello-openshift}	Normal		Started		Started container with docker id 2674481be26d
----

Test that your pod is responding with `Hello OpenShift`:

----
[andrew@master ~]$ ip=`oc describe pod hello-openshift|grep IP:|awk '{print $2}'`
[andrew@master ~]$ curl http://${ip}:8080
----

* This output denotes a correct response:
+
----
Hello OpenShift!
----

Delete all the objects in your `hello-pod.json` definition file, which, at this point, is the pod only:

----
[andrew@master ~]$ oc delete -f hello-pod.json
pod "hello-openshift" deleted
----

TIP: You can also delete a pod using the following command format: #oc delete pod <PODNAME>.

Create a new definition file that launches four `hello-openshift` pods:

----
[andrew@master ~]$ cat << EOF > hello-many-pods.json
{
  "metadata":{
    "name":"quota-pod-deployment-test"
  },
  "kind":"List",
  "apiVersion":"v1",
  "items":[
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-1",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.5.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-2",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.5.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-3",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.5.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-4",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.5.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    }
  ]
}
EOF
----

Create the items in the `hello-many-pods.json` file:

----
[andrew@master ~]$ oc create -f hello-many-pods.json

pod "hello-openshift-1" created
pod "hello-openshift-2" created
pod "hello-openshift-3" created
Error from server: pods "hello-openshift-4" is forbidden: Exceeded quota: test-quota, requested: pods=1, used: pods=3, limited: pods=3
----

[NOTE]
Because you defined a quota before, `oc create` created three pods only instead of four.

Delete the object in the `hello-many-pods.json` definition file:

----
[andrew@master ~]$ oc delete -f hello-many-pods.json
----

(Optional) Create a project, set the quota with a pod value of `10`, and run `hello-many-pods.json`.

==== [ Optional ] Create Services and Routes

As `andrew`, create a project called `scvslab`:

----

[andrew@master ~]$ oc new-project svcslab --display-name="Services Lab" --description="This is the project we use to learn about services"
----

The output looks like this:

----
Now using project "svcslab" on server "https://master.example.com:8443".

You can add applications to this project with the 'new-app' command. For example, try:

    $ oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-hello-world.git

to build a new hello-world application in Ruby.
----

TIP: To switch between projects, run `oc project _projectname_`.

Create the `hello-service.json` file:

----
[andrew@master ~]$ cat <<EOF > hello-service.json
{
  "kind": "Service",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-service",
    "labels": {
      "name": "hello-openshift"
    }
  },
  "spec": {
    "selector": {
      "name":"hello-openshift"
    },
    "ports": [
      {
        "protocol": "TCP",
        "port": 8888,
        "targetPort": 8080
      }
    ]
  }
}
EOF
----

Create the `hello-service` service:

----
[andrew@master ~]$ oc create -f hello-service.json

service "hello-service" created
----

Display the services that are running in the current project:

----
[andrew@master ~]$ oc get services

NAME            CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
hello-service   172.30.213.165   <none>        8888/TCP   5s
----

Examine the details of your service. Note the following:
** *Selector*: Describes which pods the service selects or lists.
** *Endpoints*: Displays all the pods that are currently listed (none in your current project).

----
[andrew@master ~]$ oc describe service hello-service

Name:			hello-service
Namespace:		svcslab
Labels:			name=hello-openshift
Selector:		name=hello-openshift
Type:			ClusterIP
IP:			172.30.213.165
Port:			<unset>	8888/TCP
Endpoints:		<none>
Session Affinity:	None
No events.
----

Create pods according to the `hello-many-pods.json` definition file:

----
[andrew@master ~]$ oc create -f hello-many-pods.json
----

Wait a few seconds and check the service again.

* The pods that share the label `name=hello-openshift` are all listed:

----
[andrew@master ~]$ oc describe service hello-service

Name:			hello-service
Namespace:		svcslab
Labels:			name=hello-openshift
Selector:		name=hello-openshift
Type:			ClusterIP
IP:			172.30.213.165
Port:			<unset>	8888/TCP
Endpoints:		10.1.2.2:8080,10.1.2.3:8080,10.1.3.2:8080 + 1 more...
Session Affinity:	None
No events.
----

Test that your service is working:

----

[andrew@master ~]$ ip=`oc describe service hello-service|grep IP:|awk '{print $2}'`
[andrew@master ~]$ curl http://${ip}:8888

Hello OpenShift!
----

==== Explore Containers and Routes

Next, take a look at the route and registry containers.

===== Create Applications As Examples

As `andrew`, create a project called `explore-example`:
----
[andrew@master ~]$ oc new-project explore-example --display-name="Explore Example" --description="This is the project we use to learn about connecting to pods"
----

Applying the same image as before, run `oc new-app` to deploy `hello-openshift`:
----
[andrew@master ~]$ oc new-app --docker-image=openshift/hello-openshift:v1.5.1 -l "todelete=yes"

--> Found Docker image fb15b0b (4 weeks old) from Docker Hub for "openshift/hello-openshift:v1.5.1"

    * An image stream will be created as "hello-openshift:v1.5.1" that will track this image
    * This image will be deployed in deployment config "hello-openshift"
    * Ports 8080/tcp, 8888/tcp will be load balanced by service "hello-openshift"
      * Other containers can access this service through the hostname "hello-openshift"
    * WARNING: Image "openshift/hello-openshift:v1.5.1" runs as the 'root' user which may not be permitted by your cluster administrator

--> Creating resources with label todelete=yes ...
    imagestream "hello-openshift" created
    deploymentconfig "hello-openshift" created
    service "hello-openshift" created
--> Success
    Run 'oc status' to view your app.
----

Verify that `oc new-app` has created a pod and the service.

----
[andrew@master ~]$ oc get svc

NAME              CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
hello-openshift   172.30.24.220   <none>        8080/TCP,8888/TCP   37s
----

----
[andrew@master ~]$ oc get pods

NAME                      READY     STATUS    RESTARTS   AGE
hello-openshift-1-g3xow   1/1       Running   0          2m

----

Expose the service and create a route for the application:
----
[andrew@master ~]$ oc expose service hello-openshift --hostname=explore.cloudapps.example.com
----

Check if the route works fine:
----
[andrew@master ~]$ curl http://explore.cloudapps.example.com

Hello OpenShift!
----

In a later section, you explore the `docker-registry` container. To save time, start an S2I build now to push an image into the registry:

----
[andrew@master ~]$ oc new-app https://github.com/openshift/sinatra-example -l "todelete=yes"

--> Found image 27e89d9 (4 weeks old) in image stream "ruby" in project "openshift" under tag "2.3" for "ruby"

    Ruby 2.3
    --------
    Platform for building and running Ruby 2.3 applications

    Tags: builder, ruby, ruby23, rh-ruby23

    * The source repository appears to match: ruby
    * A source build using source code from https://github.com/openshift/sinatra-example will be created
      * The resulting image will be pushed to image stream "sinatra-example:latest"
    * This image will be deployed in deployment config "sinatra-example"
    * Port 8080/tcp will be load balanced by service "sinatra-example"
      * Other containers can access this service through the hostname "sinatra-example"

--> Creating resources with label todelete=yes ...
    imagestream "sinatra-example" created
    buildconfig "sinatra-example" created
    deploymentconfig "sinatra-example" created
    service "sinatra-example" created
--> Success
    Build scheduled, use 'oc logs -f bc/sinatra-example' to track its progress.
    Run 'oc status' to view your app.
----

===== Connect to Default Router Container

Get back to root:
----
[andrew@master ~]$ exit
----

. As `root`, make sure to use the default project. Open a Shell into the container with `oc rsh`
 command along with the default router's pod name.

----
[root@master ~]# oc project default

Now using project "default" on server "https://master.example.com:8443".
----

----
[root@master ~]# oc get pods

NAME                      READY     STATUS    RESTARTS   AGE
docker-registry-1-26xs7    1/1       Running   9          28d
registry-console-1-tbwwj   1/1       Running   5          8d
router-1-xq3r6             1/1       Running   12         28d
----

----
[root@master ~]# oc rsh router-1-xq3r6 
----

This prompt is displayed:
----
sh-4.2$ 
----

You are now running `bash` inside the container.

. Do the following:
.. Run `id`.
.. Run `pwd` and `ls` and note the directory you are in.
.. Run `grep hello-openshift` on the `haproxy.config` file.
.. Run `cat haproxy.config` to have a look on your configuration file.
+
----
sh-4.2$ id

uid=1000020000 gid=0(root) groups=0(root),1000020000
----
+
----
sh-4.2$ pwd

/var/lib/haproxy/conf
----
+
----
sh-4.2$ ls

cert_config.map		 os_edge_http_be.map	     os_sni_passthrough.map
default_pub_keys.pem	 os_http_be.map		     os_tcp_be.map
error-page-503.http	 os_reencrypt.map	     os_wildcard_domain.map
haproxy-config.template  os_route_http_expose.map
haproxy.config		 os_route_http_redirect.map
----
+
----
sh-4.2$ grep hello-openshift haproxy.config 

backend be_http_explore-example_hello-openshift

sh-4.2$ ps -ef
UID         PID   PPID  C STIME TTY          TIME CMD
1000020+      1      0  0 07:21 ?        00:00:21 /usr/bin/openshift-router
1000020+    726      1  0 10:58 ?        00:00:44 /usr/sbin/haproxy -f /var/lib/
1000020+   1230      1  1 14:29 ?        00:00:06 /usr/sbin/haproxy -f /var/lib/
1000020+   1263      0  0 14:34 ?        00:00:00 /bin/sh
1000020+   1279   1263  0 14:37 ?        00:00:00 ps -ef
----
.. Examine the haproxy.config more closely. This could look something like this like this:
+
[subs=+macros]
----
sh-4.2$ grep -A 40 hello-openshift haproxy.config | sed '/^ *$/d'

backend be_http_explore-example_hello-openshift
  mode http
  option redispatch
  option forwardfor
  balance leastconn
  timeout check 5000ms
  http-request set-header X-Forwarded-Host %[req.hdr(host)]
  http-request set-header X-Forwarded-Port %[dst_port]
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  cookie 7cf54b74789cba0ee0faded0db7f5e0f insert indirect nocache httponly
  http-request set-header Forwarded for=%[src];host=%[req.hdr(host)];proto=%[req.hdr(X-Forwarded-Proto)]
pass:quotes[  *server*] 456a8f857d60f0a14165ad58cff18e10 10.128.2.32:8080 check inter 5000ms cookie 456a8f857d60f0a14165ad58cff18e10 weight 100
----
+
You see that you have only one endpoint defined. (The line which starts with server)
+
.. Exit the bash in the container to return to the roo@master shell
+
----
sh-4.2$ exit

[root@master ~]# _
----
. As `andrew`, scale `hello-openshift` to have five replicas of its pod:
+
----
[root@master ~]# su - andrew
----
+
----
[andrew@master ~]$ oc get deploymentconfig

NAME              REVISION   REPLICAS   TRIGGERED BY
hello-openshift   1          1          config,image(hello-openshift:v1.5.1)
sinatra-example   1          1          config,image(sinatra-example:latest)
----
+
----
[andrew@master ~]$ oc scale dc hello-openshift --replicas=5

deploymentconfig "hello-openshift" scaled
----

. As `root` go back to the router container and view the `haproxy.config` file again:
+
[subs=+macros]
----
[andrew@master ~]$ exit
----
+
----
[root@master ~]# oc rhs router-1-xq3r6
----
+
----
sh-4.2$ grep -A 70 hello-openshift haproxy.config | sed '/^ *$/d'

backend be_http_explore-example_hello-openshift
  mode http
  option redispatch
  option forwardfor
  balance leastconn
  timeout check 5000ms
  http-request set-header X-Forwarded-Host %[req.hdr(host)]
  http-request set-header X-Forwarded-Port %[dst_port]
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  cookie 7cf54b74789cba0ee0faded0db7f5e0f insert indirect nocache httponly
  http-request set-header Forwarded for=%[src];host=%[req.hdr(host)];proto=%[req.hdr(X-Forwarded-Proto)]
pass:quotes[  *server* 456a8f857d60f0a14165ad58cff18e10 10.128.2.32:8080 check inter 5000ms cookie 456a8f857d60f0a14165ad58cff18e10 weight 100
  *server* 465c8af937146549fb2d68aa3adfde77 10.128.2.36:8080 check inter 5000ms cookie 465c8af937146549fb2d68aa3adfde77 weight 100
  *server* a19dc1b5f57a5cfe76f752ad8aa6c3a5 10.130.0.20:8080 check inter 5000ms cookie a19dc1b5f57a5cfe76f752ad8aa6c3a5 weight 100
  *server* 111eec0d645bb0897b3a9425563167b9 10.131.0.18:8080 check inter 5000ms cookie 111eec0d645bb0897b3a9425563167b9 weight 100
  *server*] aa8e80663b91a03be37ee9d33c3bc9c5 10.131.0.19:8080 check inter 5000ms cookie aa8e80663b91a03be37ee9d33c3bc9c5 weight 100
----

* All of your pods within the `haproxy` configuration are listed.

NOTE: Remember, the router routes proxy connections to the pods directly and not through the service. The router uses the service only to obtain a list of the pod endpoints (IP addresses).

==== Explore Registry Container

There is two containers that deal with registry related services. There is the docker-registry and there is the registry-console. We are looking at the docker-registry in this section. We will take a quick look at the link:https://registry-console-default.cloudapps.example.com[Registry-Console] at a later time.

Please ensure that your build from earlier is complete.

. As user `*andrew*`, check the logs of the build that we stared a while back:
+
----
[andrew@master ~]$ oc logs builds/sinatra-example-1

Cloning "https://github.com/openshift/sinatra-example" ...
	Commit:	ff65a82271fffc60d4129bccde9c42ded49a199d (Merge pull request #11 from corey112358/patch-1)
	Author:	Ben Parees <bparees@users.noreply.github.com>
	Date:	Wed Jul 22 00:20:36 2015 -0400

---> Installing application source ...
---> Building your Ruby application from source ...
---> Running 'bundle install --deployment --without development:test' ...
Fetching gem metadata from https://rubygems.org/..........
Fetching version metadata from https://rubygems.org/..
Installing rack 1.6.0
Installing rack-protection 1.5.3
Installing tilt 1.4.1
Installing sinatra 1.4.5
Using bundler 1.10.6
Bundle complete! 1 Gemfile dependency, 5 gems now installed.
Gems in the groups development and test were not installed.
Bundled gems are installed into ./bundle.
---> Cleaning up unused ruby gems ...


Pushing image 172.30.17.242:5000/explore-example/sinatra-example:latest ...
Pushed 0/5 layers, 3% complete
Pushed 1/5 layers, 24% complete
Pushed 2/5 layers, 43% complete
Pushed 3/5 layers, 75% complete
Pushed 3/5 layers, 98% complete
Pushed 4/5 layers, 98% complete
Pushed 5/5 layers, 100% complete
Push successful
----
+
Notice the last few lines here. The *Push successful* indicates that the new container image was put into your internal registry.
+
. As `root`, start a shell inside the Container Context by running `oc rsh` along with the `docker-registry` pod name:
+
----
[root@master ~]# oc rsh docker-registry-1-qbv9l
----

. Do the following:
.. Run `id`.
.. Run `pwd` and `ls` and note the directory you are in.
.. Run `cat config.yml`  to verify your configuration file.
+
----
sh-4.2$ id

uid=1000010000 gid=0(root) groups=0(root),1000010000
----
+
----
sh-4.2$ pwd

/
----
+
----
sh-4.2$ ls

bin   config.yml  etc	lib    media  opt   registry  run   srv  tmp  var
boot  dev	  home	lib64  mnt    proc  root      sbin  sys  usr
----
+
----
sh-4.2$ cat config.yml

version: 0.1
log:
  level: debug
http:
  addr: :5000
storage:
  cache:
    blobdescriptor: inmemory
  filesystem:
    rootdirectory: /registry
  delete:
    enabled: true
auth:
  openshift:
    realm: openshift

    # tokenrealm is a base URL to use for the token-granting registry endpoint.
    # If unspecified, the scheme and host for the token redirect are determined from the incoming request.
    # If specified, a scheme and host must be chosen that all registry clients can resolve and access:
    #
    # tokenrealm: https://example.com:5000
middleware:
  registry:
    - name: openshift
  repository:
    - name: openshift
      options:
        acceptschema2: false
        pullthrough: true
	mirrorpullthrough: true
        enforcequota: false
        projectcachettl: 1m
        blobrepositorycachettl: 10m
  storage:
    - name: openshift
----
+
. View the repositories and images that are available:
+
----
sh-4.2$ cd /registry/docker/registry/v2/repositories
----
+
----
sh-4.2$ ls

explore-example
----
+
----
sh-4.2$ ls explore-example/sinatra-example/_layers/

sha256
----
+
----
sh-4.2$ ls explore-example/sinatra-example/_layers/sha256/

02cbff0982e427fee158df11d35632f38410ee7e8b48212e681ecf3e60660ce4
5a865e48f2fdb4c48700b9aa800ecd8d0aff8611bec51fb4ab0f70ba09a0fb8e
89af3ab0c8b470502e9ed73ce6fa83f97e89a033f2553e9ba4e8a153c52a6373
9cc048a8a74a05eabd2f114d56d759435b8e2d76091e40edbff1d137b08de613
a778b52f148e84ec73f4ad7f7a1e67690dd0a36ddf1ed2926ad223901d196bf7
d65e4475a277c626c504de9433b98c30350e4cb940feb858b8563a6031e809a5
----
+
. As user `andrew`, look at one of the pods you started earlier:
+
----
[andrew@master ~]$ oc get pods

NAME                      READY     STATUS      RESTARTS   AGE
hello-openshift-1-4ywxh   1/1       Running     0          7m
hello-openshift-1-5vsyl   1/1       Running     0          7m
hello-openshift-1-9ivns   1/1       Running     0          19m
hello-openshift-1-byte3   1/1       Running     0          7m
hello-openshift-1-riupx   1/1       Running     0          7m
sinatra-example-1-build   0/1       Completed   0          17m
sinatra-example-1-ebuiu   1/1       Running     0          14m
----

. Connect to the container:
+
----
[andrew@master ~]$ oc exec -ti sinatra-example-1-ebuiu "/bin/bash"

bash-4.2$
----

. Explore the container:
.. Run `id`.
.. Run `pwd` and `ls` and note the directory you are in.
.. Run `ps -ef` to see what processes are running.
+
----

bash-4.2$ id

uid=1000060000 gid=0(root) groups=0(root),1000060000

bash-4.2$ pwd

/opt/app-root/src

bash-4.2$ ls

Gemfile       README.md  config.ru	  example-mustache	 public
Gemfile.lock  app.rb	 example-model	  example-views		 tmp
README	      bundle	 example-modular  example-views-modular

bash-4.2$ ps -ef

UID         PID   PPID  C STIME TTY          TIME CMD
1000050+      1      0  0 22:41 ?        00:00:01 ruby /opt/app-root/src/bundle/
1000050+     33      0  0 22:51 ?        00:00:00 /bin/bash
1000050+     62     33  0 22:51 ?        00:00:00 ps -ef
----
+
[NOTE]
Your pod names and output differ slightly.

////
// We did not configure our Registry to use persistent storage so we leave out this part
// TODO implement section dealing with persistent storage
. As `andrew` on the `master` host, start an application based on the `https://github.com/openshift/sti-php` repository that would require an S2I build:
+
----
[root@master ~]# su - andrew
[andrew@master ~]$ oc new-app openshift/php~https://github.com/openshift/sti-php -l "todelete=yes"

--> Found image bbfc4eb (2 weeks old) in image stream "php" in project "openshift" under tag "5.6" for "openshift/php"

    Apache 2.4 with PHP 5.6
    -----------------------
    Platform for building and running PHP 5.6 applications

    Tags: builder, php, php56, rh-php56

    * A source build using source code from https://github.com/openshift/sti-php will be created
      * The resulting image will be pushed to image stream "sti-php:latest"
    * This image will be deployed in deployment config "sti-php"
    * Port 8080/tcp will be load balanced by service "sti-php"
      * Other containers can access this service through the hostname "sti-php"

--> Creating resources with label todelete=yes ...
    imagestream "sti-php" created
    buildconfig "sti-php" created
    deploymentconfig "sti-php" created
    service "sti-php" created
--> Success
    Build scheduled, use 'oc logs -f bc/sti-php' to track its progress.
    Run 'oc status' to view your app.
----

. Check the build logs to ensure that the build is complete and has been pushed into
 the registry (this needs some time):
+
----
[andrew@master ~]$ oc logs -f builds/sti-php-1

Cloning "https://github.com/openshift/sti-php" ...
	Commit:	06e5686866c575813ef15d925609ee73e5a88b44 (Set default opcache.memory_consumption to 128MB (#153))
	Author:	lucasnetau <james@lucas.net.au>
	Date:	Fri Mar 17 17:55:28 2017 +1100

---> Installing application source...


Pushing image 172.30.17.242:5000/explore-example/sti-php:latest ...
Pushed 0/5 layers, 2% complete
Pushed 1/5 layers, 21% complete
Pushed 2/5 layers, 42% complete
Pushed 3/5 layers, 72% complete
Pushed 3/5 layers, 94% complete
Pushed 4/5 layers, 99% complete
Pushed 5/5 layers, 100% complete
Push successful
----
TIP: The `-f` flag sets `oc logs` to "follow" the log, similar to `tail -f`.

. On `master`, verify that the registry is using the `registry-storage` volume which was configured by the ansible OpenShift installer script:
+
----
[root@master ~]# find /exports/registry/ | grep sti-php

/exports/registry/docker/registry/v2/repositories/explore-example/sti-php
/exports/registry/docker/registry/v2/repositories/explore-example/sti-php/_uploads
/exports/registry/docker/registry/v2/repositories/explore-example/sti-php/_layers
/exports/registry/docker/registry/v2/repositories/explore-example/sti-php/_layers/sha256
/exports/registry/docker/registry/v2/repositories/explore-example/sti-php/_layers/sha256/23f155615fc269417d39568cca589c6d87844490eafc2a3fde73e164b56f7e58
/exports/registry/docker/registry/v2/repositories/explore-example/sti-php/_layers/sha256/23f155615fc269417d39568cca589c6d87844490eafc2a3fde73e164b56f7e58/link
/exports/registry/docker/registry/v2/repositories/explore-example/sti-php/_layers/sha256/93b630859c0ea7dbe2b30ed22ccc5c53be542619405c6c4cb83cd2b5e7419648
/exports/registry/docker/registry/v2/repositories/explore-example/sti-php/_layers/sha256/93b630859c0ea7dbe2b30ed22ccc5c53be542619405c6c4cb83cd2b5e7419648/link
/exports/registry/docker/registry/v2/repositories/explore-example/sti-php/_layers/sha256/07f9fc72dffe9fbf14aadcb3f1580e973f5232e411641b6d9fb4da3291c21d19
/exports/registry/docker/registry/v2/repositories/explore-example/sti-php/_layers/sha256/07f9fc72dffe9fbf14aadcb3f1580e973f5232e411641b6d9fb4da3291c21d19/link
/exports/registry/docker/registry/v2/repositories/explore-example/sti-php/_layers/sha256/899d18f70e51e53b5c20a0ae2709325fae6db272e04fd6eb982176572d70026d
/exports/registry/docker/registry/v2/repositories/explore-example/sti-php/_layers/sha256/899d18f70e51e53b5c20a0ae2709325fae6db272e04fd6eb982176572d70026d/link
/exports/registry/docker/registry/v2/repositories/explore-example/sti-php/_layers/sha256/e50b1d9e90ba6803b2c3b8712a71299dafd614d8f8f6616bb692e2873207188f
/exports/registry/docker/registry/v2/repositories/explore-example/sti-php/_layers/sha256/e50b1d9e90ba6803b2c3b8712a71299dafd614d8f8f6616bb692e2873207188f/link
/exports/registry/docker/registry/v2/repositories/explore-example/sti-php/_layers/sha256/beec41a6e52f250a5a164b043891a88c6d3fa1ecfe242af83b11ce45c61e6201
/exports/registry/docker/registry/v2/repositories/explore-example/sti-php/_layers/sha256/beec41a6e52f250a5a164b043891a88c6d3fa1ecfe242af83b11ce45c61e6201/link
----
////

== Creating Applications Lab

This lab includes the following sections:

* *Deploy Application on Web Console*
+
In this section, you deploy an application from a code repository and follow the build logs on the OpenShift Container Platform web console and CLI.

* *Customize Build Script*

- Create an application from a forked Git repository, inject a custom build script, and start a rebuild from the web console.

- Review your custom script messages in the logs.

=== Deploy Application on Web Console

Here, you connect to and become familiar with the web console, create a project and an application, and scale a deployment and the topology view.

==== Connect To and Explore Web Console

. Use your browser to go to the OpenShift web console at `https://master.example.com:8443[https://master.example.com:8443]`.

. Log in as `andrew` with the password `r3dh4t1!`.

. Take a few minutes to browse your projects.

==== Create New Project

. Click *Projects* and select *View all projects* to return to the Projects view.

. Click the blue *New Project* button in the top right corner.

. Give the new project a name, display name, and description:
* *Name*: `my-ruby-project`
* *Display Name*: `My Ruby Example Project`
* *Description*: An explanation of your choice

Once the project is in place, the *Add to Project* screen is displayed.

==== Create New Application

. In the *Add to Project* screen, type `ruby` in the search field of the *Browse Catalog* Tab to filter the available instant apps, templates, and builder images.

. We choose the plain Ruby Application here
. Set the version to `2.2 - latest` 
. Click "Select"

. Specify the name and Git repository URL:
* *Name*: `my-ruby-hello-world`.
* *Git Repository URL*: `https://github.com/openshift/ruby-hello-world`.

. Click *Show advanced build and deployment options* and select the following options:
.. Notice that you get a route per default for your application.
.. Note that you can decide if Builds or Deployments should start automatically.
.. Change the scaling parameter to 3.
.. Create a label for app by the name of `environment` and the value of `dev`.

. Accept and create the application.

. Click *Continue to Overview* to go to the application's *Overview* screen.

. Click *View Log* to verify that a build is in progress.

. Review the log as the build progresses.

. Wait for the build to complete and use a browser to navigate to the
 application route: link:http://my-ruby-hello-world-my-ruby-project.cloudapps.example.com[http://my-ruby-hello-world-my-ruby-project.cloudapps.example.com]
//.. The database for our application isn't running, so expect to see the web
// page mention that.
+
[TIP]
====
* You can also use the command line to create a new application: `oc new-app https://github.com/openshift/ruby-hello-world -l  environment=dev`.

* To change scaling from the command line, use `oc scale`.
====

==== Scale Deployment 

. Go back to your application's *Overview* screen by clicking *Overview* at the upper left side.

. Observe the circle that shows the current number of pods, which is 3. You can increase that number by clicking the `^` button next to it.

. Click the `^` button twice to increase the number of replicas to 5.

. Go to *Applications* and select *Pods* to take a look at your new pods.

. Go back to your application's *Overview* screen by clicking *Overview* again.

////
// This Lab does not work as expected any more. Output from .s2i/bin/assemble is not visible in the build logs any more

=== [ Optional Side Lab ] Customize Build Script

OpenShift Container Platform 3 supports customization of both the build and run processes. Generally speaking, this involves modifying the S2I scripts from the builder image. While building your code, OpenShift Container Platform checks the scripts in your repository's `.sti/bin` folder to see if they override or supersede the builder image's scripts. If it finds scripts that do so, it executes those scripts.

For details on the scripts and their execution and customization, see the link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/creating_images/#s2i-scripts[OpenShift Documentation on Creating Images].


==== Clone Repository and Launch Application from Local Copy

. Log in to OpenShift Container Platform as `marina`:
.. Connect to the OpenShift Container Platform master by following the same steps as before.
.. When prompted, type the username and password:
** *Username*: `marina`
** *Password*: `r3dh4t1!`
+
----
[root@master ~]# su - marina
[marina@master ~]$ oc login https://master.example.com:8443 -u marina
[marina@master ~]$ oc new-project custom-s2i-script --display-name="Custom S2I Build Script" --description="This is the project we use to learn how to create a customized build script"
----

==== Fork Repository

IMPORTANT: This section requires a GitHub account. Create one if you do not have one already. It is free and useful.

. From the GitHub web UI, fork the `https://github.com/openshift/ruby-hello-world[https://github.com/openshift/ruby-hello-world]` Git repository into your own Git account by clicking *Fork* in the upper right corner.

* This creates a repository in your Git account with a name similar to `https://github.com/yourname/ruby-hello-world/`, where _yourname_ is your Git username.

. Clone this `https://github.com/yourname/ruby-hello-world` repository so that you can edit it locally and test a Red Hat-customized script with it:
+
CAUTION: Be sure to replace _yourname_ with your Git username.
+
----
[marina@master ~]$ git clone https://github.com/yourname/ruby-hello-world

Cloning into 'ruby-hello-world'...
remote: Counting objects: 271, done.
remote: Total 271 (delta 0), reused 0 (delta 0), pack-reused 271
Receiving objects: 100% (271/271), 41.53 KiB | 0 bytes/s, done.
Resolving deltas: 100% (89/89), done.
----

. Create an application by running `oc new-app` in the local repository:
+
----
[marina@master ~]$ cd ruby-hello-world/
[marina@master ruby-hello-world]$ oc new-app openshift/ruby~. 
----

. View the current build status and build logs:
+
----
[marina@master]$ oc get builds

NAME                 TYPE      FROM         STATUS    STARTED         DURATION
ruby-hello-world-1   Source    Git@master   Running   4 seconds ago   4s
----

. View the build log:
+
----
[marina@master ]$ oc logs -f builds/ruby-hello-world-1
----

. Verify that your pod has deployed:
+
----
[marina@master ]$ oc get pods

NAME                       READY     STATUS      RESTARTS   AGE
ruby-hello-world-1-70mlb   1/1       Running     0          12s
ruby-hello-world-1-build   0/1       Completed   0          9m
----


==== Add Script to Repository

. Open a new tab in your browser, go to `http://www.rhpet.de/assemble[http://www.rhpet.de/assemble]`, and copy all of the text there.

. Go to your GitHub repository for your application from the previous section.

. In the GitHub web UI, navigate to the `.s2i/bin` folder.

. Click the *Create new File* button at the top right (to the right of `bin` in the breadcrumb).

. Name your file `assemble`.

. In the GitHub web UI, paste the content you copied earlier into the text area.

. Type a commit message in the text field.

. Click *Commit*.


==== Create Application From Repository With Custom Build Script

. From your browser, go to the OpenShift web console at `https://master.example.com:8443`.
+

. Log in as `marina` with the password `r3dh4t1!`.

. Click the blue *New Project* button in the top right corner.

. Specify the project name, display name, and description:
* *Name*: `my-custom`
* *Display Name*: `My custom assemble script project`
* *Description*: An explanation of your choice
* Click on `Create`

** Once the project is in place, the *Select Image or Template* screen is displayed.

. In the *Select Image or Template* screen, type `ruby` in the search field to filter the available Instant Apps, Templates, and Builder Images.

. Select the `ruby:2.0` builder image from the right hand side.

. Specify the name and Git repository URL:
* *Name*: `my-custom-builder-test`
* *Git Repository URL*: `https://github.com/yourname/ruby-hello-world`
+
CAUTION: Remember to replace _yourname_ with your Git username in the above command.

. Follow the build process logs and watch for the assemble script messages, which confirms that the custom script ran:
+
----
I0522 18:22:53.179791       1 install.go:251] Using "assemble" installed from "<source-dir>/.s2i/bin/assemble"
...
I0522 18:22:53.180981       1 sti.go:166] Running "assemble" in "my-custom/my-custom-builder-test-1:adc52cfe"
----

////

////
// Test Section Do this to check if you can see the output before reactivation of the lab
# git clone http://github.com/MYGITUSER/ruby-hello-world
cd ruby-hello-world/
oc new-app openshift/ruby~.
vi .s2i/bin/assemble
chmod +x .s2i/bin/assemble
git add .s2i/bin/assemble
git config.email "mymail@forgithub.com"
git config.name "MYGITUSER"
git commit -m "adding custom asseble"
git push 

Verify that you have the assemble script in your github repository

You need to increase the verbosity of your build by adding. You can add an Environemnt Var to do this :
# oc set env bc ruby-hello-world BUILD_LOGLEVEL=3

Start a new build for your app 
# oc start-build ruby-hello-world
# oc get pods
build "ruby-hello-world-2" started
# oc log -f ruby-hello-world-3-build

////

== [ Optional ] Templates Lab

This lab includes the following sections:

* *Create and Upload Template*
+
In this section, you create a template for a two-tier application (front end and database), upload it into the shared namespace (the `openshift` project), and ensure that users can deploy it from the web console.

* *Use Templates and Template Parameters*
+
In this section, you create two separate template instances in two separate projects and establish a front-end-to-database-back-end connection by means of template parameters.

[NOTE] 
.Templates are a complex 
====
Templates allow an easy way to define all the required objects of an complex to be sepcified together and made available in Catalogs. Please see our link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-templates[OpenShift Documentation on Templates] for more information.
====
:numbered:

=== Create and Upload Template

==== Install Template

The example in this section shows an application and a service with two pods: a front-end web tier and a back-end database tier. This application uses auto-generated parameters and other sleek features of OpenShift Container Platform.  Note that this application contains predefined connectivity between the front-end and back-end components as part of its YAML definition. You add further resources in a later lab.

This example is, in effect, a "quick start" -- a predefined application that comes in a template and that you can immediately use or customize.

. As `root` on the master host, download the template's definition file:
+
----
[root@master ~]# wget http://people.redhat.com/~llange/yaml/Template_Example.yml
----

. Create the template object in the shared `openshift` project. This is also referred to as _uploading_ the template.
+
----
[root@master ~]# oc create -f Template_Example.json -n openshift

template "a-quickstart-keyvalue-application" created
----
NOTE: The `Template_Example.yml` file defines a template. You just added it to the openshift project. This make your template available throughout your OpenShift cluster. If you want to just have this temlate available for certain projects, put it directly into the project namespace and refrain from adding it to the `openshift` project.

The OpenShift Container Platform comes with a long list of preconfigured templates available for usage. You can take a look at the installed list with the following `oc` command. This list had 117 entries, that is why we did not include the output here. 

----
[root@master ~]# oc get templates -n openshift 
...
----

Do not be alarmed by the complexity of Templates. You can even create templates from existing Objects. Please see our Documentation on 
link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#export-as-template[How to Create a Template from existing Objects].

==== Create Instant App from Template

. On your browser, connect to the OpenShift web console at `https://master.example.com:8443[https://master.example.com:8443]`:
.. If prompted, accept the untrusted certificate.
.. Log in as `andrew` with the password `r3dh4t1!`.

. Click the blue *New Project* button in the top right corner.

. Specify the project name, display name, and description:
* *Name*: `instant-app`
* *Display Name*: `instant app example project`
* *Description*: `A demonstration of an instant app or template`.
+
[TIP]
====
Alternatively, perform this step from the command line:

----
[root@master ~]# oadm new-project instant-app --display-name="instant app example project" --description='A demonstration of an instant-app/template' --node-selector='region=primary' --admin=andrew
----
====

. From the `instant-app` project's *Overview* screen, click *Add to project*.
+
. Click the `ruby` tile to display ruby based applications and builder images
+
[NOTE]
Here you find the instant application, a special kind of template with the `instant-app` tag. The idea behind an instant application is that, when you create a template instance, you already have a fully functional application. In this example, your instant application is just a simple web page for key-value storage and retrieval.
+
. Select *a-quickstart-keyvalue-application*.
+
The template configuration screen is displayed. Here, you can specify certain options for instantiating the application components:
+
.. Set the `ADMIN_PASSWORD` parameter to your favorite password.
.. Add a label named `version` with the value `1`.

. Click *Create* to instantiate the services, pods, replication controllers, etc.

* The build starts immediately.
. Wait for the build to finish. You can browse the build logs to follow the progress.

[NOTE]
Our Application is currently still missing heath checks for all containers. You will deal with health checks later in this lab. If you are an experienced OpenShift User feel free to build a template with health checks included.

==== Use Application

After the build is complete, visit your application at `http://example-route-instant-app.cloudapps.example.com/[http://example-route-instant-app.cloudapps.example.com/]`.

[NOTE]
Be sure to use HTTP and _not_ HTTPS. HTTPS does not work for this example because the form submission was coded with HTTP links.

=== Use Templates and Template Parameters

Quick starts are nice and handy. But you will not be writing them from scratch. Developers are usualld stating with the components themselvs and will put the whole app together step by step first. This is what we will walk you through next. You treat the quick-start example as two separate applications to be wired together.

==== Deploy Ephemeral Database Back End

. Create a project new project for this work to live in:

.. Use your browser to connect to the OpenShift web console at `https://master.example.com:8443`.
.. If prompted, accept the untrusted certificate.
.. Log in as `marina` with the password `r3dh4t1!`.

.. Click the blue *New Project* button in the top right corner.

.. Specify the project name, display name, and description:
* *Name*: `templates`
* *Display Name*: `Templates Testing Project`
* *Description*: `Project for testing templates`
[TIP]
Alternatively, perform this step from the command line:
+
----
[root@master ~]# oadm new-project templates --display-name="Templates Testing Project" --description='Project used to test templates' --admin=marina
----

. Deploy an ephemeral MySQL database:

.. From the `templates` project's *Overview* screen, click *Add to project*.
.. Search for `mysql-ephemeral` in the `Browse Catalog`
.. Select the `mysql-ephemeral` database template.

.. Set the template parameters:
* *Database Service Name*: `database`
* *MySQL Connection Username*: `mysqluser`
* *MySQL Connection Password*: `redhat`
* *MySQL Database Name*: `mydb`
+
CAUTION: Make sure you set these values correctly, otherwise the application
 would not connect to the database backend.

.. Click *Create* and then click *Continue to overview*.
+
[TIP]
Alternatively, create the template instance from the command line:
+
----
[root@master ~]# su - marina
----
+
----
[marina@master ~]$ oc project templates
----
+
----
[marina@master ~]$ oc new-app --template=mysql-ephemeral --param MYSQL_USER=mysqluser --param MYSQL_PASSWORD=redhat --param MYSQL_DATABASE=mydb --param DATABASE_SERVICE_NAME=database
----

.. As `marina`, switch to the "templates" project (if you are not in that project already) and examine the objects that
 were created as part of the `mysql-ephemeral` template.
+
----
[marina@master ~]$ oc get projects

NAME                DISPLAY NAME                STATUS
custom-s2i-script   Custom S2I Build Script     Active
templates           Templates Testing Project   Active
----
+
----
[marina@master ~]$ oc project templates

Now using project "templates" on server "https://master.example.com:8443".
----
+
----
[marina@master ~]$ oc get dc

NAME       REVISION   DESIRED   CURRENT   TRIGGERED BY
database   1          1         1         config,image(mysql:5.7)
----
+
----
[marina@master ~]$oc get service -o wide

NAME       CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE       SELECTOR
database   172.30.142.93   <none>        3306/TCP   3m        name=database
----
+
[NOTE]
A deployment configuration is available for your instance. The service name is the same as that of your `DATABASE_SERVICE_NAME` parameter.

.. Verify that the values of the environment variables in the deployment configuration (`dc`) are correct:
+
----
[marina@master ~]$ oc env dc database --list

# deploymentconfigs database, container mysql
# MYSQL_USER from secret database, key database-user
# MYSQL_PASSWORD from secret database, key database-password
# MYSQL_ROOT_PASSWORD from secret database, key database-root-password
MYSQL_DATABASE=mydb
----

[NOTE]
.Security relevant Environment Settings
Notic that the security releavant settings in environment vars are not displayed by commented out in the above output.


==== Deploy Application's Ruby Front End

. As `marina`, create an application with the `https://github.com/openshift/ruby-hello-world` Git repository:
+
----
[marina@master ~]$ oc new-app openshift/ruby~https://github.com/openshift/ruby-hello-world MYSQL_USER=mysqluser MYSQL_PASSWORD=redhat MYSQL_DATABASE=mydb
----

. Verify that your service is in place:
+
----
[marina@master ~]$ oc get service -o wide

NAME               CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE       SELECTOR
database           172.30.142.93   <none>        3306/TCP   20m       name=database
ruby-hello-world   172.30.37.49    <none>        8080/TCP   2m        app=ruby-hello-world,deploymentconfig=ruby-hello-world
----

. Create an external route to your front-end application.

* If you do not specify a host name, the default subdomain route creates the route.
+
----
[marina@master ~]$ oc expose service ruby-hello-world

route "ruby-hello-world" exposed
----
+
----
[marina@master ~]$ oc get route

NAME               HOST/PORT                                                     PATH      SERVICE            LABELS
ruby-hello-world   ruby-hello-world-templates.cloudapps.example.com             ruby-hello-world   app=ruby-hello-world
----
+
. Wait for the build to complete. Then test your environment:
+
----
[marina@master ~]$ oc logs -f builds/ruby-hello-world-1
----

. Wait for the pods to start and verify that your application is running and connecting to the database:
+
----
http://ruby-hello-world-templates.cloudapps.example.com
----

== Deployment Life Cycle Lab

This lab includes the following sections:

* *Roll Back, Activate, and Code Life Cycle*
+
In this section, you manage the various phases of the deployment's life cycle.

* *Create and Use Web Hooks*
+
In this section, you create a Git webhook and start a new build and a new
 deployment automatically by pushing a code change in your Git repository.

:numbered:

=== Roll Back, Activate, and Code Life Cycle

Going back and forth between different versions of your application and its configuration is helpful not only for developers but for operators as well. You can switch back a deployment version that you know is a working state with OpenShift.

In this section, you take user `marina's hello-ruby` application, modify its
 front end, and then rebuild. Afterwards, you revert to the original version and
  then go on to your rebuilt version.

The next sections require a GitHub account.

==== Fork Repository

If you have not done so already: from the Git web interface, click *Fork* in the
 upper right corner of the GitHub web UI to fork the Git repository
  `https://github.com/openshift/ruby-hello-world` into your own account.

==== Create Your Application

[NOTE]
Remember that `buildconfig` (the build-configuration file) instructs OpenShift Container Platform on how to perform a build.

. As `root`, create a project for user `marina`:
+
----
[root@master ~]# oadm new-project lifecycle --display-name="Lifecycle Lab" \
    --description="This is the project we use to learn about Lifecycle management" \
    --admin=marina --node-selector='region=primary'
----

. Switch to user `marina` and use the `lifecycle` project:
+
----
[root@master ~]# su - marina
----
+
----
[marina@master ~]$ oc project lifecycle
----

. Create an application from the `https://github.com/openshift/ruby-hello-world` repository:
+
----
[marina@master ~]$ oc new-app https://github.com/openshift/ruby-hello-world 
----
+
////
[NOTE]
The `--strategy=source` option forces `oc new-app` to adopt the S2I strategy. A simpler alternative is the `new-app` command using your own repository, but you are intentionally picking the "wrong" repository as part of this learning exercise.
////
. Run `oc env` to add the environment variables for a database to be used later:
+
----
[marina@master ~]$ oc env dc/ruby-hello-world MYSQL_USER=mysqluser MYSQL_PASSWORD=redhat MYSQL_DATABASE=mydb
----

. Waiting for the build to finish. Meanwhile, expose your service to the world so that you can test it from your local browser:
+
----
[marina@master ~]$ oc expose service ruby-hello-world
----

. View the current `buildconfig` for your application:
+
----
[marina@master ~]$ oc get buildconfig ruby-hello-world -o yaml
----

. This should look like the following text:
+
[subs=+macros]
----
apiVersion: v1
kind: BuildConfig
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftNewApp
  creationTimestamp: 2016-11-15T11:28:51Z
  labels:
    app: ruby-hello-world
  name: ruby-hello-world
  namespace: lifecycle
  resourceVersion: "207409"
  selfLink: /oapi/v1/namespaces/lifecycle/buildconfigs/ruby-hello-world
  uid: af4f7bf4-ab26-11e6-8733-2cc2602a6dc8
spec:
  output:
    to:
      kind: ImageStreamTag
      name: ruby-hello-world:latest
  postCommit: {}
  resources: {}
  source:
    git:
pass:quotes[      *uri: https://github.com/openshift/ruby-hello-world*]
    secrets: []
    type: Git
  strategy:
    sourceStrategy:
      from:
        kind: ImageStreamTag
        name: ruby:2.3
        namespace: openshift
    type: Source
  triggers:
  - github:
      secret: yx3JIc_qegmYlwF4dVnT
    type: GitHub
  - generic:
      secret: GH4lDKWvCeLfBh0-O2u6
    type: Generic
  - type: ConfigChange
  - imageChange:
      lastTriggeredImageID: registry.access.redhat.com/rhscl/ruby-23-rhel7@sha256:3601dd48c3ee5def47fd641188bcf676f7447346296d4607c40862261b522d93
    type: ImageChange
status:
  lastVersion: 1
----

. Observe that the current configuration points to the `openshift/ruby-hello-world` github repository.

* Because you forked this repository earlier, you can now point to your git location.

==== Point to your git location

. Run `oc edit` to change the configuration.
+
----
[marina@master ~]$ oc edit bc ruby-hello-world
----

.. Change the `uri` reference to match the name of your GitHub repository, which is based in part on your GitHub username: `https://github.com/GitHubUsername/ruby-hello-world`.
+
[IMPORTANT]
Replace `GitHubUsername` with your actual GitHub username. For example, if your GitHub username is `jeandeaux`, the name of your GitHub repository is `'https://github.com/jeandeaux/ruby-hello-world`.

.. Save and exit `vi` by typing *:wq*.
+
NOTE: There are other ways to achieve this outcome, this way is used to cover
 the `oc edit` and the `oc start-build` commands.


. Run `oc get buildconfig ruby-hello-world -o yaml | grep uri`. Notice that `uri` has been updated.

. Run `oc get builds` to check if the new build has started:
+
----
[marina@master ~]$ oc get builds
----
+
No build was started, the change we made does not count as a config change. We essentially changed the application source. You will need to start a build manually :
+
----
[marina@master ~]$ oc get bc

NAME               TYPE      SOURCE
ruby-hello-world   Docker    https://github.com/YOURUSERNAME/ruby-hello-world
----
+
----
[marina@master ~]$ oc start-build ruby-hello-world

ruby-hello-world-2
----
+
This has started a new build :
+
----
[marina@master ~]$ oc get builds -w

NAME                 TYPE      FROM	 STATUS     STARTED              DURATION
ruby-hello-world-1   Source    Git	 Complete   16 minutes ago	 4m25s
ruby-hello-world-2   Source    Git	 Complete   About a minute ago   1m46s
----
+
Follow the build logs with :
+
----
[marina@master ~]$ oc logs -f bc/ruby-hello-world

I0709 23:41:08.493756       1 docker.go:69] Starting Docker build from justanother1/ruby-hello-world-7 BuildConfig ...
I0709 23:41:08.508448       1 tar.go:133] Adding to tar: /tmp/docker-build062004796/.gitignore as .gitignore
I0709 23:41:08.509588       1 tar.go:133] Adding to tar: /tmp/docker-build062004796/.sti/bin/README as .sti/bin/README
I0709 23:41:08.509953       1 tar.go:133] Adding to tar: /tmp/docker-build062004796/.sti/environment as .sti/environment
I0709 23:41:08.510183       1 tar.go:133] Adding to tar: /tmp/docker-build062004796/Dockerfile as Dockerfile
I0709 23:41:08.510548       1 tar.go:133] Adding to tar: /tmp/docker-build062004796/Gemfile as Gemfile
.......
Cropped Output
.......
----

. Search for the available `mysql` applications (templates):
+
----
[marina@master ~]$ oc new-app --search mysql 
...
----
+
The above command outputs quite a lot of info in current clusters. Lets run this again but reduce detail with grep. I just want the templates or images that start with mysql. As the next two lines are descriptions for these templates or images, lets display them as well.
+
----
[marina@master ~]$ oc new-app --search mysql | grep ^mysql -A 2

mysql-persistent
  Project: openshift
  MySQL database service, with persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mysql-container/blob/master/5.7/README.md.
--
mysql-ephemeral
  Project: openshift
  MySQL database service, without persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mysql-container/blob/master/5.7/README.md.
--
mysql
  Project: openshift
  Tags:    5.6, 5.7, latest
--
mysql
  Registry: Docker Hub
  Tags:     latest
----

. Create the `database` application by running `oc new-app`:
+
----
[marina@master ~]$ oc new-app --template=mysql-ephemeral --param MYSQL_USER=mysqluser --param MYSQL_PASSWORD=redhat --param MYSQL_DATABASE=mydb --param DATABASE_SERVICE_NAME=database
----

. Verify that your values were processed correctly:
+
----
[marina@master ~]$ oc env dc/database --list
----
+
----
# deploymentconfigs database, container mysql
# MYSQL_USER from secret database, key database-user
# MYSQL_PASSWORD from secret database, key database-password
# MYSQL_ROOT_PASSWORD from secret database, key database-root-password
MYSQL_DATABASE=mydb
----
+
Notice that you can net see the values of MYSQL_USER, MYSQL_PASSWORD and MYSQL_ROOT_PASSWORD as they are marked as secret keys in the template definition. The values are only shown in the output ofthe `oc new-app` command above.
+
. You must redeploy your front end so that it checks for the database again. You
 can either delete just the pod, or you can redeploy the application with :
+
----
[marina@master ~]$ oc deploy ruby-hello-world --latest
----

. You can see the logs for your latest deployment if you use the `oc logs` command this way:
+
----
[marina@master ~]$  oc logs -f dc/ruby-hello-world

I1222 01:54:45.485814       1 deployer.go:198] Deploying from lifecycle/ruby-hello-world-3 to lifecycle/ruby-hello-world-4 (replicas: 1)
I1222 01:54:46.913895       1 rolling.go:232] RollingUpdater: Continuing update with existing controller ruby-hello-world-4.
I1222 01:54:47.019320       1 rolling.go:232] RollingUpdater: Scaling up ruby-hello-world-4 from 0 to 1, scaling down ruby-hello-world-3 from 1 to 0 (keep 0 pods available, don't exceed 2 pods)
I1222 01:54:47.020399       1 rolling.go:232] RollingUpdater: Scaling ruby-hello-world-4 up to 1
I1222 01:54:51.372703       1 rolling.go:232] RollingUpdater: Scaling ruby-hello-world-3 down to 0
----
+
. Check that your Application is working now and that you can put and get keys.


=== [ Optional ] Create and Use Webhooks

You can integrate external systems into your OpenShift Container Platform
environment so that they can start OpenShift Container Platform builds. This allows for use cases where making a change in the source code triggers a build process in OpenShift. This process is triggerd by web hooks. This is a special URL that e.g. your code repository can make a call to, when new code is available.

==== Find the Webhook URL

Your GitHub account can configure a webhook whenever you push a commit to a specific branch.

. Find the webhook URL:
.. Go to the openshift web console, log in as Marina.
.. Navigate to your project.
.. Click *Builds* and then click the sub menu entry *Builds*. 
.. Select your application build config `ruby-hello-world` from the list
.. Select the "Configuration" Tab.

* Two webhook URLs are displayed.

. Copy the github URL, which looks like this:
+
----

https://<Put External master name here>:8443/oapi/v1/namespaces/lifecycle/buildconfigs/ruby-hello-world/webhooks/_hoMePVjqAPrLKk526hP/github

----
+
You can get the External Master name from the https://lab.rhpet.de[Red Hat Partner Enablement Lab Portal].
+
.You can also see the webhook on the command line 
[subs=+macros]
----
[marina@master ~]$ oc describe bc ruby-hello-world 
Name:		ruby-hello-world
Namespace:	lifecycle
Created:	13 minutes ago
Labels:		app=ruby-hello-world
Annotations:	openshift.io/generated-by=OpenShiftNewApp
Latest Version:	1

Strategy:	Docker
URL:		https://github.com/LutzLange/ruby-hello-world
From Image:	ImageStreamTag openshift/ruby:2.3
Output to:	ImageStreamTag ruby-hello-world:latest

Build Run Policy:	Serial
Triggered by:		Config, ImageChange
Webhook Generic:
	URL:		https://master.example.com:8443/oapi/v1/namespaces/lifecycle/buildconfigs/ruby-hello-world/webhooks/a9ui0d5DbzH6_IGOttGO/generic
	AllowEnv:	false
pass:quotes[*Webhook GitHub:
	URL:	https://master.example.com:8443/oapi/v1/namespaces/lifecycle/buildconfigs/ruby-hello-world/webhooks/_hoMePVjqAPrLKk526hP/github*]

Build			Status		Duration	Creation Time
ruby-hello-world-1 	complete 	3m34s 		2017-06-16 02:56:23 -0400 EDT

No events.

----
+
. In the GitHub repository, which you forked earlier, go to *Settings -> Webhooks*.
. Click "Add webhook"
. Paste the URL (with the external Master Host Name in it) into the *Payload URL* field.
. Set "Content type" to `application/json`
. Disable SSL verification.
. Click *Add Webhook*.

==== Test Your Webhook

To test your webhook by changing and commiting / pushing some code in the Git repository. Do the following:

[NOTE]
Alternatively, you can test the webhook the usual way by cloning your repository locally, making the required changes, and pushing them to the repository.

. Go to your forked repository (`https://github.com/GitHubUsername/ruby-hello-world`) and find the `main.erb` file in the `views` folder.

* You can edit files in the GitHub web UI.

. Change this HTML code
+
----
    <div class="page-header" align=center>
      <h1> Welcome to an OpenShift v3 Demo App! </h1>
    </div>
----
+
to read as follows (including the deliberately misspelled `crustom`):
+
----
    <div class="page-header" align=center>
      <h1> This is my crustom demo! </h1>
    </div>
----

. Commit the change to the repository.

. Check if a build has started.
+
[CAUTION]
If another build is already running, this latest build may fail because both builds are pushing to the registry. Either run `oc delete build` to stop the earlier build or `oc start-build` to restart the failed build.

. Log in as `marina` and check the web UI to verify that the build is running.

. Wait for the build to complete. It can take a minute for your service endpoint to update.
. Use your browser to go to the application at `http://ruby-hello-world.lifecycle.cloudapps.example.com/`.

* The output includes the deliberately misspelled `crustom`.
* If you try to access the application before the update is complete, you may see a `503` error.


==== Roll Back Your Application

Because you failed to properly test your application and your typo made it into production, you must revert to the previous version of your application.

. Log in to the web console as `marina`.

. Go to your `Lifecycle Lab` 

. Open the *Application* Menu

. Select *Deployments* 

. Select your frontend `ruby-hello-world` from the list.

* You should see at least 2 deployments versions for your frontend. The lastest version is marked active.
+
[TIP]
====
Alternatively, view this information from the CLI:

----
[marina@master ~]$ oc describe dc/ruby-hello-world
...
----

====

. From the CLI, roll back the deployment:

.. Determine which rollouts are available:
+
----
[marina@master ~] oc rollout history dc/ruby-hello-world 

deploymentconfigs "ruby-hello-world"
REVISION	STATUS		CAUSE
1		Complete	image change
2		Complete	config change

----
.. Choose a deployment and see what a rollback to `ruby-hello-world-X` would
 look like:
+
----

[marina@master ~]$ oc rollback ruby-hello-world --to-version=X --dry-run # X is your desired deployment
Name:		ruby-hello-world
Namespace:	lifecycle
Created:	About an hour ago
Labels:		app=ruby-hello-world
Annotations:	openshift.io/generated-by=OpenShiftNewApp
Latest Version:	4
Selector:	app=ruby-hello-world,deploymentconfig=ruby-hello-world
Replicas:	1
Triggers:	Config, Image(ruby-hello-world@latest, auto=false)
Strategy:	Rolling
Template:
  Labels:	app=ruby-hello-world
		deploymentconfig=ruby-hello-world
  Annotations:	openshift.io/generated-by=OpenShiftNewApp
  Containers:
   ruby-hello-world:
    Image:			172.30.120.134:5000/lifecycle/ruby-hello-world@sha256:20c8bf8238467e3343e3302ac36fc5f7fe3bbb9b5f48ff65a37dcc790339e48e
    Port:			8080/TCP
    Volume Mounts:		<none>
    Environment Variables:	<none>
  No volumes.

Latest Deployment:	<none>
----

* From the above output, you can see that you can go ahead with the rollback.

.. Roll back the deployment:
+
----
[marina@master ~]$ oc rollback ruby-hello-world --to-version=1 # X is your desired deployment

#2 rolled back to ruby-hello-world-1
Warning: the following images triggers were disabled: ruby-hello-world:latest
  You can re-enable them with: oc set triggers dc/ruby-hello-world --auto
----

. Click the *Overview* tab for your project and note that you have a new deployment is happening. 

[NOTE]
Rolling back to an old deployment version creates the old state as a new deployment. You just reference an old state when rolling back.

. After a few minutes, go back to the application in your browser.

* The old "Welcome . . ." message is displayed.

==== Roll Your Application Forward

To roll forward (activate) the typo-enabled application:

----
[marina@master ~]$ oc rollback ruby-hello-world-X # X is your desired deployment

#11 rolled back to ruby-hello-world-X
Warning: the following images triggers were disabled: ruby-hello-world
  You can re-enable them with: oc deploy ruby-hello-world --enable-triggers
----

== Cluster Metrics Introduction

Metrics are an important part of every monitoring solution. Metrics allow you to watch certain counters in your environment. OpenShift Container Platform comes with a metric stack that you can use for multiple purposes. The default use case collects information about CPU usage, memory usage, network troughput and a few other measurements.

[NOTE]
.Self-Signed-Certificates 
====
If you are using self signed certificates, you will need to direct your browser to the metrics URL once and accept the certificate in order to see metrics in the WebUI of OCP.
====

image::http://people.redhat.com/~llange/labimg/Open-Metrics-URL-Error-in-WebUI.png[Metrics Error in WebUI]

. Click on "Open Metrics URL" - this will open a new tab in your browser.
. In Firefox, click the "Advanced" button. Choose "Add Exeception..." and click "Confirm Security Exception".
. The page will look like this, if everything is working.
+
image::http://people.redhat.com/~llange/labimg/Hawkular-Metrics-working.png[]
+
. Reload the WebUI tab that showed the initial error. OCP 3.5 will show CPU, Memory and Network metrics in the WebUI Overview section of the projects and in the metrics tab of each pod.
+
image::http://people.redhat.com/~llange/labimg/Metrics-Default-Project-Overview-Page-Registy-Console.png[]
+
. In the default project, click the pod of the registry-console, you get to the pod details :
+
image::http://people.redhat.com/~llange/labimg/Registry-Console-Pod-Details.png[]
+
. Go to the metrics tab will display more details and allow you to select what time range of metrics you want to display.
+
image::http://people.redhat.com/~llange/labimg/Metrics-Details-Registy-Console.png[]

The metrics stack is usually deployed in the openshift-infra project. You can check status there as well. This can be done from the command line or in the Web UI. This is how you do this on the command line :

----
$ oc project openshift-infra
$ oc get pods
NAME                         READY     STATUS    RESTARTS   AGE
hawkular-cassandra-1-6tx6k   1/1       Running   0          36m
hawkular-metrics-zp23l       1/1       Running   1          36m
heapster-8fjdc               1/1       Running   0          36m
----

There are 3 pods in the output above. Notice that all pods are marked as running and all containers that are supposed to run in each pod are up (1/1). Heapster collects the metrics, hawkular provides it for retrieval and stores the metrics in the cassandra backend. There are 4 services created for internal communication and one route that allows access the the hawkular-metrics service from the WebUI.

----
$ oc get services
NAME                       CLUSTER-IP       EXTERNAL-IP   PORT(S)                               AGE
hawkular-cassandra         172.30.181.23    <none>        9042/TCP,9160/TCP,7000/TCP,7001/TCP   2h
hawkular-cassandra-nodes   None             <none>        9042/TCP,9160/TCP,7000/TCP,7001/TCP   2h
hawkular-metrics           172.30.78.27     <none>        443/TCP                               2h
heapster                   172.30.203.233   <none>        80/TCP                                2h
----
----
$ oc get route
NAME               HOST/PORT                                SERVICES           PORT   
hawkular-metrics   hawkular-metrics.cloudapps.example.com   hawkular-metrics   <all>  
----

The next diagram illustrates what you have just seen in the Web UI and on the Command line:

image::http://people.redhat.com/~llange/labimg/OpenShift-Hawkular-Stack-Overview.png[Metric Stack Overview]

Verify the metric stack status as well in your WebUI :

image::http://people.redhat.com/~llange/labimg/Metrics-Status-Overview-WebUI.png[]

You can also check the oadm diagnostics output to look for health information of the metric stack. Note that oadm offers a rich set ouf diagnostics to choose from.

----
# oadm diagnostics --help
... 
 
  oadm diagnostics <DiagnosticName>
  
The available diagnostic names are: AggregatedLogging, AnalyzeLogs, ClusterRegistry, ClusterRoleBindings, ClusterRoles,
ClusterRouter, ConfigContexts, DiagnosticPod, MasterConfigCheck, MasterNode, MetricsApiProxy, NetworkCheck,
NodeConfigCheck, NodeDefinitions, ServiceExternalIPs, UnitStatus.

Usage:
  oadm diagnostics [options]

...
----

. Run the oadm diagnostics for the MetricsApiProxy :

----
# oadm diagnostics MetricsApiProxy
[Note] Determining if client configuration exists for client/cluster diagnostics
Info:  Successfully read a client config file at '/root/.kube/config'
Info:  Using context for cluster-admin access: 'openshift-infra/master-example-com:8443/system:admin'

[Note] Running diagnostic: MetricsApiProxy
       Description: Check the integrated heapster metrics can be reached via the API proxy
       
[Note] Summary of diagnostics execution (version v3.5.5.8):
[Note] Completed with no errors or warnings seen.
----

== CloudForms Introduction

CloudForms is the designated Operations Tool for the Openshift Container Platform. But CloudForms is much more than just a tool to look at and manage OpenShift. It originally found it's way into the Red Hat portfolio though the acquisition of the company ManageIQ. It was primarily a virtualisation management tool in the beginning. The big differentiator to other existing tools was the main focus on *Operational Visibility* or *Insight* as it is called back in the day.

CloudForms is a manager of managers. It talks to the APIs of other management infrastructures. These are called providers.

.CloudForms can be the central manager for all these infrastructures
* AWS
* Google Cloud
* Azure
* Red Hat OpenStack
* Microsoft System Center VMM
* Red Hat Virtualization
* VmWare vCenter
* Ansible Tower
* *Red Hat OpenShift Container Platform*

image::http://people.redhat.com/~llange/labimg/CloudForms-Overview1.png[]

CloudForms gathers information about objects first and puts these in the internal (PostgreSQL) database. You can then take action based on the found information and influence the managed infrastructures. This was called *Control*. Provisioning instances or virtual machines or containers is an example for control.

Control is the basis for *Service Automation*. This is where the Service Catalog features of CloudForms come into play. You can build your own Services by designing your own or reusing existing Forms to collect all the details you need to provision and manage your workloads. The Service Lifecycle goes through several stages. You can order a service from a catalog. This will create a request that is then approved or denied. Approval can be manual or automatic. An approved request is then scheduled an run, thus creating a service. Another important part of a services life is retirement. That process can be planned and implemented with CloudForms. 

You can think of CloudForms as a framework that allows for implementation of *your processes* across all the supported infrastrutures.

CloudForms can even take a look into the managed workloads. This is called *Smart State Analysis* and works storage based. CloudForms extracts a certain level of detail from the analysed workloads. These are things like installed Software with versions and Users e.g. You can even extend this mechanism to autodetect and label certain workloads. This tagging is another key aspect in CloudForms that helps you to structure your environment in a sensible way. Some people use Tags to designate Service Level Agreements. 

*Policy & Compliance* give me mechanisms to define what states I want to see in my environment and how to get there. I could for example postulate that every Microsoft Windows system needs to have a current virus scanner. CloudForms can see into the workloads and can see what software is installed. I could than declare / implement what should happen if there is no or an out of date virus scanner found. That action is highly depending on your environment and can range from inform someone, to do not allow this workload to run.

The following image illustrates the level of detail that CloudForms collects for workloads. This is a virtual machine called ansible-tower. It is running a RHEL instance, the same level of detail is available for Microsoft Windows Workloads.

image::http://people.redhat.com/~llange/labimg/Virtual-Machine-Details-CloudForms.png[Virtual Machine Details]
:

=== Connect to CloudForms

We did deploy a CloudForms 4.5 for you as part of this Lab. Open your Browser and connect to it via http://cf.example.com. 

. Log into the CloudForms Interface using the User "admin" and the password r3dh4t1!. 
+
You will find the main navigation panel on the right hand side. Hover over Compute, move to Containers and Click on "Overview" in the 3rd side panel. This will bring you to the Container Dashboard. This is an Overview over all configured OpenShift environments.
+
image::http://people.redhat.com/~llange/labimg/CloudForms-Container-Provider-Dashboardv2.png[Container Dashboard]
+
The container dashboard give a quick overview of the known / configured OpenShift Cluster Environments. The section at the top of the board lists the number of known Objects. Below this are several usage statistics. These are filled only if the hawkular metric stack is set up in your OpenShift Container Platform. Note that it will take up to 24h after configuring the Hawkular part of the provider setup in CloudForms until the usage information is displayed.
+
CloudForms offers another tool called *Topology*. This view might be familiar to you if you know OpenStack Horizon. The Topology view can be quite full and overwhelming if your cluster is bigger or has many applications.  
+
. Go to Compute -> Containers -> Click on Topology.
+
image::http://people.redhat.com/~llange/labimg/CloudForms-Container-Topology.png[]
+
If this view is too full use the service icons to toggle visibilty of the respective objects. You could also use the search field to grey out every object not matching your search. 
+
The nice thing about the topology view is that every object is displayed with a status indicator. In our case every object has a green border. It an object has a failed state, you will see it with a red boarder instead. You could chose to display object names, or hover over the object with the mouse cursor to see name, type and status of that object. A double click on the object will bring you to the details page of that object in CloudForms.

== Containers and Security

In this Lab you will:

* Start a Ruby based example application
* Add a Heath Check
* Scan the container image for known vulnerabilities
** [Optional : prevent execution of images with know vulnerabilities]
* Patch issues found if possible
* Create a schedule for scanning container images 
* Configure OpenShift to always get the latest ruby builder image

=== Application Setup

For more background information on Application setup consult the official link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-new-app[OpenShift Documentation here].

There are multiple ways to start or create your application in OpenShift. You can use the oc tool from the command line, or you can use the WebUI. You could even do it with the RestAPI. We will document how to use the command line to create a test application here. You are free to use the Web UI as well. The command line offers a powerful oc sub command called new-app. *oc new-app* is the swiss army knife for application creation as it will create all the objects you need to run your application in the OpenShift Container Platform. 

You will first create a project for you application to live in. Projects are used to separate Application Management. There can be multiple apps with the same name on the same OpenShift cluster, as long as they live in different projects. 

. Create a project called "testproject" as user andrew now :
+
----
$ oc new-project testproject --description="My Test Project" --display-name="Test Project"
now using project "testproject" on server "https://master.example.com:8443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.
----
+
. take a look at the --help output from the oc new-app command :
+
----
$ oc new-app --help
Create a new application by specifying source code, templates, and/or images
...
----
+
You can use ImageStreams, Templates and Docker Images to create an Application.
+
An *image stream* comprises any number of Docker-formatted container images identified by tags. It presents a single virtual view of related images, similar to an image repository. Image streams can be used to automatically perform an action when new images are created. Builds and deployments can watch an image stream to receive notifications when new images are added and react by performing a build or deployment, respectively.
+
A *template* describes a set of objects that can be parameterized and processed to produce a list of objects for creation by OpenShift Container Platform. The objects to create can include anything that users have permission to create within a project, for example services, build configurations, and deployment configurations. A template may also define a set of labels to apply to every object defined in the template.
+
Be aware that the *Docker Container images* need to be compatible with the OpenShift security restrictions to run in the platform. Most images found on Docker Hub do not adhere to security best practices and run as root. This is not allowed on a default OpenShift installation. Take a good look at the link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/creating_images/#creating-images-guidelines[OpenShift Guidelines for Container Images]. Be sure to look through the link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/creating_images/#openshift-container-platform-specific-guidelines[OpenShift Specific Guidelines] as well.
+
. Create a Test Application using the ruby:2.3 builder image and the ruby-ex example application. You can do this in the WebUI or on the command line. This is how to do it on the command line : 
+
----
$ oc new-app openshift/ruby:2.3~https://github.com/openshift/ruby-ex --name=rtest
----
+
. Go to the Overview Page of your testproject in the Web UI. If you are quick enough, you will be able to see the following screen. Notice that there is a build in progress.
+
image::http://people.redhat.com/~llange/labimg/TestProject-rtest1-Deploy-1.png[]
+
. Click "View Log" and go to the build page. You can view the logs of the build process here. Notice that the last line should read "Push successful". This tells us that the resulting image is saved in internal Registry.
+
image::http://people.redhat.com/~llange/labimg/TestProject-rtest1-buildlog.png[]
+
. After successful deployemnt your Overview page of the Test Project should look like this :
+
image::http://people.redhat.com/~llange/labimg/TestProject-rtest1-no-route.png[]
+
You will need to create a route object to expose you application to access from the outside. Note that there will be a Route for your application already if you created it using the Web UI. If there is no route for you application you will find the "Create Route" button in top right corner like in the screen shot above. You could use this button to create a route in the Web UI. Or you could expose you application on the command line with :
+
[source,cmd,indent=o]
----
[andrew@master ~]# oc project testproject
Already on project "testproject" on server "https://master.example.com:8443".
----
+
----
[andrew@master ~]# oc get service
NAME      CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
rtest     172.30.111.54   <none>        8080/TCP   20h
----
+
----
[andrew@master ~]# oc expose service rtest
route "rtest" exposed
----
+
[source,cmd,indent=o]
----
[andrew@master ~]# oc get route
NAME      HOST/PORT                                 PATH      SERVICES   PORT       TERMINATION   WILDCARD
rtest     rtest-testproject.cloudapps.example.com             rtest      8080-tcp                 None
----
+
You Overview page of the Test Project should now display the URL link:http://rtest-testproject.cloudapps.example.com[http://rtest-testproject.cloudapps.example.com] instead of the "Create Route" button. Click the link to go see if your application is working. The result should look like this :
+
image::http://people.redhat.com/~llange/labimg/TestProject-rtest1-the-app.png[]
+
If something went wrong and you want to delete your application, you can do this with the oc tool using the label app=truby. *Do not delete the app* if it runs without problems, we will use it in the next section of this lab. 
+
. Delete the app, if you want to start over and try the oc new-app command again in step 2.
+
----
[andrew@master ~]$ oc delete all -l app=rtest
----

=== Application Health Checks

Read more about Readiness and Liveness Check in the link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-application-health[OpenShift Developer Guide about health checks].

It is good style for your application to provide health information for the platform to consume. A best practice pattern is to offer a web page that provides a good return code if your app is healthy. The ruby example that we used provides this information here : link:http://rtest-testproject.cloudapps.example.com/health[http://rtest-testproject.cloudapps.example.com/health]. 

You can take a look at the application source on Github.com : https://github.com/openshift/ruby-ex for more details.

. Got to the Web UI and open the Overview page for your testproject. Notic that you are displayed a warning about missing health checks. 
+
image::http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-1.png[]
+
. Add the health check to your deployement by clicking "Add Health Checks" and then "Add Liveliness Probe".
+
image::http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-2-App-Probe.png[]
+
There are three possible types of checks that you can chose. HTTP Get is suitable for our application here. Don't forget to add the path "/health" before you hit the "save" button. 
+
image::http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-3-add-probe-details.png[]
+
Notice that saving your changes changes your deployement settings, thus a new deployement is triggerd by the configuration change. 
+
image::http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-4-new-deployment-list.png[]
+
If you are quick enough, you can see the new deployment happening live.
+
. Go to the Overview Page of your "Test Project"
+
image::http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-5-new-deployement-overview.png[]
+
The platform is now able to even detect internal application failure situations, but is of cause depending on the nature of the the faults in the application and the quality of the health checks implemented. If you do not provide any health checks, OpenShift falls back onto checking if the docker container is listed as up and running. 

////
TODO find a good way to break the liveiness check
- break health
  # oc rsh 

- watch ocp take aktion
////

=== Container Image Security Scans with CloudForms

Container Scans help you to determine known vulnerabilities in your container images. You can run Container Images scans from CloudForms. Initiate a Container Scan for your the container image of the "rtest" application. 

. Sign in to your CloudForms instance link:https://cf.example.com[https://cf.example.com]. 
. Log in as User "admin" with the password "r3dh4t1!".
. Go to : Compute -> Containers -> Container Images
. Fill out the Search box in the upper right corner search for "rtest".
+
[NOTE]
.Can't find the image -> Initiate a Container Provider Refresh
[subs=+macros]
====
If you can't see your image, you can wait and try again later or you can initiate a Provider Refresh in CloudForms. 
+
. Go to "Compute" -> "Containers" -> "Providers" 
. Select the OpenShift Cluster 1. 
. Select the Configuration Menu and click "Refresh Items and Relationships". 
+
It might still take ~15 min for the Refresh to run.
+
image::http://people.redhat.com/~llange/labimg/CloudForms-Container-Provider-Refresh.png[] 
====
+
. In the search results page, click on the name field of the rtest container image to take a loot at the details. 
. Notice that there is no information in the Configuration Box about the RPM *packages*. 
. There is no OpenSCAP Results as well. And the Compliance Box tells us that there is no status available.
+
image::http://people.redhat.com/~llange/labimg/rtest-Image-details-before-scan.png[]
+
We want to change this. 
+
. Assign a Policy Profile first.
+
.. Open the Policy Menu
.. Click "Manage Policies"
.. Select the Policy Profile "OpenSCAP profile"
.. Click "save"
+
. You should see a box that reads "Policy Provile assigned successfully". 
. Request an analysis of image content, or in other words start a Container Image Scan.
+
.. Open the Configuration Menu
.. Select "Perform SmartState Analysis"
.. Confirm "Perform SmartState Analysis on the selected items"
+
Look for the scan in the Web UI
+
.. Go back to the OpenShift Web UI
.. Make sure you are loged in as `admin`
.. Select the "management-infra" project
.. You should see a workload starting for the container scan. Notice the openshift3/image-inspector image.
+
image::http://people.redhat.com/~llange/labimg/Image-Scan-in-progress.png[]
+
The scan and the transfer of the information takes a few minutes in our demo environment. After the scan you should be able to see the scan details after a reload of the page in the CloudForms Interface.
+
image::http://people.redhat.com/~llange/labimg/rtest-scan-results-overview.png[]
+
You should now see values like these :
+
====
* 430 Packages
* 458 OpenSCAP Results
* An OpenSCAP html listed as available
* The OpenSCAP Failed Rules Summary list 3 High severities results.
* The Image was marked a *Non-Compliant*
* We do have a Compliance History available now.
====
+
// TODO explain SCAP and OpenSCAP
. Click on the line "OpenScap Scan Results"
. Click the String "Result" to sort for fails
. You will notice that the name of the rule that failed is no helpful information for us 
+
image::http://people.redhat.com/~llange/labimg/OpenSCAP-scan-results-in-cf-rtest.png[]
+
. Click on the OpenSCAP html line. Use your browser to display this html page.
. Deselect the box "pass" to see the failed rules quickly. This list has the Red Hat Security Advisory Numbers and short text in the Rule Overview section table under tiles.
+
image::http://people.redhat.com/~llange/labimg/rtest-scan-results-scap-html.png[]
+
Every Red Hat Security Advisory is explained in fine detail on https://rhn.redhat.com/errata and https://access.redhat.com/errata. The first page is the older incarnation that is still around and has more detail than the Customer Portal equivalent. You can go directly to https://access.redhat.com/errata/RHSA-2017:0372 . 
+
====
.Links to the Customer Portal Advisory Pages for the issues found
. link:https://access.redhat.com/errata/RHSA-2017:0372[RHSA-2017:0372: kernel-aarch64 security and bug fix update (Important)]
. link:https://access.redhat.com/errata/RHSA-2017:1308[RHSA-2017:1308: kernel security, bug fix, and enhancement update (Important)]
. link:https://access.redhat.com/errata/RHSA-2017:1365[RHSA-2017:1365: nss security and bug fix update (Important)]
====
+
Lets go through the list to see and evaluate what this scan found in our image.

==== RHSA-2017:0372
This is an issue that is effecting kernels on Arm Architectures. Red Hat provides updated packages for aarch64 to fix this. The scans that we are doing are based on package numbers. And our container image holds a kernel specific package as well.

. Check the package list of the Container Image for kernel packages.

.. You could loCloudForms, the WebUI or on the command line.

----
# oc project testproject
Now using project "testproject" on server "https://master.example.com:8443".
# oc get pods
NAME            READY     STATUS      RESTARTS   AGE
rtest-1-build   0/1       Completed   0          17h
rtest-1-rfs06   1/1       Running     0          17h
# oc rsh rtest-1-rfs06
sh-4.2$ rpm -qa | grep kernel
kernel-headers-3.10.0-514.16.1.el7.x86_64
----

As we are clearly not on aarch64. This is x86_64 so we can ignore this result and file a bugzilla against CloudForms and the Container Scanner. This is a false finding.

==== RHSA-2017:1308

This advisory relates to multiple issues with only one issue marked as important. A local attacker could possibly use a flaw in the packet_set_ring() function of the kernel to produce a buffer overflow and crash a system if that application ran with CAP_NET_RAW. It might be possible to use this buffer overflow to gain additional privileges.

The question to ask here is, if this is / might be a problem for the container that you are running. As the affected package here is only the kernel-headers package, we can assume that this needs to be checked and fixed on the container host side as well. 

[NOTE]
This issue will only impact you, if you did build an application on top of this image that used these kernel headers. This is why we have this advisory included. 

==== RHSA-2017:1365

The third issue listed here is found in the Name Service Switch. This issue effects the name service switch that is genuine installed as rpms in the image.

[quote, RHSA-2017:1365 and CVE-2017-7502]
____
A null pointer dereference flaw was found in the way NSS handled empty SSLv2 messages. An attacker could use this flaw to crash a server application compiled against the NSS library. (CVE-2017-7502)
____

==== Image Details

To be able to evaluate how to fix issues, you need to know where an Image came from and who created it. You should turn to the image creator first to fix issues found. There many different places that you can turn to for details of this image. 

. You can look at the detailed page for the rtest Container Image in CloudForms
. You can open the link:https://registry-console-default.cloudapps.example.com/registry#/images/testproject/rtest:latest[OpenShift Registry Console] in OpenShift ( login as admin or andrew )
. You can got to the `oc` command line in OpenShift and look at Images and ImageStreams.

We are using the `oc` command here. Note the bold printed parts below :

----
$ oc get images | grep rtest
sha256:b925ddb55063d5f26526ca09e2f55aec5a8c4e95a7e4e4b644dd6ba08e3733c4   172.30.120.134:5000/testproject/rtest@sha256:b925ddb55063d5f26526ca09e2f55aec5a8c4e95a7e4e4b644dd6ba08e3733c4

----
 
OpenShift identifies images with sha256 values. So we have to use this sha256 value to take a closer look.

[subs=+macros]
----
$ oc describe image sha256:b925ddb55063d5f26526ca09e2f55aec5a8c4e95a7e4e4b644dd6ba08e3733c4
Name:		sha256:b925ddb55063d5f26526ca09e2f55aec5a8c4e95a7e4e4b644dd6ba08e3733c4
Namespace:	<none>
Created:	23 hours ago
Labels:		<none>
Annotations:	images.openshift.io/deny-execution=true <1>
		openshift.io/image.managed=true
		security.manageiq.org/failed-policy=openscap policy <2>
Docker Image:	172.30.120.134:5000/testproject/rtest@sha256:b925ddb55063d5f26526ca09e2f55aec5a8c4e95a7e4e4b644dd6ba08e3733c4
Image Size:	170.1 MB (first layer 73.86 MB, last binary layer 912.3 kB)
Image Created:	23 hours ago
Author:		<none>
Arch:		amd64
Entrypoint:	container-entrypoint
Command:	/usr/libexec/s2i/run
Working Dir:	/opt/app-root/src
User:		1001
Exposes Ports:	8080/tcp
Docker Labels:	architecture=x86_64
		authoritative-source-url=registry.access.redhat.com <3>
		build-date=2017-04-21T09:41:29.844044
		com.redhat.build-host=ip-10-29-120-102.ec2.internal
		com.redhat.component=rh-ruby23-docker
		description=The Red Hat Enterprise Linux Base image is designed to be a fully supported foundation for your containerized applications.  This base image provides your operations and application teams with the packages, language runtimes and tools necessary to run, maintain, and troubleshoot all of your applications. This image is maintained by Red Hat and updated regularly. It is designed and engineered to be the base layer for all of your containerized applications, middleware and utilites. When used as the source for all of your containers, only one copy will ever be downloaded and cached in your production environment. Use this image just like you would a regular Red Hat Enterprise Linux distribution. Tools like yum, gzip, and bash are provided by default. For further information on how this image was built look at the /root/anacanda-ks.cfg file.
		distribution-scope=public
		io.k8s.description=Platform for building and running Ruby 2.3 applications
		io.k8s.display-name=testproject/rtest-1:00972bc1
		io.openshift.build.commit.author=Ionut Palade <PI-Victor@users.noreply.github.com>
		io.openshift.build.commit.date=Mon Dec 12 14:37:32 2016 +0100
		io.openshift.build.commit.id=855ab2de53ff897a19e1055f7554c64d19e02c50
		io.openshift.build.commit.message=Merge pull request #6 from aj07/typo
		io.openshift.build.commit.ref=master
		io.openshift.build.image=registry.access.redhat.com/rhscl/ruby-23-rhel7@sha256:4b496b8b4d306badbea387f790004f867ca774526c17fb0fffdc88d58384c495 <4>
		io.openshift.build.source-location=https://github.com/openshift/ruby-ex.git
		io.openshift.expose-services=8080:http
		io.openshift.s2i.scripts-url=image:///usr/libexec/s2i
		io.openshift.tags=builder,ruby,ruby23,rh-ruby23
		io.s2i.scripts-url=image:///usr/libexec/s2i
		name=rhscl/ruby-23-rhel7 <5>
		release=6.7 <6>
		summary=Platform for building and running Ruby 2.3 applications
		vcs-ref=368e1c5301205f920e5a1ad00b075878d6cd3d54
		vcs-type=git
		vendor=Red Hat, Inc.
		version=2.3
Environment:	OPENSHIFT_BUILD_NAME=rtest-1
		OPENSHIFT_BUILD_NAMESPACE=testproject
		OPENSHIFT_BUILD_SOURCE=https://github.com/openshift/ruby-ex.git
		OPENSHIFT_BUILD_REFERENCE=master
		OPENSHIFT_BUILD_COMMIT=855ab2de53ff897a19e1055f7554c64d19e02c50
		PATH=/opt/app-root/src/bin:/opt/app-root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
		container=oci
		STI_SCRIPTS_URL=image:///usr/libexec/s2i
		STI_SCRIPTS_PATH=/usr/libexec/s2i
		HOME=/opt/app-root/src
		BASH_ENV=/opt/app-root/etc/scl_enable
		ENV=/opt/app-root/etc/scl_enable
		PROMPT_COMMAND=. /opt/app-root/etc/scl_enable
		RUBY_VERSION=2.3
----
<1> This is a special annotation that was put in place by the container scan
<2> This denotes which policy was used for the failed scan
<3> Where did the image come from? link:https://github.com/projectatomic/ContainerApplicationGenericLabels[This and more labels are explained here]
<4> What builder image was used to create this image?
<5> The docker name of the builder image
<6> The release version of the used builder image

// Consider loading the image from the registry and analyse with docker commands


* _Labels_: There are no OpenShift Labels on this object. If there were, you could use the -l "label=value" Option with the oc command line tool to select this object. As it is here, you can only use the long id that is found in the name field.

* _Annotations_: There are special annotations on this image. The highlighted annotations *images.openshift.io/deny-execution=true* and 
*security.manageiq.org/failed-policy=openscap policy* are put into the image metadata by the security scan that we triggered in CloudForms. These annotations to document that there were "important" security issues found. You can the annotations on the OpenShift side as we will later.

* _Docker Labels_: There are labels on the image that speak for it's origin. Look at vendor, name and release. The special OpenShift Label io.openshift.build.image notes the parent image. The *io.openshift.build.image* tells us that the rthest container image was built using the *rhscl/ruby-23-rhel7* builder image from the Red Hat Registry. Further down you find the *release* label that tells us that we used a builder image that was tagged with the docker label release=6.7.

//Open TODO Explain S2I - Source 2 Image Builds.

Lets find the image in the link:https://access.redhat.com/containers[Red Hat Container Catalog]. Our Red Hat Container Catalog provides detailed information about the images that we as Red Hat provide. We do document known issues and the fixes once they become available. We recently added a "Health Index" to deliver an easy first impression about the freshness of an image. 

. Got to link:https://access.redhat.com/containers[https://access.redhat.com/containers].
+
. Fill out the search field, put in *rhscl/ruby-23* and hit return. 
+
You can see that we do have a newer release available than 6.7. Back when I did this, it was 6.8 as you can see in the screen shot below. You will need to click on "Tags" to get to that same view.
+
image::http://people.redhat.com/~llange/labimg/RHCC-ruby-23-with-tags.png[Red Hat Container Catalog Ruby 2.3 Image Tags Tab]
+
. Click the tag "link:https://access.redhat.com/containers/#/registry.access.redhat.com/rhscl/ruby-23-rhel7/images/2.3-6.7[2.3-6.7]" in your list to get to the details about the builder image that we were using.

image::http://people.redhat.com/~llange/labimg/RHCC-ruby-23-67-details.png[Red Hat Container Calalog Ruby 2.3-6.7 Details]

Lets take a closer look at the details in the 2.3-6.7 release of the rhscl/ruby-23-rhel7 builder image. The screen shot above lists the health index as B. It has an explanation on the side what this means. 

[NOTE]
.Health Index Level B
====
This image is affected by Critical ( no older than 7 days ) or Important ( no older than 30 days ) security updates
====

Also note that there is an update builder image available that fixes issues found in the release 6.7. It might be the case that not all issues are fixed in the latest available image. Red Hat is building new container images in a scheduled fashion. That is why we might not have a certain fix in the latest image.  We do divert from our scheduled build and do async updates for critical updates only.

=== Repair known Security Issues 

We just found out that there is a newer version of the ruby builder image available in the Red Hat Registry. Lets update the s2i builder Image to the latest available version to fix security issues. Images are managed in ImageStreams in OpenShift. So lets take a look at the ruby ImageStream before we go and let OpenShift get the newest version of the ruby builder image.

----
$ oc get is ruby -n openshift
NAME      DOCKER REPO                          TAGS                         UPDATED
ruby      172.30.120.134:5000/openshift/ruby   2.2,2.0,latest + 1 more...   3 weeks ago
----

Or with a lot more detail :

[subs=+macros]
----
$ oc describe is ruby -n openshift
Name:			ruby
Namespace:		openshift
Created:		3 weeks ago
Labels:			<none>
Annotations:		openshift.io/display-name=Ruby
			openshift.io/image.dockerRepositoryCheck=2017-05-16T13:09:06Z
Docker Pull Spec:	172.30.120.134:5000/openshift/ruby
pass:quotes[*Unique Images:		3*]
Tags:			4

2.3 (latest)
  tagged from registry.access.redhat.com/rhscl/ruby-23-rhel7:latest

  Build and run Ruby 2.3 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/blob/master/2.3/README.md.
  Tags: builder, ruby
  Supports: ruby:2.3, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

pass:quotes[  * *registry.access.redhat.com/rhscl/ruby-23-rhel7@sha256:4b496b8b4d306badbea387f790004f867ca774526c17fb0fffdc88d58384c495*]
      3 weeks ago

2.2
  tagged from registry.access.redhat.com/rhscl/ruby-22-rhel7:latest

  Build and run Ruby 2.2 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/tree/master/2.2/README.md.
  Tags: builder, ruby
  Supports: ruby:2.2, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

  * registry.access.redhat.com/rhscl/ruby-22-rhel7@sha256:f8b0adc1bdb409e0cfbaa39870077c4944eb52b8e222551ef3146eddf1c9e6cb
      3 weeks ago

2.0
  tagged from registry.access.redhat.com/openshift3/ruby-20-rhel7:latest

  Build and run Ruby 2.0 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/tree/master/2.0/README.md.
  Tags: hidden, builder, ruby
  Supports: ruby:2.0, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

  * registry.access.redhat.com/openshift3/ruby-20-rhel7@sha256:9cfdf4b811ace13d4c555335b249ab831832a384113035512abc9d4d5cc59716
      3 weeks ago
-
----

Notice the bold lines above. We do have *3 unique images* referenced by the ruby ImageStream currently. There is only one image listed for the tag 2.3.

==== Update the Builder Image

* The command below will update the ruby ImageStream and load the latest container image tagged ruby:2.3 from the Red Hat Registry.
* An update to the ruby ImageStream will have an effect for you rtest application. Your BuildConfig for the rtest application is setup to watch the openshift/ruby:2.3 image stream for new Images. The update will trigger a new s2i build.
* The result of that build will be a new version of your rtest application in container image format. 
* This will be pushed into the internal OpenShift registry. 
* The DeployementConfig of your rtest application watches the rtest ImageStream for new versions and will trigger a new deployment in turn. 

*You can watch all of this happening in your environment if you are quick enough*

[subs=+macros]
----
$ oc import-image ruby:2.3 -n openshift
The import completed successfully.

Name:			ruby
Namespace:		openshift
Created:		3 weeks ago
Labels:			<none>
Annotations:		openshift.io/display-name=Ruby
			openshift.io/image.dockerRepositoryCheck=2017-06-13T11:19:06Z
Docker Pull Spec:	172.30.120.134:5000/openshift/ruby
pass:quotes[*Unique Images:		4*]
Tags:			4

2.3 (latest)
  tagged from registry.access.redhat.com/rhscl/ruby-23-rhel7:latest

  Build and run Ruby 2.3 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/blob/master/2.3/README.md.
  Tags: builder, ruby
  Supports: ruby:2.3, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

pass:quotes[  * *registry.access.redhat.com/rhscl/ruby-23-rhel7@sha256:3539e468222542cbea0c127927db191c2bd823e134ab241de971c2f14fed5fc7
      Less than a second ago*]
    registry.access.redhat.com/rhscl/ruby-23-rhel7@sha256:4b496b8b4d306badbea387f790004f867ca774526c17fb0fffdc88d58384c495
      3 weeks ago

2.2
  tagged from registry.access.redhat.com/rhscl/ruby-22-rhel7:latest

  Build and run Ruby 2.2 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/tree/master/2.2/README.md.
  Tags: builder, ruby
  Supports: ruby:2.2, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

  * registry.access.redhat.com/rhscl/ruby-22-rhel7@sha256:f8b0adc1bdb409e0cfbaa39870077c4944eb52b8e222551ef3146eddf1c9e6cb
      3 weeks ago

2.0
  tagged from registry.access.redhat.com/openshift3/ruby-20-rhel7:latest

  Build and run Ruby 2.0 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/tree/master/2.0/README.md.
  Tags: hidden, builder, ruby
  Supports: ruby:2.0, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

  * registry.access.redhat.com/openshift3/ruby-20-rhel7@sha256:9cfdf4b811ace13d4c555335b249ab831832a384113035512abc9d4d5cc59716
      3 weeks ago
----

Notice that we now have 4 unique images and that there is a new image for the 2.3 tag that was synced less than a second ago.

==== Watch the new Build 

.See the build in OpenShift
====
. Go to the Overview Page of the "Test Project" and see the new build running. ( you need to be quick )

image::http://people.redhat.com/~llange/labimg/Image-Update-rtest-build-triggered.png[A new build of the rtest container image was triggerd]

. Watch the build on the the command line :

[subs=+macros]
----
# oc get bc rtest
NAME      TYPE      FROM      LATEST
rtest     Source    Git       2

[root@master ~]# oc describe bc
Name:		rtest
Namespace:	testproject
Created:	About an hour ago
Labels:		app=rtest
Annotations:	openshift.io/generated-by=OpenShiftNewApp
Latest Version:	2

Strategy:	Source
URL:		https://github.com/openshift/ruby-ex
From Image:	ImageStreamTag openshift/ruby:2.3 <1>
Output to:	ImageStreamTag rtest:latest

Build Run Policy:	Serial
Triggered by:		Config, ImageChange <2>
Webhook GitHub:
	URL:	https://master.example.com:8443/oapi/v1/namespaces/testproject/buildconfigs/rtest/webhooks/bIqI1y4ETX7uADNm-PMo/github
Webhook Generic:
	URL:		https://master.example.com:8443/oapi/v1/namespaces/testproject/buildconfigs/rtest/webhooks/aeS1J1OTCInI4Fv4kQMh/generic
	AllowEnv:	false

Build		Status		Duration	Creation Time
rtest-2 	complete 	2m29s 		2017-06-13 07:19:06 -0400 EDT
rtest-1 	complete 	2m17s 		2017-06-13 06:24:44 -0400 EDT
----

<1> This is the reference to the *openshift/ruby:2.3* ImageStream
<2> Here you see what kind of triggers are configured for this buildconfig

This is the BuildConfig that is watching the builder image ImageStream and start a new build automatically if an image change is detected.
====

==== Inspect the new Deployment

The new build did put a new image into the internal registry. There is an rtest ImageStream in the testproject that is used by the rrtest DeploymentConfig to watch for ImageChanges and trigger new deployements.

. Look up the rtest deployment in the WebUI : 
.. Select andrews "Test Project"
.. Select "Applications"
.. Click "Deployements" in the sub menu
.. Click "rtest" in the list of deployments
+
image::http://people.redhat.com/~llange/labimg/Image-Update-rtest-deployment-3.png[rtest deployment]
+
. Look at your rtest DeploymentConfig on the cmd :
+
----
[root@master ~]# oc describe dc
Name:		rtest
Namespace:	testproject
Created:	2 hours ago
Labels:		app=rtest
Annotations:	openshift.io/generated-by=OpenShiftNewApp
Latest Version:	3
Selector:	app=rtest,deploymentconfig=rtest
Replicas:	1
Triggers:	Config, Image(rtest@latest, auto=true)
Strategy:	Rolling
Template:
  Labels:	app=rtest
		deploymentconfig=rtest
  Annotations:	openshift.io/generated-by=OpenShiftNewApp
  Containers:
   rtest:
    Image:			172.30.120.134:5000/testproject/rtest@sha256:828d41ae8c7044026732d2092734b312a27044241c23238f3a01525ad5a606c2
    Port:			8080/TCP
    Liveness:			http-get http://:8080/health delay=0s timeout=1s period=10s #success=1 #failure=3
    Volume Mounts:		<none>
    Environment Variables:	<none>
  No volumes.

Deployment #3 (latest):
	Name:		rtest-3
	Created:	45 minutes ago
	Status:		Complete
	Replicas:	1 current / 1 desired
	Selector:	app=rtest,deployment=rtest-3,deploymentconfig=rtest
	Labels:		app=rtest,openshift.io/deployment-config.name=rtest
	Pods Status:	1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Deployment #2:
	Created:	about an hour ago
	Status:		Complete
	Replicas:	0 current / 0 desired
Deployment #1:
	Created:	2 hours ago
	Status:		Complete
	Replicas:	0 current / 0 desired
...

----
+
Both outputs above display that you are running on deployment No. 3 now. You can see in the Web UI that the last deployment was triggered by an image change. To see the same information on the command line you would need to take a look at the oc rollout command :
+
----
# oc rollout --help
...

# oc rollout history dc/rtest
deploymentconfigs "rtest"
REVISION	STATUS		CAUSE
1		Complete	image change
2		Complete	config change
3		Complete	image change

----
+
For more information about Deployments take a look at the link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#deployments[OpenShift Developer Guide Deployements Chapter].

If you know recent Kubernetes versions, you will have come across the Deployment Object. The OpenShift DeploymentConfig Objects are far more advanced than the Kubernetes Deployements. Read about the limitations link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-kubernetes-deployments-support[here]. You can expect the Kubernetes Deployments to get more and more features as we are upstreaming the work done in OpenShift.

==== Image Details (again) 

Which release of the ruby builder image was used to build the latest version of the rtest container image? You can check this in 3 different places, look for the release label in one of them :

. In CloudForms - Compute -> Containers -> Conainer Images - search rtest
. In the link:https://registry-console-default.cloudapps.example.com/registry[Registry Console]
. on the command line through oc describe is rtest and oc describe images $IMID 

----
# oc describe is rtest -n testproject | grep sha | head -1 
  * 172.30.120.134:5000/testproject/rtest@sha256:828d41ae8c7044026732d2092734b312a27044241c23238f3a01525ad5a606c2
# oc describe images sha256:828d41ae8c7044026732d2092734b312a27044241c23238f3a01525ad5a606c2 | grep release
		release=6.8
----


// verify the changed release number with Registy Console


// think about adding the schedule / policy before doing this manually again
==== [Optional] Rescan the new container images

Go to the CloudForms interface and schedule another image scam for the newly created rtest image. Notice that you will need to know your sha256 ID to identify the correct images quickly. In my case this sha256:828d... 

. Find the new rtest Image
.. Compute -> Containers -> Container Images -> Search "rtest"
.. Click the correct Container Image identified by the sha256:...
. Assign the OpenSCAP Policy Profile
.. Open the Menu "Policy" 
.. Click "Manage Policies"
.. Select check box "OpenSCAP profile"
.. Click save
. Schedule Container Scan
.. Click "Configuration" Menu
.. Select "Perform SmartState Analysis"
.. Confirm "Perform SmartState Analysis on this item"
.Wait a few minutes and reload the page. If the scan does not start at all, close your browser completely, start it again and schedlue the smart state analysis again. 
.Load the OpenSCAP html information by clicking the "OpenSCAP HTML" line
.. select to open in your browser
.. scoll down and deselect the check box "pass"

image::http://people.redhat.com/~llange/labimg/OpenSCAP-scan-results-in-cf-rtest-2.png[]

Notice that we have fewer issues then before. The false finding with the aarch64 issue is still in and you can continue to ignore it. The link:https://access.redhat.com/errata/RHSA-2017:1308[RHSA-2017:1308] issue is gone. The nss security bug link:https://access.redhat.com/errata/RHSA-2017:1365[RHSA-2017-1365] is still present as was to be expected. We saw that this issue was not fixed with the link:https://access.redhat.com/containers/#/registry.access.redhat.com/rhscl/ruby-23-rhel7/images/2.3-6.8[6.8 release of our rhscl/ruby-23-rhel7] builder image when we looked it up in the link:https://access.redhat.com/containers[Red Hat Container Catalog].

==== [Optional] Get the latest ruby builder image automatically 

Could OpenShift get the latest builder image as soon as it becomes available? Yes it can!

----
# oc tag is ruby:2.3 -n openshift --scheduled=true
Tag ruby:2.3 set to import is periodically.
----

This will set the ImportPolicy on the Image in the ImageStream Ruby that is tagged with 2.3. OpenShift will fetch new versions of this builder image every 15 min if the are available. The interval can be set in the master-config.yaml. The keyword is scheduledImageImportMinimumIntervalSeconds and defaults to 900 if it is not specified. 

////
Create imagestream from a docker image and tell it to store locally in the internal OCP registry:
----
oc tag --reference-policy=local --source=docker docker.io/image:tag myimagestream:tag
----
Schedule the imagestream to track new image changes in the external registry
----
oc tag --scheduled=true --source=docker docker.io/image:tag myimagestream:tag
----
////


=== [Optional] Continued Security in Container Environments 

This section will show you how you can examine your Container Images on a regular basis, and how you OpenShift can consume this information to prevent execution of additional workloads when they are found to be vulnerable.

==== [Optional] Create a Schedule for regular Security Checks  

It is usually not sufficient to trigger container scans manually. If you build and check an image today and it is found to be good, this can change tomorrow or in a week or a year as new vulnerabilities are discovered. To address this, you want to schedule security scans at regular interval. You can do this with CloudForms. 

. First assign the OpenSCAP Policy not to a single image, but on the Container Provider Level.
.. Go to Compute -> Container -> Providers
.. Select the "OpenShift Cluster 1 Provider" checkbox
.. Click the Policy Button
.. Click "Manage Policies"
+
image::http://people.redhat.com/~llange/labimg/CloudForms-Manage-Policies-for-Providers.png[Manage Provider Policies] 
+
.. Select the "OpenSCAP profile" checkbox
+
image::http://people.redhat.com/~llange/labimg/CloudForms-Manage-Policies-for-Providers-OpenSCAP.png[Manage Provider Policies assign OpenSCAP Profile]
+
.. Click the "Save" button.

. Create a Schedule to scan all images in your internal registry.
.. Open the EVM Menu on the upper right corner of the CloudForms Interfach
.. Click "Configuration"
+
image::http://people.redhat.com/~llange/labimg/CloudForms-Add-Schedule-1.png[]
+
.. Make sure to select "Schedules" in the Setting Part of the Accordion. 
.. Open the "Configuration" Menu
.. Click "Add Schedule"
+
image::http://people.redhat.com/~llange/labimg/CloudForms-Add-Schedule-2.png[]
+
.. Fill in the Details for you Schedule as in the screen shot below. But select a time ~10 min from now if you want to see CloudForms scheduling scans.
+
image::http://people.redhat.com/~llange/labimg/CloudForms-Create-Scanning-Schedule-3.png[]
+
.. Click the "Add" button to add the schedule.

Now wait for that schedule to start scanning all your container images.

==== [Optional] Prevent the starting of vulnerable Workloads

It is possible to use the annotations that the Container Scan puts into the Container Image Metadata on the OpenShift side to prevent starting more vulnerable workloads. You can consult the documentation for more background :

* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/container_security_guide/#controlling-pod-execution[Documentation on Controlling Pod Executon]

* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/cluster_administration/#admin-guide-image-policy[ImagePolicy Documentation]

. Before we implement this safe guard, verify that you can scale up your rtest application. This can be done easily from the Web UI by increasing the number of pods in the Project Overview, or in the DeploymentConfig. 
+
.Scale rtest in the Web UI Project Overview 
image::http://people.redhat.com/~llange/labimg/OpenShift-Scaling-rtest-Project-Overview-level.png[]
+
.Scale rtest in the Deployment View
image::http://people.redhat.com/~llange/labimg/OpenShift-Scaling-rtest-dc-level.png[]
+
. You can do the same on the command line with :
+
.Scale rtest in the command line
----
# oc get rc 
NAME      DESIRED   CURRENT   READY     AGE
rtest-1   0         0         0         19h
rtest-2   0         0         0         19h
rtest-3   0         0         0         19h
rtest-4   2         2         2         5m
----
+
----
# oc scale dc/rtest --replicas=3
deploymentconfig "rtest" scaled
----
+
----
# oc get rc 
NAME      DESIRED   CURRENT   READY     AGE
rtest-1   0         0         0         20h
rtest-2   0         0         0         19h
rtest-3   0         0         0         19h
rtest-4   3         3         3         7m
----
+
----
# oc get pods
[root@master ~]# oc get pods
NAME            READY     STATUS      RESTARTS   AGE
rtest-1-build   0/1       Completed   0          20h
rtest-2-build   0/1       Completed   0          19h
rtest-4-1mzg0   1/1       Running     0          2m
rtest-4-crlfp   1/1       Running     0          5m
rtest-4-s6bmz   1/1       Running     0          7m
----
+
You can see above that the rtest-4 deployment was scaled to 3 pods successfully. 
+
We can use the ImagePolicy settings in the `master-config.yaml` of the OpenShift Master to instruct OpenShift not to start any workloads that have a certain annotation. Already running workloads would not be affected by this unless you scale them down. Scaling up counts as starting new workloads in this case. 
+
. Make sure the following settings are found in your master-config.yaml. Be sure to get the indentation right.
+
.ImagePolicy in /etc/origin/master/master-config.yaml
[subs=+macros]
----
admissionConfig:
  pluginConfig:   
pass:quotes[    *openshift.io/ImagePolicy:
      configuration:
        kind: ImagePolicyConfig
        apiVersion: v1
        resolveImages: AttemptRewrite
        executionRules:
        - name: execution-denied
          onResources:
          - resource: pods
          - resource: builds
          reject: true
          matchImageAnnotations:
          - key: images.openshift.io/deny-execution
            value: "true"
          skipOnResolutionFailure: true
        - name: allow-images-from-internal-registry
          # allows images from the internal registry and tries to resolve them
          onResources:
          - resource: pods
          - resource: builds
          matchIntegratedRegistry: true
        - name: allow-images-from-dockerhub
          onResources:
          - resource: pods
          - resource: builds
          matchRegistries:
          - docker.io*]
    BuildDefaults:
...
----
+
. Restart the atomic-openshift-master service
+
----
$ systemctl restart atomic-openshift-master
----
+
. try scaling up your rtest application now.
+
.Note that you fail and scaling seems to be stuck 
image::http://people.redhat.com/~llange/labimg/OpenShift-Scaling-rtest-Project-Overview-level-not-working.png[]
+
. Lets us dig for an error message about what is going on
.. Go to Monitoring in the Web UI
.. Find the Events section on the top left
.. Click "View Details"
+
.This tells why you can't scale up any more
image::http://people.redhat.com/~llange/labimg/OpenShift-Scaling-rtest-prevented-event-view.png[]

The message reads : "Forbidden: this image is prohibited by policy". 

////

### Commands only section
#!/bin/bash
oc project default

echo "Deleting testproject"
oc get project testproject &>/dev/null && oc delete testproject && { echo "Sleeping 120" ; sleep 120 ; }

echo "Creating testproject"
oc new-project testproject --description="My Test Project" --display-name="Test Project"

echo "Sleeping 5" ; sleep 5

echo "Creating rtest app"
oc new-app openshift/ruby:2.3~https://github.com/openshift/ruby-ex --name=rtest

echo "Creating route"
oc expose service rtest

echo "adding a liveliness check"
oc set probe dc/rtest --liveness --get-url=http://:8080/health

echo "TODO scheduling a container scan via CF API"
echo "Triggering Provider Refresh"
echo "Waiting for Provider Refresh"
echo "finding the rtest container image in CF"
echo "Setting the OpenSCAP Policy Profile on the rtest container image"
echo "Requesting Container Scan / Smart State Analysis for rtest container image"

echo "Get latest ruby builder image"
oc import-image is ruby:2.3 -n openshift

echo "Display Rollout Histroy"
oc rollout history dc/rtest


////

////
=== Creating a Container based Service in CloudForms

=== Red Hat Insight for Containers
////

== [Optional] Container Native Storage Lab

=== Overview

In this section you will set up container-native storage (CNS) in your environment. You will use this to dynamically provision storage for containerized applications. It is provided by GlusterFS running in containers. +
GlusterFS in turn is backed by local storage available to the OpenShift nodes.

NOTE: All of the following tasks are carried out as root from the master node. All files created can be stored in root's home directory unless a particular path is specified. At the end of this section you will have 3 GlusterFS pods running together with the heketi API frontend properly integrated into OpenShift.

=== Deploying Container-native Storage

Make sure you are logged on to the master node.

....
[root@master ~]# hostname
master.example.com
....

First, as the root user, install the CNS deployment tool. +
We will also install ansible. Though not needed for CNS in this lab it will help us simplify an otherwise tedious manual configuration step.

 [root@master ~]# yum -y install cns-deploy ansible

'''
==== Configure OpenShift Node firewall with Ansible

NOTE: In the following section we will configure Ansible. We will use it's configuration management capabilities in order to make sure all the OpenShift nodes have the right firewall settings.

.Ansible setup
====
Replace the content of the Ansible inventory in `/etc/ansible/hosts` with the following

[source,ini]
./etc/ansible/hosts
----
[master]
master.example.com

[nodes]
node1.example.com
node2.example.com
node3.example.com
----

You should now be able to ping all hosts using Ansible
....
[root@master ~]# ansible nodes -m ping

node3.example.com | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
node2.example.com | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
node1.example.com | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
....

Create a file called `configure-firewall.yml` and copy&paste the following contents:
[source,yaml]
.configure-firewall.yml
----
---

- hosts: nodes

  tasks:

    - name: insert iptables rules required for GlusterFS
      blockinfile:
        dest: /etc/sysconfig/iptables
        block: |
          -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT
          -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT
          -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT
          -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m multiport --dports 49152:49664 -j ACCEPT
        insertbefore: "^COMMIT"

    - name: reload iptables
      systemd:
        name: iptables
        state: reloaded
----

Done. This little helper construct will save us some work in configuring the firewall. Run it with the following command:

 [root@master ~]# ansible-playbook configure-firewall.yml

Your output should look like this.

....
PLAY [nodes] *******************************************************************

TASK [setup] *******************************************************************
ok: [node2.example.com]
ok: [node1.example.com]
ok: [node3.example.com]

TASK [insert iptables rules required for GlusterFS] ****************************
changed: [node3.example.com]
changed: [node2.example.com]
changed: [node1.example.com]

TASK [reload iptables] *********************************************************
changed: [node2.example.com]
changed: [node1.example.com]
changed: [node3.example.com]

PLAY RECAP *********************************************************************
node1.example.com          : ok=3    changed=2    unreachable=0    failed=0
node2.example.com          : ok=3    changed=2    unreachable=0    failed=0
node3.example.com          : ok=3    changed=2    unreachable=0    failed=0
....
====

'''
With this we checked the requirement for additional firewall ports to be opened on the OpenShift app nodes.

==== Prepare OpenShift for CNS

Next we will create a namespace (also referred to as a _Project_) in OpenShift. It will be used to group the GlusterFS pods.
For this you need to be logged as an admin user in OpenShift.

....
[root@master ~]# oc whoami
system:admin
....

If you are for some reason not an admin, login as system admin like this:

 [root@master ~]# oc login -u system:admin -n default

Create a namespace with a designation of your choice. In this example we will use `container-native-storage`.

 [root@master ~]# oc new-project container-native-storage

GlusterFS pods need access to the physical block devices on the host. Hence they need elevated permissions. Enable containers to run in privileged mode.

 [root@master ~]# oadm policy add-scc-to-user privileged -z default

==== Describe Container-native Storage Topology

CNS will virtualize locally attached block storage on the OpenShift App nodes. In order to deploy you will need to supply the installer with information about where to find these nodes and what network and which block devices to use. +
This is done using JSON file describing the topology of your OpenShift deployment.

For this purpose, create the file topology.json with the following content:
[source,json]
.topology.yml
----
{
    "clusters": [
        {
            "nodes": [
                {
                    "node": {
                        "hostnames": {
                            "manage": [
                                "node1.example.com"
                            ],
                            "storage": [
                                "192.168.0.102"
                            ]
                        },
                        "zone": 1
                    },
                    "devices": [
                        "/dev/vdc"
                    ]
                },
                {
                    "node": {
                        "hostnames": {
                            "manage": [
                                "node2.example.com"
                            ],
                            "storage": [
                                "192.168.0.103"
                            ]
                        },
                        "zone": 2
                    },
                    "devices": [
                        "/dev/vdc"
                    ]
                },
                {
                    "node": {
                        "hostnames": {
                            "manage": [
                                "node3.example.com"
                            ],
                            "storage": [
                                "192.168.0.104"
                            ]
                        },
                        "zone": 3
                    },
                    "devices": [
                        "/dev/vdc"
                    ]
                }
            ]
        }
    ]
}
----

This file contains an additional property called `zone` per node. This identifies the failure domain. In CNS data is always replicated 3 times. Failure domains make sure that two copies are never stored on nodes in the same failure domain.

==== Deploy Container-native Storage

You are now ready to deploy CNS. Alongside GlusterFS pods the API front-end known as *heketi* is deployed. This protects the API from unauthorized access we will define passwords for the `admin` and `user` role in heketi like below.

.CNS passwords
[width="60%",options="header"]
|==============================================
| Heketi Role     | Password
| admin           | myS3cr3tpassw0rd
| user            | mys3rs3cr3tpassw0rd
|==============================================

Next start the deployment routine with the following command:

 [root@master ~]# cns-deploy -n container-native-storage -g topology.json --admin-key 'myS3cr3tpassw0rd' --user-key 'mys3rs3cr3tpassw0rd'

Answer the interactive prompt with *Y*.

The deployment will take several minutes to complete (especially waiting for the GlusterFS pods will take 2-3 minutes). +
You may want to monitor the progress in parallel also in the OpenShift UI in the `container-native-storage` project. +
On the command line the output should look like this:

----
Welcome to the deployment tool for GlusterFS on Kubernetes and OpenShift.

Before getting started, this script has some requirements of the execution
environment and of the container platform that you should verify.

The client machine that will run this script must have:
 * Administrative access to an existing Kubernetes or OpenShift cluster
 * Access to a python interpreter 'python'
 * Access to the heketi client 'heketi-cli'

Each of the nodes that will host GlusterFS must also have appropriate firewall
rules for the required GlusterFS ports:
 * 2222  - sshd (if running GlusterFS in a pod)
 * 24007 - GlusterFS Daemon
 * 24008 - GlusterFS Management
 * 49152 to 49251 - Each brick for every volume on the host requires its own
   port. For every new brick, one new port will be used starting at 49152. We
   recommend a default range of 49152-49251 on each host, though you can adjust
   this to fit your needs.

In addition, for an OpenShift deployment you must:
 * Have 'cluster_admin' role on the administrative account doing the deployment
 * Add the 'default' and 'router' Service Accounts to the 'privileged' SCC
 * Have a router deployed that is configured to allow apps to access services
   running in the cluster

Do you wish to proceed with deployment?

[Y]es, [N]o? [Default: Y]: <1>
Using OpenShift CLI.
NAME                       STATUS    AGE
container-native-storage   Active    28m
Using namespace "container-native-storage".
Checking that heketi pod is not running ... OK
template "deploy-heketi" created
serviceaccount "heketi-service-account" created
template "heketi" created
template "glusterfs" created
role "edit" added: "system:serviceaccount:container-native-storage:heketi-service-account"
node "node1.example.com" labeled <2>
node "node2.example.com" labeled <2>
node "node3.example.com" labeled <2>
daemonset "glusterfs" created
Waiting for GlusterFS pods to start ... OK <3>
service "deploy-heketi" created
route "deploy-heketi" created
deploymentconfig "deploy-heketi" created
Waiting for deploy-heketi pod to start ... OK
Creating cluster ... ID: 307f708621f4e0c9eda962b713272e81
Creating node node1.example.com ... ID: f60a225a16e8678d5ef69afb4815e417 <4>
Adding device /dev/vdc ... OK <5>
Creating node node2.example.com ... ID: 13b7c17c541069862d7e66d142ab789e <4>
Adding device /dev/vdc ... OK <5>
Creating node node3.example.com ... ID: 5a6fbe5eb1864e711f8bd9b0cb5946ea <4>
Adding device /dev/vdc ... OK <5>
heketi topology loaded.
Saving heketi-storage.json
secret "heketi-storage-secret" created
endpoints "heketi-storage-endpoints" created
service "heketi-storage-endpoints" created
job "heketi-storage-copy-job" created
deploymentconfig "deploy-heketi" deleted
route "deploy-heketi" deleted
service "deploy-heketi" deleted
job "heketi-storage-copy-job" deleted
pod "deploy-heketi-1-599rc" deleted
secret "heketi-storage-secret" deleted
service "heketi" created
route "heketi" created
deploymentconfig "heketi" created <6>
Waiting for heketi pod to start ... OK
heketi is now running.
Ready to create and provide GlusterFS volumes.
----
<1> Enter *Y* and press Enter.
<2> OpenShift nodes are labeled. Label is referred to in a DaemonSet.
<3> GlusterFS daemonset is started. DaemonSet means: start exactly *one* pod per node.
<4> All nodes will be referenced in heketi's database by a UUID.
<5> Node block devices are formatted for mounting by GlusterFS.
<6> heketi is deployed in a pod as well.


==== Verifying the deployment

You now have deployed CNS. Let's verify all components are in place. While still in the `container-native-storage` project on the CLI list all running pods.

----
[root@master ~]# oc get pods -o wide
NAME              READY     STATUS    RESTARTS   AGE       IP              NODE
glusterfs-37vn8   1/1       Running   0          3m       192.168.0.102   node1.example.com <1>
glusterfs-cq68l   1/1       Running   0          3m       192.168.0.103   node2.example.com <1>
glusterfs-m9fvl   1/1       Running   0          3m       192.168.0.104   node3.example.com <1>
heketi-1-cd032    1/1       Running   0          1m       10.130.0.4      node3.example.com <2>
----
<1> GlusterFS pods, notice how all designated nodes run exactly one pod.
<2> heketi API frontend pod

NOTE: The exact pod names will be different in your environment, since they are auto-generated.

The GlusterFS pods use the hosts network and disk devices to run the software-defined storage system. Hence they attached to the host's network. See schematic below for a visualization.

.GlusterFS pods in CNS in detail.
image::http://people.redhat.com/~llange/labimg/cns_diagram_pod.png[]

heketi is a component that will expose an API for GlusterFS to OpenShift. This allows OpenShift to dynamically allocate storage from CNS in a programmatic fashion. See below for a visualization. Note that for simplicity, in our example heketi runs on the OpenShift App nodes, not on the Infra node.

.heketi pod running in CNS
image::http://people.redhat.com/~llange/labimg/cns_diagram_heketi.png[]

To expose heketi's API a `service` named _heketi_ has been generated in OpenShift.

----
[root@master ~]# oc get service/heketi
NAME      CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
heketi    172.30.5.231   <none>        8080/TCP   31m
----

To also use heketi outside of OpenShift in addition to the service a route has been deployed:

[source,options="nowrap"]
----
[root@master ~]# oc get route/heketi
NAME      HOST/PORT                                               PATH      SERVICES   PORT      TERMINATION   WILDCARD
heketi    heketi-container-native-storage.cloudapps.example.com             heketi     <all>                   None
----

Hence, heketi will be available via:

Heketi Service URL:: http://heketi-container-native-storage.cloudapps.example.com

You may verify this with a trivial health check:

----
[root@master ~]# curl http://heketi-container-native-storage.cloudapps.example.com/hello
Hello from Heketi
----

=== Using Container-native Storage in OpenShift

==== Creating a StorageClass

OpenShift uses Kubernetes' PersistentStorage facility to dynamically allocate storage for applications. This is a fairly simple framework in which only 3 components exists: the storage provider, the storage volume and the request for a storage volume.

.OpenShift Storage Lifecycle
image::http://people.redhat.com/~llange/labimg/cns_diagram_pvc.png[]

OpenShift knows non-ephemeral storage as "persistent" volumes. This is storage that is decoupled from pod lifecycles.
Users can request such storage by submitting a *PersistentVolumeClaim* to the system, which carries aspects like desired capacity or access mode (shared, single, read-only).

A storage provider in the system is represented by a *StorageClass* and is referenced in the claim. Upon receiving the claim it talks to the API of the actual storage system to provision the storage. 

The storage is represented in OpenShift as a *PersistentVolume* which can directly be used by pods to mount it.

With these basics defined we can configure our system for CNS. First we will set up the credentials for CNS in OpenShift.

. Create an encoded value for the CNS admin user like below:
+
----
[root@master ~]# echo -n "myS3cr3tpassw0rd" | base64
bXlTM2NyM3RwYXNzdzByZA==
----
+
We will store this encoded value in an OpenShift secret. 
+
. Create a file called `cns-secret.yml` as per below:
+
[source,yaml]
.cns-secret.yml
----
apiVersion: v1
kind: Secret
metadata:
  name: cns-secret
  namespace: default
data:
  key: bXlTM2NyM3RwYXNzdzByZA== <1>
type: kubernetes.io/glusterfs
----
+
<1> 'key' contains base64-encoded version of 'myS3cr3tpassw0rd'
+
. Create the secret in OpenShift with the following command:
+
----
[root@master ~]# oc create -f cns-secret.yml
----
+
To represent CNS as a storage provider in the system you first have to create a StorageClass.
+
. Define the Storage Class by creating a file called `cns-storageclass.yml` which references the secret and the heketi URL shown earlier with the contents as below:
+
[source,yaml]
.cns-storageclass.yml
----
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: container-native-storage
  annotations:
    storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: "http://heketi-container-native-storage.cloudapps.example.com"
  restauthenabled: "true"
  restuser: "admin"
  volumetype: "replicate:3"
  secretNamespace: "default"
  secretName: "cns-secret"
----
+
. Create the StorageClass in OpenShift with the following command:
+
----
[root@master ~]# oc create -f cns-storageclass.yml
----
+
With these components in place the system is ready to dynamically provision storage capacity from Container-native Storage.

==== Requesting Storage

To get storage provisioned as a user you have to "claim" storage. The _PersistentVolumeClaim_ (PVC) basically acts a request to the system to provision storage with certain properties, like a specific capacity. +
Also the access mode is set here, where _ReadWriteOnce_ allows one container at a time to mount this storage.


. Create a claim by specifying a file called `cns-pvc.yml` with the following contents:
+
[source,yaml]
.cns-pvc.yml
----
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-container-storage
  annotations:
    volume.beta.kubernetes.io/storage-class: container-native-storage
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
----
+
With above PVC we are requesting 10 GiB of non-shared storage. Instead of _ReadWriteOnce_ you could also have specified _ReadWriteOnly_ (for read-only) and _ReadWriteMany_ (for shared storage).
+
. Submit the PVC to the system like so:
+
----
[root@master ~]# oc create -f cns-pvc.yml
persistentvolumeclaim "my-container-storage" created
----
+
. Look at the requests state with the following command:
+
----
[root@master ~]# oc get pvc
NAME                   STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
my-container-storage   Bound     pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8   10Gi       RWO           16s
----
+
NOTE: It may take up to 15 seconds for the claim to be in *bound*.
+
CAUTION: If the PVC is stuck in _PENDING_ state you will need to investigate. Run `oc describe pvc/my-container-storage` to see a more detailed explanation. Typically there are two root causes - the StorageClass is not properly setup (wrong name, wrong credentials, incorrect secret name, wrong heketi URL, heketi service not up, heketi pod not up...) or the PVC is malformed (wrong StorageClass, name already taken ...)
+
TIP: You can also do this step with the UI. If you like you can switch to an arbitrary project you have access to and go to the "Storage" tab. Select "Create" storage and make selections accordingly to the PVC described before.
+
When the claim was fulfilled successfully it is in the *Bound* state. That means the system has successfully (via the StorageClass) reached out to the storage backend (in our case GlusterFS). The backend in turn provisioned the storage and provided a handle back OpenShift. In OpenShift the provisioned storage is then represented by a _PersistentVolume_ (PV) which is _bound_ to the PVC. +
+
. Look at the PVC for these details:
+
----
[root@master ~]# oc describe pvc/my-container-storage
Name:		my-container-storage
Namespace:	container-native-storage
StorageClass:	container-native-storage <1>
Status:		Bound
Volume:		pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8 <2>
Labels:		<none>
Capacity:	10Gi
Access Modes:	RWO
No events.
----
<1> The StorageClass against which the PVC was submitted.
<2> The name of PV that has been created.
+
NOTE: The PV name will be different in your environment since it's automatically generated.
+
. Look at the corresponding PV by it's name:
+
----
[root@master ~]# oc describe pv/pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8
Name:		pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8
Labels:		<none>
StorageClass:	container-native-storage <1>
Status:		Bound
Claim:		container-native-storage/my-container-storage <2>
Reclaim Policy:	Delete <3>
Access Modes:	RWO <4>
Capacity:	10Gi <5>
Message:
Source:
    Type:		Glusterfs (a Glusterfs mount on the host that shares a pod's lifetime) <6>
    EndpointsName:	glusterfs-dynamic-my-container-storage
    Path:		vol_304670f0d50bf5aa4717a69652bd48ff
    ReadOnly:		false
No events.
----
<1> The StorageClass which provisioned this PV.
<2> The claim that initiated the provisioning.
<3> What happens to the storage when the PV object is deleted: here it's deleted as well.
<4> The desired access mode. RWO = ReadWriteOnce.
<5> The capacity of the provisioned storage.
<6> The type of storage: in our case GlusterFS as part of CNS.
+
TIP: Note that in earlier documentation you will find references to administrators  *pre-provisioning* PVs. Later PVCs would "pick up" a suitable PV by looking at it's capacity. This was needed for storage like NFS that does not have an API and therefore does not support *dynamic provisioning*. +
This kind of storage should not be used anymore as it requires manual intervention, risky capacity planning and incurs inefficient storage utilization.
+
. Release this storage capacity again, since it's in the wrong namespace anyway.
+
Storage is freed up by deleting the *PVC*. The PVC controls the lifecycle of the storage, not the PV.
+
IMPORTANT: Never delete PVs that are dynamically provided. They are only handles for pods mounting the storage. Storage lifecycle is entirely controlled via PVCs.
+
. Delete the storage by deleting the PVC like this:
+
----
 [root@master ~]# oc delete pvc/my-container-storage
----

==== Using non-shared storage for databases

Normally a user doesn't request storage with a PVC directly. Rather the PVC is integrated in a larger template that describe the entire application. Such examples ship with OpenShift out of the box.

TIP: The following steps can again also be done with the UI. For this purpose follow these steps:

'''
. Log on to a project you have access to and quota available
. next to the project's name select _Add to project_
. In the _Browse Catalog_ view select _Ruby_ from the list of programming languages
. Select the example app entitled _Rails + PostgreSQL (Persistent)_
. Optionally change the _Volume Capacity_ parameter to something greater than 1GiB, e.g. 15 GiB
. Select _Create_ to start deploying the app
. Select _Continue to Overview_ in the confirmation screen
. Back on the overview page select the deploymentconfig _postgresql_
. On the following page select _Actions_ > _Edit Health Checks_
. In the settings menu change the _Initial Delay_ values for both _Readiness Probe_ and _Liveliness Probe_ to 180 seconds

'''

Log on to the system as `marina` und create a project with an arbitrary name.

 [root@master ~]# oc login -u marina --insecure-skip-tls-verify --server=https://master.example.com:8443
 [root@master ~]# oc new-project my-test-project

To use some of the examples that ship with OpenShift enter the following command to export the template for a sample Ruby on Rails with PostgreSQL application:

 [root@master ~]# oc export template/rails-pgsql-persistent -n openshift -o yaml > rails-app-template.yml

In the file `rails-app-template.yml` you can now review the template for this entire application stack in all it's glory. In essence it creates Rails Application instance which mimics a very basic blogging application. The articles are saved in a PostgreSQL database which runs in another pod. In addition a PVC is issued (line 194) to supply this pod with persistent storage below the mount point /var/lib/pgsql/data (line 275).

We need to modify this template now. Open it in your favorite editor and increase the values for `initialDelaySeconds` in both sections (`livenessProbe` and `readinessProbe`), around lines 255 - 270:

[source,yaml]
.rails-app-template.yml
----
[...omitted...]

          livenessProbe:
            initialDelaySeconds: 180 <1>
            tcpSocket:
              port: 5432
            timeoutSeconds: 1
          name: postgresql
          ports:
          - containerPort: 5432
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -i
              - -c
              - psql -h 127.0.0.1 -U ${POSTGRESQL_USER} -q -d ${POSTGRESQL_DATABASE}
                -c 'SELECT 1'
            initialDelaySeconds: 180 <1>
            timeoutSeconds: 1
          resources:

[...omitted...]
----
<1> Set the _initialDelaySeconds_ value to 180 in both the livenessProbe and readinessProbe section

IMPORTANT: In production you don't have to change these values. Your test environment however is using nested virtualization and therefore has much lower performance than a production environment in the cloud or on-premise. Therefore the postgres container takes longer to initialize and would be declared unhealthy by OpenShift with the default delays when checking the container health.

Next we are going to create all the resources from the templates while passing in an additional parameter to override the default storage capacity requested from the PVC.

TIP: To list all available parameters from this template run `oc process -f rails-app-template.yml --parameters`

The parameter in the template is called `VOLUME_CAPACITY`. We will process the template with the CLI client and override this parameter with a value of _15Gi_ as follows:

 [root@master ~]# oc process -f rails-app-template.yml -o yaml -p VOLUME_CAPACITY=15Gi > my-rails-app.yml

The `oc process` command parses the template and replaces any parameters with their default values if not supplied explicitly like we did for the volume capacity.

The result `my-rails-app.yml` file contains all resources for this application ready to deploy, like so:

----
[root@master ~]# oc create -f my-rails-app.yml
secret "rails-pgsql-persistent" created
service "rails-pgsql-persistent" created
route "rails-pgsql-persistent" created
imagestream "rails-pgsql-persistent" created
buildconfig "rails-pgsql-persistent" created
deploymentconfig "rails-pgsql-persistent" created
persistentvolumeclaim "postgresql" created
service "postgresql" created
deploymentconfig "postgresql" created
----

You can now use the OpenShift UI (while being logged in as _marina_ in the newly created project) to follow the deployment process. Alternatively watch the containers deploy like this:

----
[root@master ~]# oc get pods -w
NAME                             READY     STATUS              RESTARTS   AGE
postgresql-1-deploy              0/1       ContainerCreating   0          11s
rails-pgsql-persistent-1-build   0/1       ContainerCreating   0          11s
NAME                  READY     STATUS    RESTARTS   AGE
postgresql-1-deploy   1/1       Running   0          14s
postgresql-1-81gnm   0/1       Pending   0         0s
postgresql-1-81gnm   0/1       Pending   0         0s
rails-pgsql-persistent-1-build   1/1       Running   0         19s
postgresql-1-81gnm   0/1       Pending   0         15s
postgresql-1-81gnm   0/1       ContainerCreating   0         16s
postgresql-1-81gnm   0/1       Running   0         47s
postgresql-1-81gnm   1/1       Running   0         4m
postgresql-1-deploy   0/1       Completed   0         4m
postgresql-1-deploy   0/1       Terminating   0         4m
postgresql-1-deploy   0/1       Terminating   0         4m
rails-pgsql-persistent-1-deploy   0/1       Pending   0         0s
rails-pgsql-persistent-1-deploy   0/1       Pending   0         0s
rails-pgsql-persistent-1-deploy   0/1       ContainerCreating   0         0s
rails-pgsql-persistent-1-build   0/1       Completed   0         11m
rails-pgsql-persistent-1-deploy   1/1       Running   0         6s
rails-pgsql-persistent-1-hook-pre   0/1       Pending   0         0s
rails-pgsql-persistent-1-hook-pre   0/1       Pending   0         0s
rails-pgsql-persistent-1-hook-pre   0/1       ContainerCreating   0         0s
rails-pgsql-persistent-1-hook-pre   1/1       Running   0         6s
rails-pgsql-persistent-1-hook-pre   0/1       Completed   0         15s
rails-pgsql-persistent-1-dkj7w   0/1       Pending   0         0s
rails-pgsql-persistent-1-dkj7w   0/1       Pending   0         0s
rails-pgsql-persistent-1-dkj7w   0/1       ContainerCreating   0         0s
rails-pgsql-persistent-1-dkj7w   0/1       Running   0         1m
rails-pgsql-persistent-1-dkj7w   1/1       Running   0         1m
rails-pgsql-persistent-1-deploy   0/1       Completed   0         1m
rails-pgsql-persistent-1-deploy   0/1       Terminating   0         1m
rails-pgsql-persistent-1-deploy   0/1       Terminating   0         1m
rails-pgsql-persistent-1-hook-pre   0/1       Terminating   0         1m
rails-pgsql-persistent-1-hook-pre   0/1       Terminating   0         1m
----

Exit out of the watch mode with kbd:[Ctrl + c]

NOTE: It may take up to 10 minutes for the deployment to complete.

You should also see a PVC being issued and in the _Bound_ state.

----
[root@master ~]# oc get pvc
NAME         STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
postgresql   Bound     pvc-9bb84d88-4ac6-11e7-b56f-2cc2602a6dc8   15Gi       RWO           4m
----

TIP: Why did this even work? If you paid close attention you likely noticed that the PVC in the template does not specify a particular _StorageClass_. This still yields a PV deployed because our _StorageClass_ has been defined as the system-wide default.

Now go ahead and try out the application. The overview page in the OpenShift UI will tell you the `route` which has been deployed as well. Otherwise get it on the CLI like this:

----
[root@master ~]# oc get route
NAME                     HOST/PORT                                                      PATH      SERVICES                 PORT      TERMINATION   WILDCARD
rails-pgsql-persistent   rails-pgsql-persistent-my-test-project.cloudapps.example.com             rails-pgsql-persistent   <all>                   None
----

Following this output, point your browser to http://rails-pgsql-persistent-my-test-project.cloudapps.example.com/articles. +
The username/password to create articles and comments is by default 'openshift'/'secret'.

You should be able to successfully create articles and comments. They are saved they in the PostgreSQL database which stores it's table spaces on a GlusterFS volume provided by CNS.

Now let's take a look at how this was actually achieved. First you need to acquire necessary permissions:

 [root@master ~]# oc login -u system:admin

Select the example project of the user `marina` if not already/still selected:

 [root@master ~]# oc project my-test-project

Look at the PVC to determine the PV:

----
[root@master ~]# oc get pvc
NAME         STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
postgresql   Bound     pvc-9bb84d88-4ac6-11e7-b56f-2cc2602a6dc8   15Gi       RWO           17m
----

NOTE: Your PV name will be different as it's dynamically generated.

Look at the details of this PV:

----
[root@master ~]# oc describe pv/pvc-9bb84d88-4ac6-11e7-b56f-2cc2602a6dc8
Name:		pvc-9bb84d88-4ac6-11e7-b56f-2cc2602a6dc8 <1>
Labels:		<none>
StorageClass:	container-native-storage
Status:		Bound
Claim:		my-test-project/postgresql
Reclaim Policy:	Delete
Access Modes:	RWO
Capacity:	15Gi
Message:
Source:
    Type:		Glusterfs (a Glusterfs mount on the host that shares a pod's lifetime)
    EndpointsName:	glusterfs-dynamic-postgresql
    Path:		vol_e8fe7f46fedf7af7628feda0dcbf2f60 <2>
    ReadOnly:		false
No events.
----
<1> The unique name of this PV in the system OpenShift refers to
<2> The unique volume name backing the PV known to GlusterFS

Note the GlusterFS volume name, in this case *vol_e8fe7f46fedf7af7628feda0dcbf2f60*.

Now let's switch to the namespace we used for CNS deployment:

 [root@master ~]# oc project container-native-storage

Look at the GlusterFS pods running and pick one (which one is not important):

----
[root@master ~]# oc get pods -o wide
NAME              READY     STATUS    RESTARTS   AGE       IP              NODE
glusterfs-37vn8   1/1       Running   1          15m       192.168.0.102   node1.example.com
glusterfs-cq68l   1/1       Running   1          15m       192.168.0.103   node2.example.com
glusterfs-m9fvl   1/1       Running   1          15m       192.168.0.104   node3.example.com
heketi-1-cd032    1/1       Running   1          13m       10.130.0.5      node3.example.com
----

Remember the IP address of the pod you select. Log on to GlusterFS pod with a remote terminal session like so:

----
[root@master ~]# oc rsh glusterfs-37vn8
sh-4.2#
----

You have now access to this container's namespace which has the GlusterFS CLI utilities installed. +
Let's list all known volumes:

----
sh-4.2# gluster volume list
heketidbstorage <1>
vol_e8fe7f46fedf7af7628feda0dcbf2f60 <2>
----
<1> A special volume dedicated to heketi's internal database.
<2> The volume backing the PV of the PostgreSQL database deployed earlier.

Interrogate GlusterFS about the topology of this volume:

----
sh-4.2# gluster volume info vol_e8fe7f46fedf7af7628feda0dcbf2f60

Volume Name: vol_e8fe7f46fedf7af7628feda0dcbf2f60
Type: Replicate
Volume ID: c2bedd16-8b0d-432c-b9eb-4ab1274826dd
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 3 = 3
Transport-type: tcp
Bricks:
Brick1: 192.168.0.103:/var/lib/heketi/mounts/vg_63b05bee6695ee5a63ad95bfbce43bf7/brick_aa28de668c8c21192df55956a822bd3c/brick
Brick2: 192.168.0.102:/var/lib/heketi/mounts/vg_0246fd563709384a3cbc3f3bbeeb87a9/brick_684a01f8993f241a92db02b117e0b912/brick <1>
Brick3: 192.168.0.104:/var/lib/heketi/mounts/vg_5a8c767e65feef7455b58d01c6936b83/brick_25972cf5ed7ea81c947c62443ccb308c/brick
Options Reconfigured:
transport.address-family: inet
performance.readdir-ahead: on
nfs.disable: on
----
<1> According to the output of `oc get pods -o wide` this is the container we are logged on to.

NOTE: Identify the right brick by looking at the host IP of the pod you have just logged on to. `oc get pods -o wide` will give you this information.

GlusterFS created this volume as a 3-way replica set across all GlusterFS pods, in therefore across all your OpenShift App nodes running CNS. +
Each pod/node exposes his local storage via the GlusterFS protocol. This local storage is known as a *brick* in GlusterFS and is usually backed by a local SAS disk or NVMe device. The brick is simply formatted with XFS and thus made available to GlusterFS.

You can even look at this yourself:

----
sh-4.2# ls -ahl /var/lib/heketi/mounts/vg_0246fd563709384a3cbc3f3bbeeb87a9/brick_684a01f8993f241a92db02b117e0b912/brick
total 16K
drwxrwsr-x.   5 root       2001   57 Jun  6 14:44 .
drwxr-xr-x.   3 root       root   19 Jun  6 14:44 ..
drw---S---. 263 root       2001 8.0K Jun  6 14:46 .glusterfs
drwxr-sr-x.   3 root       2001   25 Jun  6 14:44 .trashcan
drwx------.  20 1000080000 2001 8.0K Jun  6 14:46 userdata

sh-4.2# ls -ahl /var/lib/heketi/mounts/vg_0246fd563709384a3cbc3f3bbeeb87a9/brick_684a01f8993f241a92db02b117e0b912/brick/userdata

total 68K
drwx------. 20 1000080000 2001 8.0K Jun  6 14:46 .
drwxrwsr-x.  5 root       2001   57 Jun  6 14:44 ..
-rw-------.  2 1000080000 root    4 Jun  6 14:44 PG_VERSION
drwx------.  6 1000080000 root   54 Jun  6 14:46 base
drwx------.  2 1000080000 root 8.0K Jun  6 14:47 global
drwx------.  2 1000080000 root   18 Jun  6 14:44 pg_clog
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_commit_ts
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_dynshmem
-rw-------.  2 1000080000 root 4.6K Jun  6 14:46 pg_hba.conf
-rw-------.  2 1000080000 root 1.6K Jun  6 14:44 pg_ident.conf
drwx------.  2 1000080000 root   32 Jun  6 14:46 pg_log
drwx------.  4 1000080000 root   39 Jun  6 14:44 pg_logical
drwx------.  4 1000080000 root   36 Jun  6 14:44 pg_multixact
drwx------.  2 1000080000 root   18 Jun  6 14:46 pg_notify
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_replslot
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_serial
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_snapshots
drwx------.  2 1000080000 root    6 Jun  6 14:46 pg_stat
drwx------.  2 1000080000 root   84 Jun  6 15:16 pg_stat_tmp
drwx------.  2 1000080000 root   18 Jun  6 14:44 pg_subtrans
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_tblspc
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_twophase
drwx------.  3 1000080000 root   60 Jun  6 14:44 pg_xlog
-rw-------.  2 1000080000 root   88 Jun  6 14:44 postgresql.auto.conf
-rw-------.  2 1000080000 root  21K Jun  6 14:46 postgresql.conf
-rw-------.  2 1000080000 root   46 Jun  6 14:46 postmaster.opts
-rw-------.  2 1000080000 root   89 Jun  6 14:46 postmaster.pid
----

NOTE: The exact path name will be different in your environment as it has been automatically generated.

You are looking at the PostgreSQL internal data file structure from the perspective of the GlusterFS server side. It's a normal local filesystem here.

Clients, like the OpenShift nodes and their application pods talk to this storage with the GlusterFS protocol. Which abstracts the 3-way replication behind a single FUSE mount point. +
When a pod starts that mounts storage from a PV backed by GlusterFS OpenShift will mount the GlusterFS volume on the App Node and then _bind-mount_ this directory to the right pod. +
This is happen transparently to the application inside the pod and looks like a normal local filesystem.

You may exit your remote session to the GlusterFS pod.

 sh-4.2# exit

==== Providing shared storage to multiple application instances

So far only very few options, like the basic NFS support existed, to provide a PersistentVolume to more than one container at once. The access mode used for this is *ReadWriteMany*.

With CNS this capabilities is now available to all OpenShift deployments, no matter where they are deployed. To demonstrate this capability with an application we will deploy a PHP file uploader that has multiple front-end instances sharing a common storage repository.


First log back in as `marina`

 [root@master ~]# oc login -u marina --insecure-skip-tls-verify --server=https://master.example.com:8443

Next deploy the example application:

----
[root@master ~]# oc new-app openshift/php:7.0~https://github.com/christianh814/openshift-php-upload-demo --name=file-uploader
--> Found image a1ebebb (6 weeks old) in image stream "openshift/php" under tag "7.0" for "openshift/php:7.0"

    Apache 2.4 with PHP 7.0
    -----------------------
    Platform for building and running PHP 7.0 applications

    Tags: builder, php, php70, rh-php70

    * A source build using source code from https://github.com/christianh814/openshift-php-upload-demo will be created
      * The resulting image will be pushed to image stream "file-uploader:latest"
      * Use 'start-build' to trigger a new build
    * This image will be deployed in deployment config "file-uploader"
    * Port 8080/tcp will be load balanced by service "file-uploader"
      * Other containers can access this service through the hostname "file-uploader"

--> Creating resources ...
    imagestream "file-uploader" created
    buildconfig "file-uploader" created
    deploymentconfig "file-uploader" created
    service "file-uploader" created
--> Success
    Build scheduled, use 'oc logs -f bc/file-uploader' to track its progress.
    Run 'oc status' to view your app.
----

Wait for the application to be deployed with the suggest command:

----
[root@master ~]# oc logs -f bc/file-uploader
Cloning "https://github.com/christianh814/openshift-php-upload-demo" ...
	Commit:	7508da63d78b4abc8d03eac480ae930beec5d29d (Update index.html)
	Author:	Christian Hernandez <christianh814@users.noreply.github.com>
	Date:	Thu Mar 23 09:59:38 2017 -0700
---> Installing application source...
Pushing image 172.30.120.134:5000/my-test-project/file-uploader:latest ...
Pushed 0/5 layers, 2% complete
Pushed 1/5 layers, 20% complete
Pushed 2/5 layers, 40% complete
Push successful
----

Again kbd:[Ctrl + c] out of the tail mode.
When the build is completed ensure the pods are running:

----
[root@master ~]# oc get pods
NAME                             READY     STATUS      RESTARTS   AGE
file-uploader-1-build            0/1       Completed   0          2m
file-uploader-1-k2v0d            1/1       Running     0          1m
...
----

Note the name of the single pod currently running the app: *file-uploader-1-k2v0d*. The container called `file-uploader-1-build` is the builder container and is not relevant for us. A service has been created for our app but not exposed yet. Let's fix this:

 [root@master ~]# oc expose svc/file-uploader

Check the route that has been created:

----
[root@master ~]# oc get route
NAME                     HOST/PORT                                                      PATH      SERVICES                 PORT       TERMINATION   WILDCARD
file-uploader            file-uploader-my-test-project.cloudapps.example.com                      file-uploader            8080-tcp                 None
...
----

Point your browser the the URL advertised by the route (http://file-uploader-my-test-project.cloudapps.example.com)

The application simply lists all file previously uploaded and offers the ability to upload new ones as well as download the existing data. Right now there is nothing.

Select an arbitrary from your local system and upload it to the app.

.A simple PHP-based file upload tool
image::http://people.redhat.com/~llange/labimg/uploader_screen_upload.png[]

After uploading a file validate it has been stored locally in the container by following the link _List uploaded files_ in the browser or logging into it via a remote session (using the name noted earlier):

 [root@master ~]# oc rsh file-uploader-1-k2v0d

----
sh-4.2$ cd uploaded
sh-4.2$ pwd
/opt/app-root/src/uploaded
sh-4.2$ ls -lh
total 16K
-rw-r--r--. 1 1000080000 root 16K May 26 09:32 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz
----

NOTE: The exact name of the pod will be different in your environment.

The app should also list the file in the overview:

.The file has been uploaded and can be downloaded again

image::http://people.redhat.com/~llange/labimg/uploader_screen_list.png[uploader]

This pod currently does not use any persistent storage. It stores the file locally.

CAUTION: Never store data in a pod. It's ephemeral by definition and will be lost as soon as the pod terminates.

Let's see when this become a problem. Exit out of the container shell:

 sh-4.2$ exit

Let's scale the deployment to 3 instances of the app:

 [root@master ~]# oc scale dc/file-uploader --replicas=3

Watch the additional pods getting spawned:

----
[root@master ~]# oc get pods
NAME                             READY     STATUS      RESTARTS   AGE
file-uploader-1-3cgh1            1/1       Running     0          20s
file-uploader-1-3hckj            1/1       Running     0          20s
file-uploader-1-build            0/1       Completed   0          4m
file-uploader-1-k2v0d            1/1       Running     0          3m
...
----

NOTE: The pod names will be different in your environment since they are automatically generated.

When you log on to one of the new instances you will see they have no data.

----
[root@master ~]# oc rsh file-uploader-1-3cgh1
sh-4.2$ cd uploaded
sh-4.2$ pwd
/opt/app-root/src/uploaded
sh-4.2$ ls -hl
total 0
----

Similarly, other users of the app will sometimes see your uploaded files and sometimes not - whenever the load balancing service in OpenShift points to the pod that has the file stored locally. You can simulate this with another instance of your browser in "Incognito mode" pointing to your app.

The app is of course not usable like this. We can fix this by providing shared storage to this app.

First create a PVC with the appropriate setting in a file called `cns-rwx-pvc.yml` with below contents:

[source,yaml]
.cns-rwx-pvc.yml
----
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-shared-storage
  annotations:
    volume.beta.kubernetes.io/storage-class: container-native-storage
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
----

Submit the request to the system:

 [root@master ~]# oc create -f cns-rwx-pvc.yml

Let's look at the result:

----
[root@master ~]# oc get pvc
NAME                STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
my-shared-storage   Bound     pvc-62aa4dfe-4ad2-11e7-b56f-2cc2602a6dc8   10Gi       RWX           22s
...
----

Notice the ACCESSMODE being set to *RWX* (short for _ReadWriteMany_, synonym for "shared storage").

We can now update the _DeploymentConfig_ of our application to use this PVC to provide the application with persistent, shared storage for uploads.

 [root@master ~]# oc volume dc/file-uploader --add --name=shared-storage --type=persistentVolumeClaim --claim-name=my-shared-storage --mount-path=/opt/app-root/src/uploaded

Our app will now re-deploy (in a rolling fashion) with the new settings - all pods will mount the volume identified by the PVC under /opt/app-root/src/upload (the path is predictable so we can hard-code it here).

You can watch it like this:

----
[root@master ~]# oc logs dc/file-uploader -f
--> Scaling up file-uploader-2 from 0 to 3, scaling down file-uploader-1 from 3 to 0 (keep 3 pods available, don't exceed 4 pods)
    Scaling file-uploader-2 up to 1
    Scaling file-uploader-1 down to 2
    Scaling file-uploader-2 up to 2
    Scaling file-uploader-1 down to 1
    Scaling file-uploader-2 up to 3
    Scaling file-uploader-1 down to 0
--> Success
----

The new config `file-uploader-2` will have 3 pods all sharing the same storage.

----
[root@master ~]# oc get pods
NAME                             READY     STATUS      RESTARTS   AGE
file-uploader-1-build            0/1       Completed   0          18m
file-uploader-2-jd22b            1/1       Running     0          1m
file-uploader-2-kw9lq            1/1       Running     0          2m
file-uploader-2-xbz24            1/1       Running     0          1m
...
----

Try it out in your application: upload new files and watch them being visible from within all application pods. In the browser the application behaves fluently as it circles through the pods between browser requests.


----
[root@master ~]# oc rsh file-uploader-2-jd22b
sh-4.2$ ls -lh uploaded
total 16K
-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz
sh-4.2$ exit
exit
[root@master ~]# oc rsh file-uploader-2-kw9lq
sh-4.2$ ls -lh uploaded
-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz
sh-4.2$ exit
exit
[root@master ~]# oc rsh file-uploader-2-xbz24
sh-4.2$ ls -lh uploaded
-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz
sh-4.2$ exit
----

That's it. You have successfully provided shared storage to pods throughout the entire system, therefore avoiding the need for data to be replicated at the application level to each pod.

With CNS this is available wherever OpenShift is deployed with no external dependency.
