<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
    "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8" />
<meta name="generator" content="AsciiDoc 8.6.8" />
<title>Manage your Container with Red Hat OpenShift</title>
<style type="text/css">
@import url(http://cdnjs.cloudflare.com/ajax/libs/font-awesome/3.2.0/css/font-awesome.css);
/* normalize.css v2.1.1 | MIT License | git.io/normalize */
/* ========================================================================== HTML5 display definitions ========================================================================== */
/** Correct `block` display not defined in IE 8/9. */
article, aside, details, figcaption, figure, footer, header, hgroup, main, nav, section, summary { display: block; }

/** Correct `inline-block` display not defined in IE 8/9. */
audio, canvas, video { display: inline-block; }

/** Prevent modern browsers from displaying `audio` without controls. Remove excess height in iOS 5 devices. */
audio:not([controls]) { display: none; height: 0; }

/** Address styling not present in IE 8/9. */
[hidden] { display: none; }

/* ========================================================================== Base ========================================================================== */
/** 1. Prevent system color scheme's background color being used in Firefox, IE, and Opera. 2. Prevent system color scheme's text color being used in Firefox, IE, and Opera. 3. Set default font family to sans-serif. 4. Prevent iOS text size adjust after orientation change, without disabling user zoom. */
html { background: #fff; /* 1 */ color: #000; /* 2 */ font-family: sans-serif; /* 3 */ -ms-text-size-adjust: 100%; /* 4 */ -webkit-text-size-adjust: 100%; /* 4 */ }

/** Remove default margin. */
body { margin: 0; }

/* ========================================================================== Links ========================================================================== */
/** Address `outline` inconsistency between Chrome and other browsers. */
a:focus { outline: thin dotted; }

/** Improve readability when focused and also mouse hovered in all browsers. */
a:active, a:hover { outline: 0; }

/* ========================================================================== Typography ========================================================================== */
/** Address variable `h1` font-size and margin within `section` and `article` contexts in Firefox 4+, Safari 5, and Chrome. */
h1 { font-size: 2em; margin: 0.67em 0; }

/** Address styling not present in IE 8/9, Safari 5, and Chrome. */
abbr[title] { border-bottom: 1px dotted; }

/** Address style set to `bolder` in Firefox 4+, Safari 5, and Chrome. */
b, strong { font-weight: bold; }

/** Address styling not present in Safari 5 and Chrome. */
dfn { font-style: italic; }

/** Address differences between Firefox and other browsers. */
hr { -moz-box-sizing: content-box; box-sizing: content-box; height: 0; }

/** Address styling not present in IE 8/9. */
mark { background: #ff0; color: #000; }

/** Correct font family set oddly in Safari 5 and Chrome. */
code, kbd, pre, samp { font-family: monospace, serif; font-size: 1em; }

/** Improve readability of pre-formatted text in all browsers. */
pre { white-space: pre-wrap; }

/** Set consistent quote types. */
q { quotes: "\201C" "\201D" "\2018" "\2019"; }

/** Address inconsistent and variable font size in all browsers. */
small { font-size: 80%; }

/** Prevent `sub` and `sup` affecting `line-height` in all browsers. */
sub, sup { font-size: 75%; line-height: 0; position: relative; vertical-align: baseline; }

sup { top: -0.5em; }

sub { bottom: -0.25em; }

/* ========================================================================== Embedded content ========================================================================== */
/** Remove border when inside `a` element in IE 8/9. */
img { border: 0; }

/** Correct overflow displayed oddly in IE 9. */
svg:not(:root) { overflow: hidden; }

/* ========================================================================== Figures ========================================================================== */
/** Address margin not present in IE 8/9 and Safari 5. */
figure { margin: 0; }

/* ========================================================================== Forms ========================================================================== */
/** Define consistent border, margin, and padding. */
fieldset { border: 1px solid #c0c0c0; margin: 0 2px; padding: 0.35em 0.625em 0.75em; }

/** 1. Correct `color` not being inherited in IE 8/9. 2. Remove padding so people aren't caught out if they zero out fieldsets. */
legend { border: 0; /* 1 */ padding: 0; /* 2 */ }

/** 1. Correct font family not being inherited in all browsers. 2. Correct font size not being inherited in all browsers. 3. Address margins set differently in Firefox 4+, Safari 5, and Chrome. */
button, input, select, textarea { font-family: inherit; /* 1 */ font-size: 100%; /* 2 */ margin: 0; /* 3 */ }

/** Address Firefox 4+ setting `line-height` on `input` using `!important` in the UA stylesheet. */
button, input { line-height: normal; }

/** Address inconsistent `text-transform` inheritance for `button` and `select`. All other form control elements do not inherit `text-transform` values. Correct `button` style inheritance in Chrome, Safari 5+, and IE 8+. Correct `select` style inheritance in Firefox 4+ and Opera. */
button, select { text-transform: none; }

/** 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio` and `video` controls. 2. Correct inability to style clickable `input` types in iOS. 3. Improve usability and consistency of cursor style between image-type `input` and others. */
button, html input[type="button"], input[type="reset"], input[type="submit"] { -webkit-appearance: button; /* 2 */ cursor: pointer; /* 3 */ }

/** Re-set default cursor for disabled elements. */
button[disabled], html input[disabled] { cursor: default; }

/** 1. Address box sizing set to `content-box` in IE 8/9. 2. Remove excess padding in IE 8/9. */
input[type="checkbox"], input[type="radio"] { box-sizing: border-box; /* 1 */ padding: 0; /* 2 */ }

/** 1. Address `appearance` set to `searchfield` in Safari 5 and Chrome. 2. Address `box-sizing` set to `border-box` in Safari 5 and Chrome (include `-moz` to future-proof). */
input[type="search"] { -webkit-appearance: textfield; /* 1 */ -moz-box-sizing: content-box; -webkit-box-sizing: content-box; /* 2 */ box-sizing: content-box; }

/** Remove inner padding and search cancel button in Safari 5 and Chrome on OS X. */
input[type="search"]::-webkit-search-cancel-button, input[type="search"]::-webkit-search-decoration { -webkit-appearance: none; }

/** Remove inner padding and border in Firefox 4+. */
button::-moz-focus-inner, input::-moz-focus-inner { border: 0; padding: 0; }

/** 1. Remove default vertical scrollbar in IE 8/9. 2. Improve readability and alignment in all browsers. */
textarea { overflow: auto; /* 1 */ vertical-align: top; /* 2 */ }

/* ========================================================================== Tables ========================================================================== */
/** Remove most spacing between table cells. */
table { border-collapse: collapse; border-spacing: 0; }

*, *:before, *:after { -moz-box-sizing: border-box; -webkit-box-sizing: border-box; box-sizing: border-box; }

html, body { font-size: 100%; }

body { background: white; color: #222222; padding: 0; margin: 0; font-family: "Helvetica Neue", "Helvetica", Helvetica, Arial, sans-serif; font-weight: normal; font-style: normal; line-height: 1; position: relative; cursor: auto; }

a:hover { cursor: pointer; }

a:focus { outline: none; }

img, object, embed { max-width: 100%; height: auto; }

object, embed { height: 100%; }

img { -ms-interpolation-mode: bicubic; }

#map_canvas img, #map_canvas embed, #map_canvas object, .map_canvas img, .map_canvas embed, .map_canvas object { max-width: none !important; }

.left { float: left !important; }

.right { float: right !important; }

.text-left { text-align: left !important; }

.text-right { text-align: right !important; }

.text-center { text-align: center !important; }

.text-justify { text-align: justify !important; }

.hide { display: none; }

.antialiased, body { -webkit-font-smoothing: antialiased; }

img { display: inline-block; vertical-align: middle; }

textarea { height: auto; min-height: 50px; }

select { width: 100%; }

p.lead, .paragraph.lead > p, #preamble > .sectionbody > .paragraph:first-of-type p { font-size: 1.21875em; line-height: 1.6; }

.subheader, #content #toctitle, .admonitionblock td.content > .title, .exampleblock > .title, .imageblock > .title, .videoblock > .title, .listingblock > .title, .literalblock > .title, .openblock > .title, .paragraph > .title, .quoteblock > .title, .sidebarblock > .title, .tableblock > .title, .verseblock > .title, .dlist > .title, .olist > .title, .ulist > .title, .qlist > .title, .hdlist > .title, .tableblock > caption { line-height: 1.4; color: #003b6b; font-weight: 300; margin-top: 0.2em; margin-bottom: 0.5em; }

/* Typography resets */
div, dl, dt, dd, ul, ol, li, h1, h2, h3, #toctitle, .sidebarblock > .content > .title, h4, h5, h6, pre, form, p, blockquote, th, td { margin: 0; padding: 0; direction: ltr; }

/* Default Link Styles */
a { color: #00579e; text-decoration: none; line-height: inherit; }
a:hover, a:focus { color: #333333; }
a img { border: none; }

/* Default paragraph styles */
p { font-family: Arial, sans-serif; font-weight: normal; font-size: 1em; line-height: 1.6; margin-bottom: 0.75em; text-rendering: optimizeLegibility; }
p aside { font-size: 0.875em; line-height: 1.35; font-style: italic; }

/* Default header styles */
h1, h2, h3, #toctitle, .sidebarblock > .content > .title, h4, h5, h6 { font-family: Arial, sans-serif; font-weight: normal; font-style: normal; color: #7b2d00; text-rendering: optimizeLegibility; margin-top: 0.5em; margin-bottom: 0.5em; line-height: 1.2125em; }
h1 small, h2 small, h3 small, #toctitle small, .sidebarblock > .content > .title small, h4 small, h5 small, h6 small { font-size: 60%; color: #ff6b15; line-height: 0; }

h1 { font-size: 2.125em; }

h2 { font-size: 1.6875em; }

h3, #toctitle, .sidebarblock > .content > .title { font-size: 1.375em; }

h4 { font-size: 1.125em; }

h5 { font-size: 1.125em; }

h6 { font-size: 1em; }

hr { border: solid #dddddd; border-width: 1px 0 0; clear: both; margin: 1.25em 0 1.1875em; height: 0; }

/* Helpful Typography Defaults */
em, i { font-style: italic; line-height: inherit; }

strong, b { font-weight: bold; line-height: inherit; }

small { font-size: 60%; line-height: inherit; }

code { font-family: Consolas, "Liberation Mono", Courier, monospace; font-weight: bold; color: #003426; }

/* Lists */
ul, ol, dl { font-size: 1em; line-height: 1.6; margin-bottom: 0.75em; list-style-position: outside; font-family: Arial, sans-serif; }

ul, ol { margin-left: 1.5em; }

/* Unordered Lists */
ul li ul, ul li ol { margin-left: 1.25em; margin-bottom: 0; font-size: 1em; /* Override nested font-size change */ }
ul.square li ul, ul.circle li ul, ul.disc li ul { list-style: inherit; }
ul.square { list-style-type: square; }
ul.circle { list-style-type: circle; }
ul.disc { list-style-type: disc; }
ul.no-bullet { list-style: none; }

/* Ordered Lists */
ol li ul, ol li ol { margin-left: 1.25em; margin-bottom: 0; }

/* Definition Lists */
dl dt { margin-bottom: 0.3em; font-weight: bold; }
dl dd { margin-bottom: 0.75em; }

/* Abbreviations */
abbr, acronym { text-transform: uppercase; font-size: 90%; color: black; border-bottom: 1px dotted #dddddd; cursor: help; }

abbr { text-transform: none; }

/* Blockquotes */
blockquote { margin: 0 0 0.75em; padding: 0.5625em 1.25em 0 1.1875em; border-left: 1px solid #dddddd; }
blockquote cite { display: block; font-size: 0.8125em; color: #e15200; }
blockquote cite:before { content: "\2014 \0020"; }
blockquote cite a, blockquote cite a:visited { color: #e15200; }

blockquote, blockquote p { line-height: 1.6; color: #333333; }

/* Microformats */
.vcard { display: inline-block; margin: 0 0 1.25em 0; border: 1px solid #dddddd; padding: 0.625em 0.75em; }
.vcard li { margin: 0; display: block; }
.vcard .fn { font-weight: bold; font-size: 0.9375em; }

.vevent .summary { font-weight: bold; }
.vevent abbr { cursor: auto; text-decoration: none; font-weight: bold; border: none; padding: 0 0.0625em; }

@media only screen and (min-width: 768px) { h1, h2, h3, #toctitle, .sidebarblock > .content > .title, h4, h5, h6 { line-height: 1.4; }
  h1 { font-size: 2.75em; }
  h2 { font-size: 2.3125em; }
  h3, #toctitle, .sidebarblock > .content > .title { font-size: 1.6875em; }
  h4 { font-size: 1.4375em; } }
/* Print styles.  Inlined to avoid required HTTP connection: www.phpied.com/delay-loading-your-print-css/ Credit to Paul Irish and HTML5 Boilerplate (html5boilerplate.com)
*/
.print-only { display: none !important; }

@media print { * { background: transparent !important; color: #000 !important; /* Black prints faster: h5bp.com/s */ box-shadow: none !important; text-shadow: none !important; }
  a, a:visited { text-decoration: underline; }
  a[href]:after { content: " (" attr(href) ")"; }
  abbr[title]:after { content: " (" attr(title) ")"; }
  .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after { content: ""; }
  pre, blockquote { border: 1px solid #999; page-break-inside: avoid; }
  thead { display: table-header-group; /* h5bp.com/t */ }
  tr, img { page-break-inside: avoid; }
  img { max-width: 100% !important; }
  @page { margin: 0.5cm; }
  p, h2, h3, #toctitle, .sidebarblock > .content > .title { orphans: 3; widows: 3; }
  h2, h3, #toctitle, .sidebarblock > .content > .title { page-break-after: avoid; }
  .hide-on-print { display: none !important; }
  .print-only { display: block !important; }
  .hide-for-print { display: none !important; }
  .show-for-print { display: inherit !important; } }
/* Tables */
table { background: white; margin-bottom: 1.25em; border: solid 1px #d8d8ce; }
table thead, table tfoot { background: -webkit-linear-gradient(top, #add386, #90b66a); font-weight: bold; }
table thead tr th, table thead tr td, table tfoot tr th, table tfoot tr td { padding: 0.5em 0.625em 0.625em; font-size: inherit; color: white; text-align: left; }
table tr th, table tr td { padding: 0.5625em 0.625em; font-size: inherit; color: #6d6e71; }
table tr.even, table tr.alt, table tr:nth-of-type(even) { background: #edf2f2; }
table thead tr th, table tfoot tr th, table tbody tr td, table tr td, table tfoot tr td { display: table-cell; line-height: 1.4; }

a:hover, a:focus { text-decoration: underline; }

.clearfix:before, .clearfix:after, .float-group:before, .float-group:after { content: " "; display: table; }
.clearfix:after, .float-group:after { clear: both; }

*:not(pre) > code { font-size: inherit; padding: 3px 2px 1px 2px; white-space: nowrap; background-color: #eeeeee; border: 1px solid #dddddd; -webkit-border-radius: 0; border-radius: 0; text-shadow: none; }

pre, pre > code { line-height: 1.6; color: black; font-family: Consolas, "Liberation Mono", Courier, monospace; font-weight: normal; }

kbd.keyseq { color: #333333; }

kbd:not(.keyseq) { display: inline-block; color: black; font-size: 0.75em; line-height: 1.4; background-color: #F7F7F7; border: 1px solid #ccc; -webkit-border-radius: 3px; border-radius: 3px; -webkit-box-shadow: 0 1px 0 rgba(0, 0, 0, 0.2), 0 0 0 2px white inset; box-shadow: 0 1px 0 rgba(0, 0, 0, 0.2), 0 0 0 2px white inset; margin: -0.15em 0.15em 0 0.15em; padding: 0.2em 0.6em 0.2em 0.5em; vertical-align: middle; white-space: nowrap; }

kbd kbd:first-child { margin-left: 0; }

kbd kbd:last-child { margin-right: 0; }

.menuseq, .menu { color: black; }

#header, #content, #footnotes, #footer { width: 100%; margin-left: auto; margin-right: auto; margin-top: 0; margin-bottom: 0; max-width: 62.5em; *zoom: 1; position: relative; padding-left: 1.5em; padding-right: 1.5em; }
#header:before, #header:after, #content:before, #content:after, #footnotes:before, #footnotes:after, #footer:before, #footer:after { content: " "; display: table; }
#header:after, #content:after, #footnotes:after, #footer:after { clear: both; }

#header { margin-bottom: 2.5em; }
#header > h1 { color: #7b2d00; font-weight: normal; border-bottom: 1px solid #dddddd; margin-bottom: -28px; padding-bottom: 32px; }
#header span { color: #333333; }
#header #revnumber { text-transform: capitalize; }
#header br { display: none; }
#header br + span { padding-left: 3px; }
#header br + span:before { content: "\2013 \0020"; }
#header br + span.author { padding-left: 0; }
#header br + span.author:before { content: ", "; }

#toc { border-bottom: 1px solid #dddddd; padding-bottom: 1.25em; }
#toc > ul { margin-left: 0.25em; }
#toc ul.sectlevel0 > li > a { font-style: italic; }
#toc ul.sectlevel0 ul.sectlevel1 { margin-left: 0; margin-top: 0.5em; margin-bottom: 0.5em; }
#toc ul { list-style-type: none; }

#toctitle { color: #003b6b; }

@media only screen and (min-width: 1280px) { body.toc2 { padding-left: 20em; }
  #toc.toc2 { position: fixed; width: 20em; left: 0; top: 0; border-right: 1px solid #dddddd; border-bottom: 0; z-index: 1000; padding: 1em; height: 100%; overflow: auto; }
  #toc.toc2 #toctitle { margin-top: 0; }
  #toc.toc2 > ul { font-size: .95em; }
  #toc.toc2 ul ul { margin-left: 0; padding-left: 1.25em; }
  #toc.toc2 ul.sectlevel0 ul.sectlevel1 { padding-left: 0; margin-top: 0.5em; margin-bottom: 0.5em; }
  body.toc2.toc-right { padding-left: 0; padding-right: 20em; }
  body.toc2.toc-right #toc.toc2 { border-right: 0; border-left: 1px solid #dddddd; left: auto; right: 0; } }
#content #toc { border-style: solid; border-width: 1px; border-color: #e6e6e6; margin-bottom: 1.25em; padding: 1.25em; background: white; border-width: 0; -webkit-border-radius: 0; border-radius: 0; }
#content #toc > :first-child { margin-top: 0; }
#content #toc > :last-child { margin-bottom: 0; }
#content #toc a { text-decoration: none; }

#content #toctitle { font-weight: bold; font-family: "Helvetica Neue", "Helvetica", Helvetica, Arial, sans-serif; font-size: 1em; padding-left: 0.125em; }

#footer { max-width: 100%; background-color: none; padding: 1.25em; }

#footer-text { color: black; line-height: 1.44; }

.sect1 { padding-bottom: 1.25em; }

.sect1 + .sect1 { border-top: 1px solid #dddddd; }

#content h1 > a.anchor, h2 > a.anchor, h3 > a.anchor, #toctitle > a.anchor, .sidebarblock > .content > .title > a.anchor, h4 > a.anchor, h5 > a.anchor, h6 > a.anchor { position: absolute; width: 1em; margin-left: -1em; display: block; text-decoration: none; visibility: hidden; text-align: center; font-weight: normal; }
#content h1 > a.anchor:before, h2 > a.anchor:before, h3 > a.anchor:before, #toctitle > a.anchor:before, .sidebarblock > .content > .title > a.anchor:before, h4 > a.anchor:before, h5 > a.anchor:before, h6 > a.anchor:before { content: '\00A7'; font-size: .85em; vertical-align: text-top; display: block; margin-top: 0.05em; }
#content h1:hover > a.anchor, #content h1 > a.anchor:hover, h2:hover > a.anchor, h2 > a.anchor:hover, h3:hover > a.anchor, #toctitle:hover > a.anchor, .sidebarblock > .content > .title:hover > a.anchor, h3 > a.anchor:hover, #toctitle > a.anchor:hover, .sidebarblock > .content > .title > a.anchor:hover, h4:hover > a.anchor, h4 > a.anchor:hover, h5:hover > a.anchor, h5 > a.anchor:hover, h6:hover > a.anchor, h6 > a.anchor:hover { visibility: visible; }
#content h1 > a.link, h2 > a.link, h3 > a.link, #toctitle > a.link, .sidebarblock > .content > .title > a.link, h4 > a.link, h5 > a.link, h6 > a.link { color: #7b2d00; text-decoration: none; }
#content h1 > a.link:hover, h2 > a.link:hover, h3 > a.link:hover, #toctitle > a.link:hover, .sidebarblock > .content > .title > a.link:hover, h4 > a.link:hover, h5 > a.link:hover, h6 > a.link:hover { color: #622400; }

.imageblock, .literalblock, .listingblock, .verseblock, .videoblock { margin-bottom: 1.25em; }

.admonitionblock td.content > .title, .exampleblock > .title, .imageblock > .title, .videoblock > .title, .listingblock > .title, .literalblock > .title, .openblock > .title, .paragraph > .title, .quoteblock > .title, .sidebarblock > .title, .tableblock > .title, .verseblock > .title, .dlist > .title, .olist > .title, .ulist > .title, .qlist > .title, .hdlist > .title { text-align: left; font-weight: bold; }

.tableblock > caption { text-align: left; font-weight: bold; white-space: nowrap; overflow: visible; max-width: 0; }

table.tableblock #preamble > .sectionbody > .paragraph:first-of-type p { font-size: inherit; }

.admonitionblock > table { border: 0; background: none; width: 100%; }
.admonitionblock > table td.icon { text-align: center; width: 80px; }
.admonitionblock > table td.icon img { max-width: none; }
.admonitionblock > table td.icon .title { font-weight: bold; text-transform: uppercase; }
.admonitionblock > table td.content { padding-left: 1.125em; padding-right: 1.25em; border-left: 1px solid #dddddd; color: #333333; }
.admonitionblock > table td.content > :last-child > :last-child { margin-bottom: 0; }

.exampleblock > .content { border-style: solid; border-width: 1px; border-color: #e6e6e6; margin-bottom: 1.25em; padding: 1.25em; background: white; -webkit-border-radius: 0; border-radius: 0; }
.exampleblock > .content > :first-child { margin-top: 0; }
.exampleblock > .content > :last-child { margin-bottom: 0; }
.exampleblock > .content h1, .exampleblock > .content h2, .exampleblock > .content h3, .exampleblock > .content #toctitle, .sidebarblock.exampleblock > .content > .title, .exampleblock > .content h4, .exampleblock > .content h5, .exampleblock > .content h6, .exampleblock > .content p { color: #333333; }
.exampleblock > .content h1, .exampleblock > .content h2, .exampleblock > .content h3, .exampleblock > .content #toctitle, .sidebarblock.exampleblock > .content > .title, .exampleblock > .content h4, .exampleblock > .content h5, .exampleblock > .content h6 { line-height: 1; margin-bottom: 0.625em; }
.exampleblock > .content h1.subheader, .exampleblock > .content h2.subheader, .exampleblock > .content h3.subheader, .exampleblock > .content .subheader#toctitle, .sidebarblock.exampleblock > .content > .subheader.title, .exampleblock > .content h4.subheader, .exampleblock > .content h5.subheader, .exampleblock > .content h6.subheader { line-height: 1.4; }

.exampleblock.result > .content { -webkit-box-shadow: 0 1px 8px #aaaaaa; box-shadow: 0 1px 8px #aaaaaa; }

.sidebarblock { border-style: solid; border-width: 1px; border-color: #e6e6e6; margin-bottom: 1.25em; padding: 1.25em; background: white; -webkit-border-radius: 0; border-radius: 0; }
.sidebarblock > :first-child { margin-top: 0; }
.sidebarblock > :last-child { margin-bottom: 0; }
.sidebarblock h1, .sidebarblock h2, .sidebarblock h3, .sidebarblock #toctitle, .sidebarblock > .content > .title, .sidebarblock h4, .sidebarblock h5, .sidebarblock h6, .sidebarblock p { color: #333333; }
.sidebarblock h1, .sidebarblock h2, .sidebarblock h3, .sidebarblock #toctitle, .sidebarblock > .content > .title, .sidebarblock h4, .sidebarblock h5, .sidebarblock h6 { line-height: 1; margin-bottom: 0.625em; }
.sidebarblock h1.subheader, .sidebarblock h2.subheader, .sidebarblock h3.subheader, .sidebarblock .subheader#toctitle, .sidebarblock > .content > .subheader.title, .sidebarblock h4.subheader, .sidebarblock h5.subheader, .sidebarblock h6.subheader { line-height: 1.4; }
.sidebarblock > .content > .title { color: #003b6b; margin-top: 0; line-height: 1.6; }

.exampleblock > .content > :last-child > :last-child, .exampleblock > .content .olist > ol > li:last-child > :last-child, .exampleblock > .content .ulist > ul > li:last-child > :last-child, .exampleblock > .content .qlist > ol > li:last-child > :last-child, .sidebarblock > .content > :last-child > :last-child, .sidebarblock > .content .olist > ol > li:last-child > :last-child, .sidebarblock > .content .ulist > ul > li:last-child > :last-child, .sidebarblock > .content .qlist > ol > li:last-child > :last-child { margin-bottom: 0; }

.literalblock > .content pre, .listingblock > .content pre { background: #eeeeee; border-width: 1px; border-style: dashed; border-color: #666666; -webkit-border-radius: 0; border-radius: 0; padding: 1.25em 1.5625em 1.125em 1.5625em; word-wrap: break-word; }
.literalblock > .content pre.nowrap, .listingblock > .content pre.nowrap { overflow-x: auto; white-space: pre; word-wrap: normal; }
.literalblock > .content pre > code, .listingblock > .content pre > code { display: block; }
@media only screen { .literalblock > .content pre, .listingblock > .content pre { font-size: 0.72em; } }
@media only screen and (min-width: 768px) { .literalblock > .content pre, .listingblock > .content pre { font-size: 0.81em; } }
@media only screen and (min-width: 1280px) { .literalblock > .content pre, .listingblock > .content pre { font-size: 0.9em; } }

.listingblock > .content { position: relative; }

.listingblock:hover code[class*=" language-"]:before { text-transform: uppercase; font-size: 0.9em; color: #999; position: absolute; top: 0.375em; right: 0.375em; }

.listingblock:hover code.asciidoc:before { content: "asciidoc"; }
.listingblock:hover code.clojure:before { content: "clojure"; }
.listingblock:hover code.css:before { content: "css"; }
.listingblock:hover code.groovy:before { content: "groovy"; }
.listingblock:hover code.html:before { content: "html"; }
.listingblock:hover code.java:before { content: "java"; }
.listingblock:hover code.javascript:before { content: "javascript"; }
.listingblock:hover code.python:before { content: "python"; }
.listingblock:hover code.ruby:before { content: "ruby"; }
.listingblock:hover code.scss:before { content: "scss"; }
.listingblock:hover code.xml:before { content: "xml"; }
.listingblock:hover code.yaml:before { content: "yaml"; }

.listingblock.terminal pre .command:before { content: attr(data-prompt); padding-right: 0.5em; color: #999; }

.listingblock.terminal pre .command:not([data-prompt]):before { content: '$'; }

table.pyhltable { border: 0; margin-bottom: 0; }

table.pyhltable td { vertical-align: top; padding-top: 0; padding-bottom: 0; }

table.pyhltable td.code { padding-left: .75em; padding-right: 0; }

.highlight.pygments .lineno, table.pyhltable td:not(.code) { color: #999; padding-left: 0; padding-right: .5em; border-right: 1px solid #dddddd; }

.highlight.pygments .lineno { display: inline-block; margin-right: .25em; }

table.pyhltable .linenodiv { background-color: transparent !important; padding-right: 0 !important; }

.quoteblock { margin: 0 0 0.75em; padding: 0.5625em 1.25em 0 1.1875em; border-left: 1px solid #dddddd; }
.quoteblock blockquote { margin: 0 0 0.75em 0; padding: 0 0 0.5625em 0; border: 0; }
.quoteblock blockquote > .paragraph:last-child p { margin-bottom: 0; }
.quoteblock .attribution { margin-top: -.25em; padding-bottom: 0.5625em; font-size: 0.8125em; color: #e15200; }
.quoteblock .attribution br { display: none; }
.quoteblock .attribution cite { display: block; margin-bottom: 0.625em; }

table thead th, table tfoot th { font-weight: bold; }

table.tableblock.grid-all { border-collapse: separate; border-spacing: 1px; -webkit-border-radius: 0; border-radius: 0; border-top: 1px solid #dddddd; border-bottom: 1px solid #dddddd; }

table.tableblock.frame-topbot, table.tableblock.frame-none { border-left: 0; border-right: 0; }

table.tableblock.frame-sides, table.tableblock.frame-none { border-top: 0; border-bottom: 0; }

table.tableblock td .paragraph:last-child p, table.tableblock td > p:last-child { margin-bottom: 0; }

th.tableblock.halign-left, td.tableblock.halign-left { text-align: left; }

th.tableblock.halign-right, td.tableblock.halign-right { text-align: right; }

th.tableblock.halign-center, td.tableblock.halign-center { text-align: center; }

th.tableblock.valign-top, td.tableblock.valign-top { vertical-align: top; }

th.tableblock.valign-bottom, td.tableblock.valign-bottom { vertical-align: bottom; }

th.tableblock.valign-middle, td.tableblock.valign-middle { vertical-align: middle; }

p.tableblock.header { color: white; font-weight: bold; }

td > div.verse { white-space: pre; }

ol { margin-left: 1.75em; }

ul li ol { margin-left: 1.5em; }

dl dd { margin-left: 1.125em; }

dl dd:last-child, dl dd:last-child > :last-child { margin-bottom: 0; }

ol > li p, ul > li p, ul dd, ol dd, .olist .olist, .ulist .ulist, .ulist .olist, .olist .ulist { margin-bottom: 0.375em; }

ul.unstyled, ol.unnumbered, ul.checklist, ul.none { list-style-type: none; }

ul.unstyled, ol.unnumbered, ul.checklist { margin-left: 0.625em; }

ul.checklist li > p:first-child > i[class^="icon-check"]:first-child, ul.checklist li > p:first-child > input[type="checkbox"]:first-child { margin-right: 0.25em; }

ul.checklist li > p:first-child > input[type="checkbox"]:first-child { position: relative; top: 1px; }

ul.inline { margin: 0 auto 0.375em auto; margin-left: -1.375em; margin-right: 0; padding: 0; list-style: none; overflow: hidden; }
ul.inline > li { list-style: none; float: left; margin-left: 1.375em; display: block; }
ul.inline > li > * { display: block; }

.unstyled dl dt { font-weight: normal; font-style: normal; }

ol.arabic { list-style-type: decimal; }

ol.decimal { list-style-type: decimal-leading-zero; }

ol.loweralpha { list-style-type: lower-alpha; }

ol.upperalpha { list-style-type: upper-alpha; }

ol.lowerroman { list-style-type: lower-roman; }

ol.upperroman { list-style-type: upper-roman; }

ol.lowergreek { list-style-type: lower-greek; }

.hdlist > table, .colist > table { border: 0; background: none; }
.hdlist > table > tbody > tr, .colist > table > tbody > tr { background: none; }

td.hdlist1 { padding-right: .8em; font-weight: bold; }

td.hdlist1, td.hdlist2 { vertical-align: top; }

.literalblock + .colist, .listingblock + .colist { margin-top: -0.5em; }

.colist > table tr > td:first-of-type { padding: 0 .8em; line-height: 1; }
.colist > table tr > td:last-of-type { padding: 0.25em 0; }

.qanda > ol > li > p > em:only-child { color: #004985; }

.thumb, .th { line-height: 0; display: inline-block; border: solid 4px white; -webkit-box-shadow: 0 0 0 1px #dddddd; box-shadow: 0 0 0 1px #dddddd; }

.imageblock.left, .imageblock[style*="float: left"] { margin: 0.25em 0.625em 1.25em 0; }
.imageblock.right, .imageblock[style*="float: right"] { margin: 0.25em 0 1.25em 0.625em; }
.imageblock > .title { margin-bottom: 0; }
.imageblock.thumb, .imageblock.th { border-width: 6px; }
.imageblock.thumb > .title, .imageblock.th > .title { padding: 0 0.125em; }

.image.left, .image.right { margin-top: 0.25em; margin-bottom: 0.25em; display: inline-block; line-height: 0; }
.image.left { margin-right: 0.625em; }
.image.right { margin-left: 0.625em; }

a.image { text-decoration: none; }

span.footnote, span.footnoteref { vertical-align: super; font-size: 0.875em; }
span.footnote a, span.footnoteref a { text-decoration: none; }

#footnotes { padding-top: 0.75em; padding-bottom: 0.75em; margin-bottom: 0.625em; }
#footnotes hr { width: 20%; min-width: 6.25em; margin: -.25em 0 .75em 0; border-width: 1px 0 0 0; }
#footnotes .footnote { padding: 0 0.375em; line-height: 1.3; font-size: 0.875em; margin-left: 1.2em; text-indent: -1.2em; margin-bottom: .2em; }
#footnotes .footnote a:first-of-type { font-weight: bold; text-decoration: none; }
#footnotes .footnote:last-of-type { margin-bottom: 0; }

#content #footnotes { margin-top: -0.625em; margin-bottom: 0; padding: 0.75em 0; }

.gist .file-data > table { border: none; background: #fff; width: 100%; margin-bottom: 0; }
.gist .file-data > table td.line-data { width: 99%; }

div.unbreakable { page-break-inside: avoid; }

.big { font-size: larger; }

.small { font-size: smaller; }

.underline { text-decoration: underline; }

.overline { text-decoration: overline; }

.line-through { text-decoration: line-through; }

.aqua { color: #00bfbf; }

.aqua-background { background-color: #00fafa; }

.black { color: black; }

.black-background { background-color: black; }

.blue { color: #0000bf; }

.blue-background { background-color: #0000fa; }

.fuchsia { color: #bf00bf; }

.fuchsia-background { background-color: #fa00fa; }

.gray { color: #606060; }

.gray-background { background-color: #7d7d7d; }

.green { color: #006000; }

.green-background { background-color: #007d00; }

.lime { color: #00bf00; }

.lime-background { background-color: #00fa00; }

.maroon { color: #600000; }

.maroon-background { background-color: #7d0000; }

.navy { color: #000060; }

.navy-background { background-color: #00007d; }

.olive { color: #606000; }

.olive-background { background-color: #7d7d00; }

.purple { color: #600060; }

.purple-background { background-color: #7d007d; }

.red { color: #bf0000; }

.red-background { background-color: #fa0000; }

.silver { color: #909090; }

.silver-background { background-color: #bcbcbc; }

.teal { color: #006060; }

.teal-background { background-color: #007d7d; }

.white { color: #bfbfbf; }

.white-background { background-color: #fafafa; }

.yellow { color: #bfbf00; }

.yellow-background { background-color: #fafa00; }

span.icon > [class^="icon-"], span.icon > [class*=" icon-"] { cursor: default; }

.admonitionblock td.icon [class^="icon-"]:before { font-size: 2.5em; text-shadow: 1px 1px 2px rgba(0, 0, 0, 0.5); cursor: default; }
.admonitionblock td.icon .icon-note:before { content: "\f05a"; color: #00579e; color: #004176; }
.admonitionblock td.icon .icon-tip:before { content: "\f0eb"; text-shadow: 1px 1px 2px rgba(155, 155, 0, 0.8); color: #111; }
.admonitionblock td.icon .icon-warning:before { content: "\f071"; color: #bf6900; }
.admonitionblock td.icon .icon-caution:before { content: "\f06d"; color: #bf3400; }
.admonitionblock td.icon .icon-important:before { content: "\f06a"; color: #bf0000; }

.conum { display: inline-block; color: white !important; background-color: black; -webkit-border-radius: 100px; border-radius: 100px; text-align: center; width: 20px; height: 20px; font-size: 12px; font-weight: bold; line-height: 20px; font-family: Arial, sans-serif; font-style: normal; position: relative; top: -2px; letter-spacing: -1px; }
.conum * { color: white !important; }
.conum + b { display: none; }
.conum:after { content: attr(data-value); }
.conum:not([data-value]):empty { display: none; }

h1, h2, h3, #toctitle, .sidebarblock > .content > .title, h4, h5, h6 { border-bottom: 1px solid #dddddd; }

#toctitle { color: #00406F; font-weight: normal; margin-top: 1.5em; }

.sidebarblock { border-color: #aaa; }

code { -webkit-border-radius: 4px; border-radius: 4px; }

p.tableblock.header { color: #6d6e71; }


</style>
<script type="text/javascript">
/*<![CDATA[*/
var asciidoc = {  // Namespace.

/////////////////////////////////////////////////////////////////////
// Table Of Contents generator
/////////////////////////////////////////////////////////////////////

/* Author: Mihai Bazon, September 2002
 * http://students.infoiasi.ro/~mishoo
 *
 * Table Of Content generator
 * Version: 0.4
 *
 * Feel free to use this script under the terms of the GNU General Public
 * License, as long as you do not remove or alter this notice.
 */

 /* modified by Troy D. Hanson, September 2006. License: GPL */
 /* modified by Stuart Rackham, 2006, 2009. License: GPL */

// toclevels = 1..4.
toc: function (toclevels) {

  function getText(el) {
    var text = "";
    for (var i = el.firstChild; i != null; i = i.nextSibling) {
      if (i.nodeType == 3 /* Node.TEXT_NODE */) // IE doesn't speak constants.
        text += i.data;
      else if (i.firstChild != null)
        text += getText(i);
    }
    return text;
  }

  function TocEntry(el, text, toclevel) {
    this.element = el;
    this.text = text;
    this.toclevel = toclevel;
  }

  function tocEntries(el, toclevels) {
    var result = new Array;
    var re = new RegExp('[hH]([1-'+(toclevels+1)+'])');
    // Function that scans the DOM tree for header elements (the DOM2
    // nodeIterator API would be a better technique but not supported by all
    // browsers).
    var iterate = function (el) {
      for (var i = el.firstChild; i != null; i = i.nextSibling) {
        if (i.nodeType == 1 /* Node.ELEMENT_NODE */) {
          var mo = re.exec(i.tagName);
          if (mo && (i.getAttribute("class") || i.getAttribute("className")) != "float") {
            result[result.length] = new TocEntry(i, getText(i), mo[1]-1);
          }
          iterate(i);
        }
      }
    }
    iterate(el);
    return result;
  }

  var toc = document.getElementById("toc");
  if (!toc) {
    return;
  }

  // Delete existing TOC entries in case we're reloading the TOC.
  var tocEntriesToRemove = [];
  var i;
  for (i = 0; i < toc.childNodes.length; i++) {
    var entry = toc.childNodes[i];
    if (entry.nodeName.toLowerCase() == 'div'
     && entry.getAttribute("class")
     && entry.getAttribute("class").match(/^toclevel/))
      tocEntriesToRemove.push(entry);
  }
  for (i = 0; i < tocEntriesToRemove.length; i++) {
    toc.removeChild(tocEntriesToRemove[i]);
  }

  // Rebuild TOC entries.
  var entries = tocEntries(document.getElementById("content"), toclevels);
  for (var i = 0; i < entries.length; ++i) {
    var entry = entries[i];
    if (entry.element.id == "")
      entry.element.id = "_toc_" + i;
    var a = document.createElement("a");
    a.href = "#" + entry.element.id;
    a.appendChild(document.createTextNode(entry.text));
    var div = document.createElement("div");
    div.appendChild(a);
    div.className = "toclevel" + entry.toclevel;
    toc.appendChild(div);
  }
  if (entries.length == 0)
    toc.parentNode.removeChild(toc);
},


/////////////////////////////////////////////////////////////////////
// Footnotes generator
/////////////////////////////////////////////////////////////////////

/* Based on footnote generation code from:
 * http://www.brandspankingnew.net/archive/2005/07/format_footnote.html
 */

footnotes: function () {
  // Delete existing footnote entries in case we're reloading the footnodes.
  var i;
  var noteholder = document.getElementById("footnotes");
  if (!noteholder) {
    return;
  }
  var entriesToRemove = [];
  for (i = 0; i < noteholder.childNodes.length; i++) {
    var entry = noteholder.childNodes[i];
    if (entry.nodeName.toLowerCase() == 'div' && entry.getAttribute("class") == "footnote")
      entriesToRemove.push(entry);
  }
  for (i = 0; i < entriesToRemove.length; i++) {
    noteholder.removeChild(entriesToRemove[i]);
  }

  // Rebuild footnote entries.
  var cont = document.getElementById("content");
  var spans = cont.getElementsByTagName("span");
  var refs = {};
  var n = 0;
  for (i=0; i<spans.length; i++) {
    if (spans[i].className == "footnote") {
      n++;
      var note = spans[i].getAttribute("data-note");
      if (!note) {
        // Use [\s\S] in place of . so multi-line matches work.
        // Because JavaScript has no s (dotall) regex flag.
        note = spans[i].innerHTML.match(/\s*\[([\s\S]*)]\s*/)[1];
        spans[i].innerHTML =
          "[<a id='_footnoteref_" + n + "' href='#_footnote_" + n +
          "' title='View footnote' class='footnote'>" + n + "</a>]";
        spans[i].setAttribute("data-note", note);
      }
      noteholder.innerHTML +=
        "<div class='footnote' id='_footnote_" + n + "'>" +
        "<a href='#_footnoteref_" + n + "' title='Return to text'>" +
        n + "</a>. " + note + "</div>";
      var id =spans[i].getAttribute("id");
      if (id != null) refs["#"+id] = n;
    }
  }
  if (n == 0)
    noteholder.parentNode.removeChild(noteholder);
  else {
    // Process footnoterefs.
    for (i=0; i<spans.length; i++) {
      if (spans[i].className == "footnoteref") {
        var href = spans[i].getElementsByTagName("a")[0].getAttribute("href");
        href = href.match(/#.*/)[0];  // Because IE return full URL.
        n = refs[href];
        spans[i].innerHTML =
          "[<a href='#_footnote_" + n +
          "' title='View footnote' class='footnote'>" + n + "</a>]";
      }
    }
  }
},

install: function(toclevels) {
  var timerId;

  function reinstall() {
    asciidoc.footnotes();
    if (toclevels) {
      asciidoc.toc(toclevels);
    }
  }

  function reinstallAndRemoveTimer() {
    clearInterval(timerId);
    reinstall();
  }

  timerId = setInterval(reinstall, 500);
  if (document.addEventListener)
    document.addEventListener("DOMContentLoaded", reinstallAndRemoveTimer, false);
  else
    window.onload = reinstallAndRemoveTimer;
}

}
asciidoc.install(2);
/*]]>*/
</script>
</head>
<body class="article">
<div id="header">
<h1>Manage your Container with Red Hat OpenShift</h1>
<span id="author">Dennis Deitermann &lt;dennis@redhat.com&gt;, Daniel Messer &lt;dmesser@redhat.com&gt;, Lutz Lange &lt;llange@redhat.com&gt;</span><br />
<div id="toc">
  <div id="toctitle">Table of Contents</div>
  <noscript><p><b>JavaScript must be enabled in your browser to display the table of contents.</b></p></noscript>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="_openshift_introduction_lab">1. OpenShift Introduction Lab</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_what_is_openshift">1.1. What is OpenShift ?</h3>
<div class="paragraph"><p>OpenShift Online is Red Hat’s public cloud application development and hosting platform that automates the provisioning, management and scaling of applications so that you can focus on writing the code for your business, startup, or big idea.</p></div>
<div class="paragraph"><p>Here is a Videos explaining OpenShift:
<a href="https://youtu.be/D_Lj0rObunI">Introduction into OpenShift</a><br /></p></div>
<div class="paragraph"><p>Official documentation for <a href="https://docs.openshift.com/container-platform/3.5/welcome/index.html">OpenShift Container Platform</a></p></div>
<div class="sect3">
<h4 id="_overview">1.1.1. Overview</h4>
<div class="paragraph"><p>OpenShift v3 is a layered system designed to expose underlying Docker-formatted container image and Kubernetes concepts as accurately as possible, with a focus on easy composition of applications by a developer. For example, install Ruby, push code, and add MySQL.</p></div>
<div class="paragraph"><p>Unlike OpenShift v2, more flexibility of configuration is exposed after creation in all aspects of the model. The concept of an application as a separate object is removed in favor of more flexible composition of "services", allowing two web containers to reuse a database or expose a database directly to the edge of the network.</p></div>
</div>
<div class="sect3">
<h4 id="_what_are_the_layers">1.1.2. What are the Layers?</h4>
<div class="paragraph"><p>The Docker service provides the abstraction for packaging and creating Linux-based, lightweight container images. Kubernetes provides the cluster management and orchestrates containers on multiple hosts.</p></div>
<div class="paragraph"><p>OpenShift Container Platform adds:</p></div>
<div class="ulist"><ul>
<li>
<p>
Source code management, builds, and deployments for developers
</p>
</li>
<li>
<p>
Managing and promoting images at scale as they flow through your system
</p>
</li>
<li>
<p>
Application management at scale
</p>
</li>
<li>
<p>
Team and user tracking for organizing a large developer organization
</p>
</li>
</ul></div>
<div class="imageblock">
<div class="content">
<img src="http://www.rhpet.de/pictures/OpenShift-Architecture.png" alt="OpenShift Architecture" />
</div>
</div>
</div>
<div class="sect3">
<h4 id="_core_concepts">1.1.3. Core Concepts</h4>
<div class="paragraph"><p>The following topics provide high-level, architectural information on core concepts and objects you will encounter when using OpenShift Container Platform. Many of these objects come from Kubernetes, which is extended by OpenShift Container Platform to provide a more feature-rich development lifecycle platform.</p></div>
<div class="ulist"><ul>
<li>
<p>
<a href="https://docs.openshift.com/container-platform/3.5/architecture/core_concepts/containers_and_images.html#architecture-core-concepts-containers-and-images">Containers and images</a> are the building blocks for deploying your applications.
</p>
</li>
<li>
<p>
<a href="https://docs.openshift.com/container-platform/3.5/architecture/core_concepts/pods_and_services.html">Pods and services</a> allow for containers to communicate with each other and proxy connections.
</p>
</li>
<li>
<p>
<a href="https://docs.openshift.com/container-platform/3.5/architecture/core_concepts/projects_and_users.html">Projects and users</a> provide the space and means for communities to organize and manage their content together.
</p>
</li>
<li>
<p>
<a href="https://docs.openshift.com/container-platform/3.5/architecture/core_concepts/builds_and_image_streams.html">Builds and image streams</a> allow you to build working images and react to new images.
</p>
</li>
<li>
<p>
<a href="https://docs.openshift.com/container-platform/3.5/architecture/core_concepts/deployments.html">Deployments</a> add expanded support for the software development and deployment lifecycle.
</p>
</li>
<li>
<p>
<a href="https://docs.openshift.com/container-platform/3.5/architecture/core_concepts/routes.html">Routes</a> announce your service to the world.
</p>
</li>
<li>
<p>
<a href="https://docs.openshift.com/container-platform/3.5/architecture/core_concepts/templates.html">Templates</a> allow for many objects to be created at once based on customized parameters.
</p>
</li>
</ul></div>
<div class="paragraph"><p>Click on the links above to get more information about the respective topic.</p></div>
</div>
</div>
<div class="sect2">
<h3 id="_how_to_access_the_lab_environment">1.2. How to access the Lab Environment</h3>
<div class="paragraph"><p>First login to the ssh gateway with the user <code>rhpet</code>:</p></div>
<div class="paragraph"><p>To obtain you gateway ip adress <a href="https://www.opentlc.com/pc-status/index.php">login here</a>.</p></div>
<div class="paragraph"><p>Login into the status page with your user <code>luXX</code> and your password <code>Frank_XX_furt</code>. Please use the lab number on <a href="http://192.168.103.200/dokuwiki/doku.php">this side</a> in the lower left as <code>XX</code>.</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[user@localhost ~]$ ssh rhpet@&lt;your gateway VM ip adress&gt;

The authenticity of host '85.190.180.158 (85.190.180.158)' can't be established.
ECDSA key fingerprint is SHA256:bsDGeuXiG1zpM3RlsN+RlaAPRaDSi6Y/sJoBP2IXNqU.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '85.190.180.158' (ECDSA) to the list of known hosts.
rhpet@85.190.180.158's password:
Last login: Sun Mar 13 16:28:23 2016 from ipbcc3d2c2.dynamic.kabel-deutschland.de
[rhpet@gw ~]$</code></pre>
</div></div>
<div class="paragraph"><p>You will find the password for the rhpet user in the <a href="http://192.168.103.200/dokuwiki/doku.php?id=managing_rhv_with_ansible_playground">Lab description in the Dokuwikki</a>.</p></div>
<div class="paragraph"><p>Then get the power of root:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[rhpet@gw ~]$ su -</code></pre>
</div></div>
<div class="paragraph"><p>The root pw is <code>r3dh4t1!</code></p></div>
<div class="paragraph"><p>For HTTP &amp; HTTPS connections we need to configure a Proxy in your Webbrowser. We tested it with Firefox.
Please go to <code>Settings</code> → <code>Advanced</code> → <code>Network</code> → <code>Settings</code></p></div>
<div class="imageblock">
<div class="content">
<img src="http://www.rhpet.de/pictures/Firefox-Proxy.png" alt="Firefox Proxy configuration" />
</div>
</div>
<div class="paragraph"><p>Please use your gateway IP adress, port 80 and check the checkbox at "Use this proxy server for all protocols".</p></div>
<div class="paragraph"><p>Please add <code>192.168.103.200</code> to the <code>No Proxy for:</code> field. When you don&#8217;t do this, you cannot reach the Dokuwiki anymore.</p></div>
<div class="paragraph"><p>Username for the Proxy is: <code>admin</code><br />
Password for the Proxy is: <code>r3dh4t1!</code></p></div>
</div>
<div class="sect2">
<h3 id="_lab_reference">1.3. Lab Reference</h3>
<div class="tableblock">
<table rules="all"
width="100%"
frame="border"
cellspacing="0" cellpadding="4">
<col width="33%" />
<col width="33%" />
<col width="33%" />
<thead>
<tr>
<th align="left" valign="top"> VM Name</th>
<th align="left" valign="top"> FQDN </th>
<th align="left" valign="top"> IP</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p class="table">SSH Gateway, DNS &amp; Proxy Server</p></td>
<td align="left" valign="top"><p class="table">gw.example.com</p></td>
<td align="left" valign="top"><p class="table">192.168.0.250, Ports 22&amp;80 are open</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">Master</p></td>
<td align="left" valign="top"><p class="table">master.example.com</p></td>
<td align="left" valign="top"><p class="table">192.168.0.100, Port 8443 is open</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">Infranode</p></td>
<td align="left" valign="top"><p class="table">infranode.example.com</p></td>
<td align="left" valign="top"><p class="table">192.168.0.101</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">App Node 1</p></td>
<td align="left" valign="top"><p class="table">node1.example.com</p></td>
<td align="left" valign="top"><p class="table">192.168.0.102</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">App Node 2</p></td>
<td align="left" valign="top"><p class="table">node2.example.com</p></td>
<td align="left" valign="top"><p class="table">192.168.0.103</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">App Node 3</p></td>
<td align="left" valign="top"><p class="table">node3.example.com</p></td>
<td align="left" valign="top"><p class="table">192.168.0.104</p></td>
</tr>
</tbody>
</table>
</div>
<div class="tableblock">
<table rules="all"
width="100%"
frame="border"
cellspacing="0" cellpadding="4">
<col width="33%" />
<col width="33%" />
<col width="33%" />
<thead>
<tr>
<th align="left" valign="top"> Name </th>
<th align="left" valign="top"> Password </th>
<th align="left" valign="top"> Role</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p class="table">luXX</p></td>
<td align="left" valign="top"><p class="table">ask the instructor</p></td>
<td align="left" valign="top"><p class="table">ssh user to connect to the gateway VM</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">root</p></td>
<td align="left" valign="top"><p class="table">r3dh4t1!</p></td>
<td align="left" valign="top"><p class="table">root user for all VMs</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">admin</p></td>
<td align="left" valign="top"><p class="table">r3dh4t1!</p></td>
<td align="left" valign="top"><p class="table">OSCP &amp; CloudForms Administrator &amp; Auth user for the Proxy</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">marina</p></td>
<td align="left" valign="top"><p class="table">r3dh4t1!</p></td>
<td align="left" valign="top"><p class="table">Developer/User</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">andrew</p></td>
<td align="left" valign="top"><p class="table">r3dh4t1!</p></td>
<td align="left" valign="top"><p class="table">Developer/User</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_introduction_into_the_cli_of_openshift">1.4. Introduction into the CLI of OpenShift</h3>
<div class="paragraph"><p>With the OpenShift Container Platform command line interface (CLI), you can create applications and manage OpenShift Container Platform projects from a terminal. The CLI is ideal in situations where you are:</p></div>
<div class="ulist"><ul>
<li>
<p>
Working directly with project source code.
</p>
</li>
<li>
<p>
Scripting OpenShift Container Platform operations.
</p>
</li>
<li>
<p>
Restricted by bandwidth resources and cannot use the web console.
</p>
</li>
</ul></div>
<div class="paragraph"><p>The CLI is available using the <code>oc</code> command:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>$ oc &lt;command&gt;</code></pre>
</div></div>
<div class="sect3">
<h4 id="_basic_setup_and_login">1.4.1. Basic Setup and Login</h4>
<div class="paragraph"><p>The <code>oc login</code> command is the best way to initially set up the CLI, and it serves as the entry point for most users. The interactive flow helps you establish a session to an OpenShift Container Platform server with the provided credentials. The information is automatically saved in a CLI configuration file that is then used for subsequent commands.</p></div>
<div class="paragraph"><p>Login into the master host and the login into OpenShift as admin user:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@gw ~]# ssh master
Last login: Thu Jun  8 10:10:12 2017 from 192.168.0.250</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc login https://master.example.com:8443

Authentication required for https://master.example.com:8443 (openshift)
Username: admin
Password: r3dh4t1!
Login successful.

You have access to the following projects and can switch between them with 'oc project &lt;projectname&gt;':

    default
    kube-system
    logging
    management-infra
    openshift
  * openshift-infra

Using project "openshift-infra".</code></pre>
</div></div>
<div class="paragraph"><p>You can log out of CLI using the <code>oc logout</code> command. But we don&#8217;t do this now.</p></div>
</div>
<div class="sect3">
<h4 id="_projects">1.4.2. Projects</h4>
<div class="paragraph"><p>A project in OpenShift Container Platform contains multiple objects to make up a logical application.</p></div>
<div class="paragraph"><p>Most oc commands run in the context of a project. The <code>oc login</code> selects a default project during initial setup to be used with subsequent commands. Use the following command to display the project currently in use:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc project</code></pre>
</div></div>
<div class="paragraph"><p>If you have access to multiple projects, use the following syntax to switch to a particular project by specifying the project name:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc project default

Now using project "default" on server "https://master.example.com:8443".</code></pre>
</div></div>
<div class="paragraph"><p>The <code>oc status</code> command shows a high level overview of the project currently in use, with its components and their relationships, as shown in the following example:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc status

In project default on server https://master.example.com:8443

https://docker-registry-default.cloudapps.example.com (passthrough) to pod port 5000-tcp (svc/docker-registry)
  dc/docker-registry deploys docker.io/openshift3/ose-docker-registry:v3.5.5.8
    deployment #1 deployed 3 weeks ago - 1 pod

svc/kubernetes - 172.30.0.1 ports 443, 53-&gt;8053, 53-&gt;8053

https://registry-console-default.cloudapps.example.com (passthrough) to pod port registry-console (svc/registry-console)
  dc/registry-console deploys registry.access.redhat.com/openshift3/registry-console:3.5
    deployment #1 deployed 2 days ago - 1 pod

svc/router - 172.30.49.219 ports 80, 443, 1936
  dc/router deploys docker.io/openshift3/ose-haproxy-router:v3.5.5.8
    deployment #1 deployed 3 weeks ago - 1 pod</code></pre>
</div></div>
<div class="paragraph"><p>If you want to learn more about the <code>oc</code> command, please look at the following documentation:<br />
<a href="https://docs.openshift.com/container-platform/3.5/cli_reference/basic_cli_operations.html">Developer CLI Operations</a><br />
<a href="https://docs.openshift.com/container-platform/3.5/cli_reference/admin_cli_operations.html">Administrator CLI Operations</a></p></div>
</div>
</div>
<div class="sect2">
<h3 id="_verify_your_openshift_environment">1.5. Verify Your OpenShift Environment</h3>
<div class="paragraph"><p>Login into the master host:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@gw ~]# ssh master</code></pre>
</div></div>
<div class="paragraph"><p>Make sure that oc is in the default project</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@gw ~]# oc project default
Now using project "default" on server "https://master.example.com:8443".</code></pre>
</div></div>
<div class="paragraph"><p>Run oc get nodes to check the status of your hosts:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get nodes
NAME                    STATUS                     AGE
infranode.example.com   Ready                      28d
master.example.com      Ready,SchedulingDisabled   28d
node1.example.com       Ready                      28d
node2.example.com       Ready                      28d
node3.example.com       Ready                      28d</code></pre>
</div></div>
<div class="paragraph"><p>Check if the installer has deployed the router and the registry containers:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get pods
NAME                       READY     STATUS    RESTARTS   AGE
docker-registry-1-26xs7    1/1       Running   9          28d
registry-console-1-tbwwj   1/1       Running   5          8d
router-1-xq3r6             1/1       Running   12         28d</code></pre>
</div></div>
<div class="paragraph"><p>Use your browser to connect to the OpenShift web console at <a href="https://master.example.com:8443/">https://master.example.com:8443/</a> and accept the untrusted Certificate.
Please don&#8217;t login this time. We will do this a little bit later.</p></div>
</div>
<div class="sect2">
<h3 id="_configure_openshift">1.6. Configure OpenShift</h3>
<div class="paragraph"><p>In this section, you check the labels and do some intial configuration.</p></div>
</div>
<div class="sect2">
<h3 id="_pods">1.7. Pods</h3>
<div class="paragraph"><p>OpenShift leverages the Kubernetes concept of a pod, which is one or more containers deployed together on one host, and the smallest compute unit that can be defined, deployed, and managed.</p></div>
<div class="paragraph"><p>Pods are the rough equivalent of OpenShift v2 gears, with containers the rough equivalent of v2 cartridge instances. Each pod is allocated its own internal IP address, therefore owning its entire port space, and containers within pods can share their local storage and networking.</p></div>
<div class="paragraph"><p>Pods have a lifecycle; they are defined, then they are assigned to run on a node, then they run until their container(s) exit or they are removed for some other reason. Pods, depending on policy and exit code, may be removed after exiting, or may be retained in order to enable access to the logs of their containers.</p></div>
<div class="paragraph"><p>OpenShift treats pods as largely immutable; changes cannot be made to a pod definition while it is running. OpenShift implements changes by terminating an existing pod and recreating it with modified configuration, base image(s), or both. Pods are also treated as expendable, and do not maintain state when recreated. Therefore pods should usually be managed by higher-level controllers, rather than directly by users.</p></div>
</div>
<div class="sect2">
<h3 id="_labels">1.8. Labels</h3>
<div class="paragraph"><p>Labels are used to organize, group, or select API objects. For example, pods are "tagged" with labels, and then services use label selectors to identify the pods they proxy to. This makes it possible for services to reference groups of pods, even treating pods with potentially different containers as related entities.</p></div>
<div class="paragraph"><p>Most objects can include labels in their metadata. So labels can be used to group arbitrarily-related objects; for example, all of the pods, services, replication controllers, and deployment configurations of a particular application can be grouped.</p></div>
<div class="paragraph"><p>Labels are simple key/value pairs, as in the following example:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>labels:
  key1: value1
  key2: value2</code></pre>
</div></div>
<div class="paragraph"><p>Consider:</p></div>
<div class="ulist"><ul>
<li>
<p>
A pod consisting of an <strong>nginx</strong> container, with the label <strong>role=webserver</strong>.
</p>
</li>
<li>
<p>
A pod consisting of an <strong>Apache httpd</strong> container, with the same label <strong>role=webserver</strong>.
</p>
</li>
</ul></div>
<div class="paragraph"><p>A service or replication controller that is defined to use pods with the <strong>role=webserver</strong> label treats both of these pods as part of the same group.</p></div>
</div>
<div class="sect2">
<h3 id="_set_regions_and_zones">1.9. Set Regions and Zones</h3>
<div class="paragraph"><p>We have already labeled your nodes.</p></div>
<div class="paragraph"><p>Check the labels of the nodes:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get nodes --show-labels
NAME                    STATUS                     AGE       LABELS
infranode.example.com   Ready                      22d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=infranode.example.com,region=infra,zone=infranodes
master.example.com      Ready,SchedulingDisabled   22d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master.example.com,region=master
node1.example.com       Ready                      22d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node1.example.com,region=primary,zone=east
node2.example.com       Ready                      22d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node2.example.com,region=primary,zone=west
node3.example.com       Ready                      22d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node3.example.com,region=primary,zone=north</code></pre>
</div></div>
<div class="paragraph"><p>You now have a running OpenShift environment across five hosts with one master and four nodes, divided into three regions: master, infra and primary.</p></div>
<div class="paragraph"><p>Check that registry and router are running on the infranode:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get pods -o wide
NAME                       READY     STATUS    RESTARTS   AGE       IP              NODE
docker-registry-1-26xs7    1/1       Running   5          22d       10.128.0.10     infranode.example.com
registry-console-1-tbwwj   1/1       Running   1          2d        10.128.0.11     infranode.example.com
router-1-xq3r6             1/1       Running   7          22d       192.168.0.101   infranode.example.com</code></pre>
</div></div>
<div class="paragraph"><p>As you can see, all infrastructure pods are running on the infranode, because we configured a default node selector for this.
Please look <a href="https://blog.openshift.com/deploying-applications-to-specific-nodes/">here</a> for more information.</p></div>
</div>
<div class="sect2">
<h3 id="_optional_check_registry">1.10. [ Optional ] Check Registry</h3>
<div class="paragraph"><p>In this lab scenario, infranode is the target for both the registry and the default router.</p></div>
<div class="paragraph"><p>To check the URL of the docker registry run <code>oc status</code>:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc status
In project default on server https://master.example.com:8443

https://docker-registry-default.cloudapps.example.com (passthrough) to pod port 5000-tcp (svc/docker-registry)
  dc/docker-registry deploys docker.io/openshift3/ose-docker-registry:v3.5.5.8
    deployment #1 deployed 3 weeks ago - 1 pod

svc/kubernetes - 172.30.0.1 ports 443, 53-&gt;8053, 53-&gt;8053

https://registry-console-default.cloudapps.example.com (passthrough) to pod port registry-console (svc/registry-console)
  dc/registry-console deploys registry.access.redhat.com/openshift3/registry-console:3.5
    deployment #1 deployed 2 days ago - 1 pod

svc/router - 172.30.49.219 ports 80, 443, 1936
  dc/router deploys docker.io/openshift3/ose-haproxy-router:v3.5.5.8
    deployment #1 deployed 3 weeks ago - 1 pod</code></pre>
</div></div>
<div class="paragraph"><p>Test the status of the registry with the curl command to communicate with the registry’s service port, <code>curl -v https://registry-console-default.cloudapps.example.com --insecure</code>.</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# curl -v https://registry-console-default.cloudapps.example.com --insecure | grep "Red Hat Container Registry"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* About to connect() to registry-console-default.cloudapps.example.com port 443 (#0)
*   Trying 192.168.0.101...
* Connected to registry-console-default.cloudapps.example.com (192.168.0.101) port 443 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
* skipping SSL peer certificate verification
* SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
* Server certificate:
*       subject: CN=registry-console-1-tbwwj
*       start date: Jun 08 11:03:26 2017 GMT
*       expire date: Mai 15 11:03:27 2117 GMT
*       common name: registry-console-1-tbwwj
*       issuer: CN=registry-console-1-tbwwj
&gt; GET / HTTP/1.1
&gt; User-Agent: curl/7.29.0
&gt; Host: registry-console-default.cloudapps.example.com
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; Content-Security-Policy: default-src 'self' 'unsafe-inline'; connect-src 'self' ws: wss:
&lt; Transfer-Encoding: chunked
&lt; Cache-Control: no-cache, no-store
&lt;
{ [data not shown]
var environment = {"page":{"title":"Red Hat Container Registry","connect":true},"hostname":"registry-console-1-tbwwj","os-release":{"NAME":"Red Hat Container Registry","ID":"registry","PRETTY_NAME":"Red Hat Container Registry"},"OAuth":{"URL":"https://master.example.com:8443//oauth/authorize?client_id=cockpit-oauth-client&amp;response_type=token","ErrorParam":null,"TokenParam":null}};
100 42229    0 42229    0     0   212k      0 --:--:-- --:--:-- --:--:--  213k
* Connection #0 to host registry-console-default.cloudapps.example.com left intact</code></pre>
</div></div>
<div class="sect3">
<h4 id="_set_regions_and_zones_2">1.10.1. Set Regions and Zones</h4>
<div class="paragraph"><p>We have already labeled your nodes.</p></div>
<div class="paragraph"><p>Check the labels of the nodes:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get nodes --show-labels
infranode.example.com   Ready                      28d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=infranode.example.com,region=infra,zone=infranodes
master.example.com      Ready,SchedulingDisabled   28d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master.example.com,region=master
node1.example.com       Ready                      28d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node1.example.com,region=primary,zone=east
node2.example.com       Ready                      28d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node2.example.com,region=primary,zone=west
node3.example.com       Ready                      28d       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node3.example.com,region=primary,zone=north</code></pre>
</div></div>
<div class="paragraph"><p>You now have a running OpenShift Container Platform environment across four hosts with one master and three nodes, divided into two regions: infra and primary.</p></div>
<div class="paragraph"><p>Check that registry, registry-console and router are running on the infranode:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get pods -o wide -n default
NAME                       READY     STATUS    RESTARTS   AGE       IP              NODE
docker-registry-1-26xs7    1/1       Running   9          28d       10.128.0.19     infranode.example.com
registry-console-1-tbwwj   1/1       Running   5          8d        10.128.0.18     infranode.example.com
router-1-xq3r6             1/1       Running   12         28d       192.168.0.101   infranode.example.com</code></pre>
</div></div>
<div class="paragraph"><p>As you can see, all infrastructure pods are running on the infranode, because we configured a default node selector for this.
Please look <a href="https://blog.openshift.com/deploying-applications-to-specific-nodes/">here</a> for more information.</p></div>
</div>
</div>
<div class="sect2">
<h3 id="_optional_resource_management_lab">1.11. [ Optional ] Resource Management Lab</h3>
<div class="paragraph"><p>In this lab, you learn how to manage OpenShift Container Platform resources.</p></div>
<div class="ulist"><ul>
<li>
<p>
<strong>Manage Users, Projects, and Quotas</strong>
</p>
<div class="paragraph"><p>In this section, you create projects and test the use of quotas and limits.</p></div>
</li>
<li>
<p>
<strong>Create Services and Routes</strong>
</p>
<div class="paragraph"><p>In this section, you manually create services and routes for pods and review the changes to a service when scaling an application.</p></div>
</li>
<li>
<p>
<strong>Explore Containers</strong>
</p>
<div class="paragraph"><p>In this section, you run commands within active pods and explore the <code>docker-registry</code> and <code>Default Router</code> containers.</p></div>
</li>
</ul></div>
<div class="sect3">
<h4 id="_manage_users_projects_and_quotas">1.11.1. Manage Users, Projects, and Quotas</h4>
<div class="sect4">
<h5 id="_create_project">Create Project</h5>
<div class="paragraph"><p>On the master host, run <code>oadm</code> to create and assign the administrative user <code>andrew</code> to a project:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oadm new-project resourcemanagement --display-name="Resources Management" \
--description="This is the project we use to learn about resource management" \
--admin=andrew  --node-selector='region=primary'</code></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content"><code>andrew</code> can create his own project with the <code>oc new-project</code> command, an option you will experiment with later in this course. Note that defining the <code>--node-selector</code> is optional.</td>
</tr></table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_view_resources_in_web_console">1.11.2. View Resources in Web Console</h4>
<div class="paragraph"><p>Now have a look at the web console.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Open your web browser and go to <a href="https://master.example.com:8443">https://master.example.com:8443</a>
</p>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">
<div class="paragraph"><p>The web console could take up to 90 seconds to become available after a restart of the master.</p></div>
</td>
</tr></table>
</div>
</li>
<li>
<p>
When prompted, type the username and password, as follows:
</p>
<div class="ulist"><ul>
<li>
<p>
<strong>Username</strong>: <code>andrew</code>
</p>
</li>
<li>
<p>
<strong>Password</strong>: <code>r3dh4t1!</code>
</p>
</li>
</ul></div>
</li>
<li>
<p>
In the web console, click the <strong>Resources Management</strong> project.
</p>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">The project is empty because it has no apps. You change that as part of this lab.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">
<div class="title">An error occured getting metrics</div>
<div class="paragraph"><p>We are using self signed certificates here, this is why your browser can&#8217;t contact our metrics stack. Click "OpenMetricsURL and accept the certificate in your Browser or ignore the error for now.</p></div>
</td>
</tr></table>
</div>
</li>
</ol></div>
<div class="sect4">
<h5 id="_apply_quota_to_project">Apply Quota to Project</h5>
<div class="paragraph"><p>A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per project. It can limit the quantity of objects that can be created in a project by type, as well as the total amount of compute resources and storage that may be consumed by resources in that project.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Create a quota definition file:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# cat &lt;&lt; EOF &gt; quota.json
{
  "apiVersion": "v1",
  "kind": "ResourceQuota",
  "metadata": {
    "name": "test-quota"
  },
  "spec": {
    "hard": {
      "memory": "512Mi",
      "cpu": "20",
      "pods": "3",
      "services": "5",
      "replicationcontrollers":"5",
      "resourcequotas":"1"
    }
  }
}
EOF</code></pre>
</div></div>
</li>
<li>
<p>
On the master host, do the following:
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Run <code>oc create</code> to apply the file you just created:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc create -f quota.json --namespace=resourcemanagement</code></pre>
</div></div>
</li>
<li>
<p>
Verify that the quota exists:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get quota -n resourcemanagement
NAME         AGE
test-quota   11s</code></pre>
</div></div>
</li>
<li>
<p>
Verify the limits and examine the usage:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc describe quota test-quota -n resourcemanagement
Name:                   test-quota
Namespace:              resourcemanagement
Resource                Used    Hard
--------                ----    ----
cpu                     0       20
memory                  0       512Mi
pods                    0       3
replicationcontrollers  0       5
resourcequotas          1       1
services                0       5</code></pre>
</div></div>
</li>
</ol></div>
</li>
<li>
<p>
On the web console, click the <strong>Resource Management</strong> project.
</p>
</li>
<li>
<p>
Click the <strong>Resources</strong> tab
</p>
</li>
<li>
<p>
Click <strong>Quota</strong> for information about the quota set.
</p>
</li>
</ol></div>
</div>
</div>
<div class="sect3">
<h4 id="_apply_limit_ranges_to_project">1.11.3. Apply Limit Ranges to Project</h4>
<div class="paragraph"><p>For quotas to be effective, you must create <em>limit ranges</em>. They allocate the maximum, minimum, and default memory and CPU at both the pod and container level. Deployments to projects with a quota set will fail, if there are no default limits set for containers and pods. Pod and Containers with no limits are called unbound and are forbidden to run in quota projects.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Create the <code>limits.json</code> file:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# cat &lt;&lt; EOF &gt; limits.json
{
    "kind": "LimitRange",
    "apiVersion": "v1",
    "metadata": {
        "name": "limits",
        "creationTimestamp": null
    },
    "spec": {
        "limits": [
            {
                "type": "Pod",
                "max": {
                    "cpu": "500m",
                    "memory": "750Mi"
                },
                "min": {
                    "cpu": "10m",
                    "memory": "5Mi"
                }
            },
            {
                "type": "Container",
                "max": {
                    "cpu": "500m",
                    "memory": "750Mi"
                },
                "min": {
                    "cpu": "10m",
                    "memory": "5Mi"
                },
                "default": {
                    "cpu": "100m",
                    "memory": "100Mi"
                }
            }
        ]
    }
}
EOF</code></pre>
</div></div>
</li>
<li>
<p>
On the master host, run <code>oc create</code> against the <code>limits.json</code> file and the
 <code>resourcemanagement</code> project:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc create -f limits.json --namespace=resourcemanagement</code></pre>
</div></div>
</li>
<li>
<p>
Review your limit ranges:
</p>
</li>
</ol></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc describe limitranges limits -n resourcemanagement
Name:           limits
Namespace:      resourcemanagement
Type            Resource        Min     Max     Default Request Default Limit   Max Limit/Request Ratio
----            --------        ---     ---     --------------- -------------   -----------------------
Pod             cpu             10m     500m    -               -               -
Pod             memory          5Mi     750Mi   -               -               -
Container       cpu             10m     500m    100m            100m            -
Container       memory          5Mi     750Mi   100Mi           100Mi           -</code></pre>
</div></div>
</div>
<div class="sect3">
<h4 id="_test_quota_and_limit_settings">1.11.4. Test Quota and Limit Settings</h4>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">You are running commands as the Linux users <code>andrew</code> and <code>root</code> in a lab environment. As a user it is unusual to use the <code>oc</code> command directly on the master. It is common to install <code>oc</code> on your workstation or notebook. You can get the Upstream OpenShift client tools for your operating system <a href="https://github.com/openshift/origin/releases/tag/v1.5.1">here</a>. And the OpenShift Client tools with support are found on the <a href="https://access.redhat.com/downloads/content/290">Red Hat Customer Portal</a>.</td>
</tr></table>
</div>
<div class="paragraph"><p>The following lab will be done on the command line interface.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Authenticate to OpenShift Container Platform and choose your project:
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Connect to the shell of the OpenShift Container Platform master according to the procedure you followed
 previously.
</p>
</li>
<li>
<p>
When prompted, type the username and password:
</p>
<div class="ulist"><ul>
<li>
<p>
<strong>Username</strong>: <code>andrew</code>
</p>
</li>
<li>
<p>
<strong>Password</strong>: <code>r3dh4t1!</code>
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# su - andrew
[andrew@master ~]$ oc login https://master.example.com:8443 -u andrew</code></pre>
</div></div>
<div class="ulist"><ul>
<li>
<p>
The output is as follows:
</p>
<div class="listingblock">
<div class="content">
<pre><code>Login successful.

You have one project on this server: "resourcemanagement"

Using project "resourcemanagement".
Welcome! See 'oc help' to get started.</code></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">This lab shows you the manual, step-by-step method of creating each object. This is done only for educational purpose. There are easier ways to create deployments and all the required objects. The most powerful way to create apps on OpenShift is the <code>oc new-app</code> command, which is covered later in this lab.</td>
</tr></table>
</div>
</li>
</ul></div>
</li>
</ul></div>
</li>
</ol></div>
</li>
<li>
<p>
Create the <code>hello-pod.json</code> pod definition file:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ cat &lt;&lt;EOF &gt; hello-pod.json
{
  "kind": "Pod",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-openshift",
    "creationTimestamp": null,
    "labels": {
      "name": "hello-openshift"
    }
  },
  "spec": {
    "containers": [
      {
        "name": "hello-openshift",
        "image": "openshift/hello-openshift:v1.5.1",
        "ports": [
          {
            "containerPort": 8080,
            "protocol": "TCP"
          }
        ],
        "resources": {
        },
        "terminationMessagePath": "/dev/termination-log",
        "imagePullPolicy": "IfNotPresent",
        "capabilities": {},
        "securityContext": {
          "capabilities": {},
          "privileged": false
        }
      }
    ],
    "restartPolicy": "Always",
    "dnsPolicy": "ClusterFirst",
    "serviceAccount": ""
  },
  "status": {}
}
EOF</code></pre>
</div></div>
</li>
</ol></div>
<div class="sect4">
<h5 id="_run_pod">Run Pod</h5>
<div class="paragraph"><p>Here, you create a simple pod without a <em>route</em> or <em>service</em>:</p></div>
<div class="paragraph"><p>Create and verify the <code>hello-openshift</code> pod:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc create -f hello-pod.json

pod "hello-openshift" created</code></pre>
</div></div>
<div class="paragraph"><p>Wait a few seconds until the pod is up and running. (~40 seconds are needed)</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc get pods

NAME              READY     STATUS    RESTARTS   AGE
hello-openshift   1/1       Running   0          41s</code></pre>
</div></div>
<div class="paragraph"><p>Run <code>oc describe</code> for details on your pod:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc describe pod hello-openshift

Name:                   hello-openshift
Namespace:              resourcemanagement
Security Policy:        restricted
Node:                   node2.example.com/192.168.0.103
Start Time:             Tue, 25 Apr 2017 19:15:01 -0400
Labels:                 name=hello-openshift
Status:                 Running
IP:                     10.130.0.2
Controllers:            &lt;none&gt;
Containers:
  hello-openshift:
    Container ID:       docker://2674481be26d544323fa637c1cc5ba36a5eaafd4707f7735b2620045c495cb07
    Image:              openshift/hello-openshift:v1.5.1
    Image ID:           docker-pullable://docker.io/openshift/hello-openshift@sha256:7ce9d7b0c83a3abef41e0db590c5aa39fb05793315c60fd907f2c609997caf11
    Port:               8080/TCP
    Limits:
      cpu:      100m
      memory:   100Mi
    Requests:
      cpu:              100m
      memory:           100Mi
    State:              Running
      Started:          Tue, 25 Apr 2017 19:15:39 -0400
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ylt00 (ro)
    Environment Variables:      &lt;none&gt;
Conditions:
  Type          Status
  Initialized   True
  Ready         True
  PodScheduled  True
Volumes:
  default-token-ylt00:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-ylt00
QoS Class:      Guaranteed
Tolerations:    &lt;none&gt;
Events:
  FirstSeen     LastSeen        Count   From                            SubobjectPath                   Type            Reason          Message
  ---------     --------        -----   ----                            -------------                   --------        ------          -------
  2m            2m              1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned hello-openshift to node2.example.com
  1m            1m              1       {kubelet node2.example.com}     spec.containers{hello-openshift}        Normal          Pulling         pulling image "openshift/hello-openshift:v1.5.1"
  1m            1m              1       {kubelet node2.example.com}     spec.containers{hello-openshift}        Normal          Pulled          Successfully pulled image "openshift/hello-openshift:v1.5.1"
  1m            1m              1       {kubelet node2.example.com}     spec.containers{hello-openshift}        Normal          Created         Created container with docker id 2674481be26d; Security:[seccomp=unconfined]
  1m            1m              1       {kubelet node2.example.com}     spec.containers{hello-openshift}        Normal          Started         Started container with docker id 2674481be26d</code></pre>
</div></div>
<div class="paragraph"><p>Test that your pod is responding with <code>Hello OpenShift</code>:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ ip=`oc describe pod hello-openshift|grep IP:|awk '{print $2}'`
[andrew@master ~]$ curl http://${ip}:8080</code></pre>
</div></div>
<div class="ulist"><ul>
<li>
<p>
This output denotes a correct response:
</p>
<div class="listingblock">
<div class="content">
<pre><code>Hello OpenShift!</code></pre>
</div></div>
</li>
</ul></div>
<div class="paragraph"><p>Delete all the objects in your <code>hello-pod.json</code> definition file, which, at this point, is the pod only:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc delete -f hello-pod.json
pod "hello-openshift" deleted</code></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/tip.png" alt="Tip" />
</td>
<td class="content">You can also delete a pod using the following command format: #oc delete pod &lt;PODNAME&gt;.</td>
</tr></table>
</div>
<div class="paragraph"><p>Create a new definition file that launches four <code>hello-openshift</code> pods:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ cat &lt;&lt; EOF &gt; hello-many-pods.json
{
  "metadata":{
    "name":"quota-pod-deployment-test"
  },
  "kind":"List",
  "apiVersion":"v1",
  "items":[
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-1",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.5.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-2",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.5.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-3",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.5.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-4",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.5.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    }
  ]
}
EOF</code></pre>
</div></div>
<div class="paragraph"><p>Create the items in the <code>hello-many-pods.json</code> file:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc create -f hello-many-pods.json

pod "hello-openshift-1" created
pod "hello-openshift-2" created
pod "hello-openshift-3" created
Error from server: pods "hello-openshift-4" is forbidden: Exceeded quota: test-quota, requested: pods=1, used: pods=3, limited: pods=3</code></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">Because you defined a quota before, <code>oc create</code> created three pods only instead of four.</td>
</tr></table>
</div>
<div class="paragraph"><p>Delete the object in the <code>hello-many-pods.json</code> definition file:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc delete -f hello-many-pods.json</code></pre>
</div></div>
<div class="paragraph"><p>(Optional) Create a project, set the quota with a pod value of <code>10</code>, and run <code>hello-many-pods.json</code>.</p></div>
</div>
</div>
<div class="sect3">
<h4 id="_optional_create_services_and_routes">1.11.5. [ Optional ] Create Services and Routes</h4>
<div class="paragraph"><p>As <code>andrew</code>, create a project called <code>scvslab</code>:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc new-project svcslab --display-name="Services Lab" --description="This is the project we use to learn about services"</code></pre>
</div></div>
<div class="paragraph"><p>The output looks like this:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>Now using project "svcslab" on server "https://master.example.com:8443".

You can add applications to this project with the 'new-app' command. For example, try:

    $ oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-hello-world.git

to build a new hello-world application in Ruby.</code></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/tip.png" alt="Tip" />
</td>
<td class="content">To switch between projects, run <code>oc project _projectname_</code>.</td>
</tr></table>
</div>
<div class="paragraph"><p>Create the <code>hello-service.json</code> file:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ cat &lt;&lt;EOF &gt; hello-service.json
{
  "kind": "Service",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-service",
    "labels": {
      "name": "hello-openshift"
    }
  },
  "spec": {
    "selector": {
      "name":"hello-openshift"
    },
    "ports": [
      {
        "protocol": "TCP",
        "port": 8888,
        "targetPort": 8080
      }
    ]
  }
}
EOF</code></pre>
</div></div>
<div class="paragraph"><p>Create the <code>hello-service</code> service:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc create -f hello-service.json

service "hello-service" created</code></pre>
</div></div>
<div class="paragraph"><p>Display the services that are running in the current project:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc get services

NAME            CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
hello-service   172.30.213.165   &lt;none&gt;        8888/TCP   5s</code></pre>
</div></div>
<div class="paragraph"><p>Examine the details of your service. Note the following:
<strong> <strong>Selector</strong>: Describes which pods the service selects or lists.
</strong> <strong>Endpoints</strong>: Displays all the pods that are currently listed (none in your current project).</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc describe service hello-service

Name:                   hello-service
Namespace:              svcslab
Labels:                 name=hello-openshift
Selector:               name=hello-openshift
Type:                   ClusterIP
IP:                     172.30.213.165
Port:                   &lt;unset&gt; 8888/TCP
Endpoints:              &lt;none&gt;
Session Affinity:       None
No events.</code></pre>
</div></div>
<div class="paragraph"><p>Create pods according to the <code>hello-many-pods.json</code> definition file:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc create -f hello-many-pods.json</code></pre>
</div></div>
<div class="paragraph"><p>Wait a few seconds and check the service again.</p></div>
<div class="ulist"><ul>
<li>
<p>
The pods that share the label <code>name=hello-openshift</code> are all listed:
</p>
</li>
</ul></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc describe service hello-service

Name:                   hello-service
Namespace:              svcslab
Labels:                 name=hello-openshift
Selector:               name=hello-openshift
Type:                   ClusterIP
IP:                     172.30.213.165
Port:                   &lt;unset&gt; 8888/TCP
Endpoints:              10.1.2.2:8080,10.1.2.3:8080,10.1.3.2:8080 + 1 more...
Session Affinity:       None
No events.</code></pre>
</div></div>
<div class="paragraph"><p>Test that your service is working:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ ip=`oc describe service hello-service|grep IP:|awk '{print $2}'`
[andrew@master ~]$ curl http://${ip}:8888

Hello OpenShift!</code></pre>
</div></div>
</div>
<div class="sect3">
<h4 id="_explore_containers_and_routes">1.11.6. Explore Containers and Routes</h4>
<div class="paragraph"><p>Next, take a look at the route and registry containers.</p></div>
<div class="sect4">
<h5 id="_create_applications_as_examples">Create Applications As Examples</h5>
<div class="paragraph"><p>As <code>andrew</code>, create a project called <code>explore-example</code>:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc new-project explore-example --display-name="Explore Example" --description="This is the project we use to learn about connecting to pods"</code></pre>
</div></div>
<div class="paragraph"><p>Applying the same image as before, run <code>oc new-app</code> to deploy <code>hello-openshift</code>:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc new-app --docker-image=openshift/hello-openshift:v1.5.1 -l "todelete=yes"

--&gt; Found Docker image fb15b0b (4 weeks old) from Docker Hub for "openshift/hello-openshift:v1.5.1"

    * An image stream will be created as "hello-openshift:v1.5.1" that will track this image
    * This image will be deployed in deployment config "hello-openshift"
    * Ports 8080/tcp, 8888/tcp will be load balanced by service "hello-openshift"
      * Other containers can access this service through the hostname "hello-openshift"
    * WARNING: Image "openshift/hello-openshift:v1.5.1" runs as the 'root' user which may not be permitted by your cluster administrator

--&gt; Creating resources with label todelete=yes ...
    imagestream "hello-openshift" created
    deploymentconfig "hello-openshift" created
    service "hello-openshift" created
--&gt; Success
    Run 'oc status' to view your app.</code></pre>
</div></div>
<div class="paragraph"><p>Verify that <code>oc new-app</code> has created a pod and the service.</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc get svc

NAME              CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
hello-openshift   172.30.24.220   &lt;none&gt;        8080/TCP,8888/TCP   37s</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc get pods

NAME                      READY     STATUS    RESTARTS   AGE
hello-openshift-1-g3xow   1/1       Running   0          2m</code></pre>
</div></div>
<div class="paragraph"><p>Expose the service and create a route for the application:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc expose service hello-openshift --hostname=explore.cloudapps.example.com</code></pre>
</div></div>
<div class="paragraph"><p>Check if the route works fine:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ curl http://explore.cloudapps.example.com

Hello OpenShift!</code></pre>
</div></div>
<div class="paragraph"><p>In a later section, you explore the <code>docker-registry</code> container. To save time, start an S2I build now to push an image into the registry:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc new-app https://github.com/openshift/sinatra-example -l "todelete=yes"

--&gt; Found image 27e89d9 (4 weeks old) in image stream "ruby" in project "openshift" under tag "2.3" for "ruby"

    Ruby 2.3
    --------
    Platform for building and running Ruby 2.3 applications

    Tags: builder, ruby, ruby23, rh-ruby23

    * The source repository appears to match: ruby
    * A source build using source code from https://github.com/openshift/sinatra-example will be created
      * The resulting image will be pushed to image stream "sinatra-example:latest"
    * This image will be deployed in deployment config "sinatra-example"
    * Port 8080/tcp will be load balanced by service "sinatra-example"
      * Other containers can access this service through the hostname "sinatra-example"

--&gt; Creating resources with label todelete=yes ...
    imagestream "sinatra-example" created
    buildconfig "sinatra-example" created
    deploymentconfig "sinatra-example" created
    service "sinatra-example" created
--&gt; Success
    Build scheduled, use 'oc logs -f bc/sinatra-example' to track its progress.
    Run 'oc status' to view your app.</code></pre>
</div></div>
</div>
<div class="sect4">
<h5 id="_connect_to_default_router_container">Connect to Default Router Container</h5>
<div class="paragraph"><p>Get back to root:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ exit</code></pre>
</div></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
As <code>root</code>, make sure to use the default project. Open a Shell into the container with <code>oc rsh</code>
 command along with the default router&#8217;s pod name.
</p>
</li>
</ol></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc project default

Now using project "default" on server "https://master.example.com:8443".</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get pods

NAME                      READY     STATUS    RESTARTS   AGE
docker-registry-1-26xs7    1/1       Running   9          28d
registry-console-1-tbwwj   1/1       Running   5          8d
router-1-xq3r6             1/1       Running   12         28d</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc rsh router-1-xq3r6</code></pre>
</div></div>
<div class="paragraph"><p>This prompt is displayed:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$</code></pre>
</div></div>
<div class="paragraph"><p>You are now running <code>bash</code> inside the container.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Do the following:
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Run <code>id</code>.
</p>
</li>
<li>
<p>
Run <code>pwd</code> and <code>ls</code> and note the directory you are in.
</p>
</li>
<li>
<p>
Run <code>grep hello-openshift</code> on the <code>haproxy.config</code> file.
</p>
</li>
<li>
<p>
Run <code>cat haproxy.config</code> to have a look on your configuration file.
</p>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$ id

uid=1000020000 gid=0(root) groups=0(root),1000020000</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$ pwd

/var/lib/haproxy/conf</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$ ls

cert_config.map          os_edge_http_be.map         os_sni_passthrough.map
default_pub_keys.pem     os_http_be.map              os_tcp_be.map
error-page-503.http      os_reencrypt.map            os_wildcard_domain.map
haproxy-config.template  os_route_http_expose.map
haproxy.config           os_route_http_redirect.map</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$ grep hello-openshift haproxy.config

backend be_http_explore-example_hello-openshift

sh-4.2$ ps -ef
UID         PID   PPID  C STIME TTY          TIME CMD
1000020+      1      0  0 07:21 ?        00:00:21 /usr/bin/openshift-router
1000020+    726      1  0 10:58 ?        00:00:44 /usr/sbin/haproxy -f /var/lib/
1000020+   1230      1  1 14:29 ?        00:00:06 /usr/sbin/haproxy -f /var/lib/
1000020+   1263      0  0 14:34 ?        00:00:00 /bin/sh
1000020+   1279   1263  0 14:37 ?        00:00:00 ps -ef</code></pre>
</div></div>
</li>
<li>
<p>
Examine the haproxy.config more closely. This could look something like this like this:
</p>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$ grep -A 40 hello-openshift haproxy.config | sed '/^ *$/d'

backend be_http_explore-example_hello-openshift
  mode http
  option redispatch
  option forwardfor
  balance leastconn
  timeout check 5000ms
  http-request set-header X-Forwarded-Host %[req.hdr(host)]
  http-request set-header X-Forwarded-Port %[dst_port]
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  cookie 7cf54b74789cba0ee0faded0db7f5e0f insert indirect nocache httponly
  http-request set-header Forwarded for=%[src];host=%[req.hdr(host)];proto=%[req.hdr(X-Forwarded-Proto)]
pass:quotes[  *server*] 456a8f857d60f0a14165ad58cff18e10 10.128.2.32:8080 check inter 5000ms cookie 456a8f857d60f0a14165ad58cff18e10 weight 100</code></pre>
</div></div>
<div class="paragraph"><p>You see that you have only one endpoint defined. (The line which starts with server)</p></div>
</li>
<li>
<p>
Exit the bash in the container to return to the <a href="mailto:roo@master">roo@master</a> shell
</p>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$ exit

[root@master ~]# _</code></pre>
</div></div>
</li>
</ol></div>
</li>
<li>
<p>
As <code>andrew</code>, scale <code>hello-openshift</code> to have five replicas of its pod:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# su - andrew</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc get deploymentconfig

NAME              REVISION   REPLICAS   TRIGGERED BY
hello-openshift   1          1          config,image(hello-openshift:v1.5.1)
sinatra-example   1          1          config,image(sinatra-example:latest)</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc scale dc hello-openshift --replicas=5

deploymentconfig "hello-openshift" scaled</code></pre>
</div></div>
</li>
<li>
<p>
As <code>root</code> go back to the router container and view the <code>haproxy.config</code> file again:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ exit</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc rhs router-1-xq3r6</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$ grep -A 70 hello-openshift haproxy.config | sed '/^ *$/d'

backend be_http_explore-example_hello-openshift
  mode http
  option redispatch
  option forwardfor
  balance leastconn
  timeout check 5000ms
  http-request set-header X-Forwarded-Host %[req.hdr(host)]
  http-request set-header X-Forwarded-Port %[dst_port]
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  cookie 7cf54b74789cba0ee0faded0db7f5e0f insert indirect nocache httponly
  http-request set-header Forwarded for=%[src];host=%[req.hdr(host)];proto=%[req.hdr(X-Forwarded-Proto)]
pass:quotes[  *server* 456a8f857d60f0a14165ad58cff18e10 10.128.2.32:8080 check inter 5000ms cookie 456a8f857d60f0a14165ad58cff18e10 weight 100
  *server* 465c8af937146549fb2d68aa3adfde77 10.128.2.36:8080 check inter 5000ms cookie 465c8af937146549fb2d68aa3adfde77 weight 100
  *server* a19dc1b5f57a5cfe76f752ad8aa6c3a5 10.130.0.20:8080 check inter 5000ms cookie a19dc1b5f57a5cfe76f752ad8aa6c3a5 weight 100
  *server* 111eec0d645bb0897b3a9425563167b9 10.131.0.18:8080 check inter 5000ms cookie 111eec0d645bb0897b3a9425563167b9 weight 100
  *server*] aa8e80663b91a03be37ee9d33c3bc9c5 10.131.0.19:8080 check inter 5000ms cookie aa8e80663b91a03be37ee9d33c3bc9c5 weight 100</code></pre>
</div></div>
<div class="ulist"><ul>
<li>
<p>
All of your pods within the <code>haproxy</code> configuration are listed.
</p>
</li>
</ul></div>
</li>
</ol></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">Remember, the router routes proxy connections to the pods directly and not through the service. The router uses the service only to obtain a list of the pod endpoints (IP addresses).</td>
</tr></table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_explore_registry_container">1.11.7. Explore Registry Container</h4>
<div class="paragraph"><p>There is two containers that deal with registry related services. There is the docker-registry and there is the registry-console. We are looking at the docker-registry in this section. We will take a quick look at the <a href="https://registry-console-default.cloudapps.example.com">Registry-Console</a> at a later time.</p></div>
<div class="paragraph"><p>Please ensure that your build from earlier is complete.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
As user <code>*andrew*</code>, check the logs of the build that we stared a while back:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc logs builds/sinatra-example-1

Cloning "https://github.com/openshift/sinatra-example" ...
        Commit: ff65a82271fffc60d4129bccde9c42ded49a199d (Merge pull request #11 from corey112358/patch-1)
        Author: Ben Parees &lt;bparees@users.noreply.github.com&gt;
        Date:   Wed Jul 22 00:20:36 2015 -0400

---&gt; Installing application source ...
---&gt; Building your Ruby application from source ...
---&gt; Running 'bundle install --deployment --without development:test' ...
Fetching gem metadata from https://rubygems.org/..........
Fetching version metadata from https://rubygems.org/..
Installing rack 1.6.0
Installing rack-protection 1.5.3
Installing tilt 1.4.1
Installing sinatra 1.4.5
Using bundler 1.10.6
Bundle complete! 1 Gemfile dependency, 5 gems now installed.
Gems in the groups development and test were not installed.
Bundled gems are installed into ./bundle.
---&gt; Cleaning up unused ruby gems ...


Pushing image 172.30.17.242:5000/explore-example/sinatra-example:latest ...
Pushed 0/5 layers, 3% complete
Pushed 1/5 layers, 24% complete
Pushed 2/5 layers, 43% complete
Pushed 3/5 layers, 75% complete
Pushed 3/5 layers, 98% complete
Pushed 4/5 layers, 98% complete
Pushed 5/5 layers, 100% complete
Push successful</code></pre>
</div></div>
<div class="paragraph"><p>Notice the last few lines here. The <strong>Push successful</strong> indicates that the new container image was put into your internal registry.</p></div>
</li>
<li>
<p>
As <code>root</code>, start a shell inside the Container Context by running <code>oc rsh</code> along with the <code>docker-registry</code> pod name:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc rsh docker-registry-1-qbv9l</code></pre>
</div></div>
</li>
<li>
<p>
Do the following:
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Run <code>id</code>.
</p>
</li>
<li>
<p>
Run <code>pwd</code> and <code>ls</code> and note the directory you are in.
</p>
</li>
<li>
<p>
Run <code>cat config.yml</code>  to verify your configuration file.
</p>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$ id

uid=1000010000 gid=0(root) groups=0(root),1000010000</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$ pwd

/</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$ ls

bin   config.yml  etc   lib    media  opt   registry  run   srv  tmp  var
boot  dev         home  lib64  mnt    proc  root      sbin  sys  usr</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$ cat config.yml

version: 0.1
log:
  level: debug
http:
  addr: :5000
storage:
  cache:
    blobdescriptor: inmemory
  filesystem:
    rootdirectory: /registry
  delete:
    enabled: true
auth:
  openshift:
    realm: openshift

    # tokenrealm is a base URL to use for the token-granting registry endpoint.
    # If unspecified, the scheme and host for the token redirect are determined from the incoming request.
    # If specified, a scheme and host must be chosen that all registry clients can resolve and access:
    #
    # tokenrealm: https://example.com:5000
middleware:
  registry:
    - name: openshift
  repository:
    - name: openshift
      options:
        acceptschema2: false
        pullthrough: true
        mirrorpullthrough: true
        enforcequota: false
        projectcachettl: 1m
        blobrepositorycachettl: 10m
  storage:
    - name: openshift</code></pre>
</div></div>
</li>
</ol></div>
</li>
<li>
<p>
View the repositories and images that are available:
</p>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$ cd /registry/docker/registry/v2/repositories</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$ ls

explore-example</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$ ls explore-example/sinatra-example/_layers/

sha256</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$ ls explore-example/sinatra-example/_layers/sha256/

02cbff0982e427fee158df11d35632f38410ee7e8b48212e681ecf3e60660ce4
5a865e48f2fdb4c48700b9aa800ecd8d0aff8611bec51fb4ab0f70ba09a0fb8e
89af3ab0c8b470502e9ed73ce6fa83f97e89a033f2553e9ba4e8a153c52a6373
9cc048a8a74a05eabd2f114d56d759435b8e2d76091e40edbff1d137b08de613
a778b52f148e84ec73f4ad7f7a1e67690dd0a36ddf1ed2926ad223901d196bf7
d65e4475a277c626c504de9433b98c30350e4cb940feb858b8563a6031e809a5</code></pre>
</div></div>
</li>
<li>
<p>
As user <code>andrew</code>, look at one of the pods you started earlier:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc get pods

NAME                      READY     STATUS      RESTARTS   AGE
hello-openshift-1-4ywxh   1/1       Running     0          7m
hello-openshift-1-5vsyl   1/1       Running     0          7m
hello-openshift-1-9ivns   1/1       Running     0          19m
hello-openshift-1-byte3   1/1       Running     0          7m
hello-openshift-1-riupx   1/1       Running     0          7m
sinatra-example-1-build   0/1       Completed   0          17m
sinatra-example-1-ebuiu   1/1       Running     0          14m</code></pre>
</div></div>
</li>
<li>
<p>
Connect to the container:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc exec -ti sinatra-example-1-ebuiu "/bin/bash"

bash-4.2$</code></pre>
</div></div>
</li>
<li>
<p>
Explore the container:
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Run <code>id</code>.
</p>
</li>
<li>
<p>
Run <code>pwd</code> and <code>ls</code> and note the directory you are in.
</p>
</li>
<li>
<p>
Run <code>ps -ef</code> to see what processes are running.
</p>
<div class="listingblock">
<div class="content">
<pre><code>bash-4.2$ id

uid=1000060000 gid=0(root) groups=0(root),1000060000

bash-4.2$ pwd

/opt/app-root/src

bash-4.2$ ls

Gemfile       README.md  config.ru        example-mustache       public
Gemfile.lock  app.rb     example-model    example-views          tmp
README        bundle     example-modular  example-views-modular

bash-4.2$ ps -ef

UID         PID   PPID  C STIME TTY          TIME CMD
1000050+      1      0  0 22:41 ?        00:00:01 ruby /opt/app-root/src/bundle/
1000050+     33      0  0 22:51 ?        00:00:00 /bin/bash
1000050+     62     33  0 22:51 ?        00:00:00 ps -ef</code></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">Your pod names and output differ slightly.</td>
</tr></table>
</div>
</li>
</ol></div>
</li>
</ol></div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_creating_applications_lab">2. Creating Applications Lab</h2>
<div class="sectionbody">
<div class="paragraph"><p>This lab includes the following sections:</p></div>
<div class="ulist"><ul>
<li>
<p>
<strong>Deploy Application on Web Console</strong>
</p>
<div class="paragraph"><p>In this section, you deploy an application from a code repository and follow the build logs on the OpenShift Container Platform web console and CLI.</p></div>
</li>
<li>
<p>
<strong>Customize Build Script</strong>
</p>
<div class="ulist"><ul>
<li>
<p>
Create an application from a forked Git repository, inject a custom build script, and start a rebuild from the web console.
</p>
</li>
<li>
<p>
Review your custom script messages in the logs.
</p>
</li>
</ul></div>
</li>
</ul></div>
<div class="sect2">
<h3 id="_deploy_application_on_web_console">2.1. Deploy Application on Web Console</h3>
<div class="paragraph"><p>Here, you connect to and become familiar with the web console, create a project and an application, and scale a deployment and the topology view.</p></div>
<div class="sect3">
<h4 id="_connect_to_and_explore_web_console">2.1.1. Connect To and Explore Web Console</h4>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Use your browser to go to the OpenShift web console at <code>https://master.example.com:8443[https://master.example.com:8443]</code>.
</p>
</li>
<li>
<p>
Log in as <code>andrew</code> with the password <code>r3dh4t1!</code>.
</p>
</li>
<li>
<p>
Take a few minutes to browse your projects.
</p>
</li>
</ol></div>
</div>
<div class="sect3">
<h4 id="_create_new_project">2.1.2. Create New Project</h4>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Click <strong>Projects</strong> and select <strong>View all projects</strong> to return to the Projects view.
</p>
</li>
<li>
<p>
Click the blue <strong>New Project</strong> button in the top right corner.
</p>
</li>
<li>
<p>
Give the new project a name, display name, and description:
</p>
<div class="ulist"><ul>
<li>
<p>
<strong>Name</strong>: <code>my-ruby-project</code>
</p>
</li>
<li>
<p>
<strong>Display Name</strong>: <code>My Ruby Example Project</code>
</p>
</li>
<li>
<p>
<strong>Description</strong>: An explanation of your choice
</p>
</li>
</ul></div>
</li>
</ol></div>
<div class="paragraph"><p>Once the project is in place, the <strong>Add to Project</strong> screen is displayed.</p></div>
</div>
<div class="sect3">
<h4 id="_create_new_application">2.1.3. Create New Application</h4>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
In the <strong>Add to Project</strong> screen, type <code>ruby</code> in the search field of the <strong>Browse Catalog</strong> Tab to filter the available instant apps, templates, and builder images.
</p>
</li>
<li>
<p>
We choose the plain Ruby Application here
</p>
</li>
<li>
<p>
Set the version to <code>2.2 - latest</code>
</p>
</li>
<li>
<p>
Click "Select"
</p>
</li>
<li>
<p>
Specify the name and Git repository URL:
</p>
<div class="ulist"><ul>
<li>
<p>
<strong>Name</strong>: <code>my-ruby-hello-world</code>.
</p>
</li>
<li>
<p>
<strong>Git Repository URL</strong>: <code>https://github.com/openshift/ruby-hello-world</code>.
</p>
</li>
</ul></div>
</li>
<li>
<p>
Click <strong>Show advanced build and deployment options</strong> and select the following options:
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Notice that you get a route per default for your application.
</p>
</li>
<li>
<p>
Note that you can decide if Builds or Deployments should start automatically.
</p>
</li>
<li>
<p>
Change the scaling parameter to 3.
</p>
</li>
<li>
<p>
Create a label for app by the name of <code>environment</code> and the value of <code>dev</code>.
</p>
</li>
</ol></div>
</li>
<li>
<p>
Accept and create the application.
</p>
</li>
<li>
<p>
Click <strong>Continue to Overview</strong> to go to the application&#8217;s <strong>Overview</strong> screen.
</p>
</li>
<li>
<p>
Click <strong>View Log</strong> to verify that a build is in progress.
</p>
</li>
<li>
<p>
Review the log as the build progresses.
</p>
</li>
<li>
<p>
Wait for the build to complete and use a browser to navigate to the
 application route: <a href="http://my-ruby-hello-world-my-ruby-project.cloudapps.example.com">http://my-ruby-hello-world-my-ruby-project.cloudapps.example.com</a>

</p>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/tip.png" alt="Tip" />
</td>
<td class="content">
<div class="ulist"><ul>
<li>
<p>
You can also use the command line to create a new application: <code>oc new-app https://github.com/openshift/ruby-hello-world -l  environment=dev</code>.
</p>
</li>
<li>
<p>
To change scaling from the command line, use <code>oc scale</code>.
</p>
</li>
</ul></div>
</td>
</tr></table>
</div>
</li>
</ol></div>
</div>
<div class="sect3">
<h4 id="_scale_deployment">2.1.4. Scale Deployment</h4>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Go back to your application&#8217;s <strong>Overview</strong> screen by clicking <strong>Overview</strong> at the upper left side.
</p>
</li>
<li>
<p>
Observe the circle that shows the current number of pods, which is 3. You can increase that number by clicking the <code>^</code> button next to it.
</p>
</li>
<li>
<p>
Click the <code>^</code> button twice to increase the number of replicas to 5.
</p>
</li>
<li>
<p>
Go to <strong>Applications</strong> and select <strong>Pods</strong> to take a look at your new pods.
</p>
</li>
<li>
<p>
Go back to your application&#8217;s <strong>Overview</strong> screen by clicking <strong>Overview</strong> again.
</p>
</li>
</ol></div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_optional_templates_lab">3. [ Optional ] Templates Lab</h2>
<div class="sectionbody">
<div class="paragraph"><p>This lab includes the following sections:</p></div>
<div class="ulist"><ul>
<li>
<p>
<strong>Create and Upload Template</strong>
</p>
<div class="paragraph"><p>In this section, you create a template for a two-tier application (front end and database), upload it into the shared namespace (the <code>openshift</code> project), and ensure that users can deploy it from the web console.</p></div>
</li>
<li>
<p>
<strong>Use Templates and Template Parameters</strong>
</p>
<div class="paragraph"><p>In this section, you create two separate template instances in two separate projects and establish a front-end-to-database-back-end connection by means of template parameters.</p></div>
</li>
</ul></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">
<div class="title">Templates are a complex</div>
<div class="paragraph"><p>Templates allow an easy way to define all the required objects of an complex to be sepcified together and made available in Catalogs. Please see our <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-templates">OpenShift Documentation on Templates</a> for more information.</p></div>
</td>
</tr></table>
</div>
<div class="sect2">
<h3 id="_create_and_upload_template">3.1. Create and Upload Template</h3>
<div class="sect3">
<h4 id="_install_template">3.1.1. Install Template</h4>
<div class="paragraph"><p>The example in this section shows an application and a service with two pods: a front-end web tier and a back-end database tier. This application uses auto-generated parameters and other sleek features of OpenShift Container Platform.  Note that this application contains predefined connectivity between the front-end and back-end components as part of its YAML definition. You add further resources in a later lab.</p></div>
<div class="paragraph"><p>This example is, in effect, a "quick start"&#8201;&#8212;&#8201;a predefined application that comes in a template and that you can immediately use or customize.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
As <code>root</code> on the master host, download the template&#8217;s definition file:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# wget http://people.redhat.com/~llange/yaml/Template_Example.yml</code></pre>
</div></div>
</li>
<li>
<p>
Create the template object in the shared <code>openshift</code> project. This is also referred to as <em>uploading</em> the template.
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc create -f Template_Example.json -n openshift

template "a-quickstart-keyvalue-application" created</code></pre>
</div></div>
</li>
</ol></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">The <code>Template_Example.yml</code> file defines a template. You just added it to the openshift project. This make your template available throughout your OpenShift cluster. If you want to just have this temlate available for certain projects, put it directly into the project namespace and refrain from adding it to the <code>openshift</code> project.</td>
</tr></table>
</div>
<div class="paragraph"><p>The OpenShift Container Platform comes with a long list of preconfigured templates available for usage. You can take a look at the installed list with the following <code>oc</code> command. This list had 117 entries, that is why we did not include the output here.</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get templates -n openshift
...</code></pre>
</div></div>
<div class="paragraph"><p>Do not be alarmed by the complexity of Templates. You can even create templates from existing Objects. Please see our Documentation on
<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#export-as-template">How to Create a Template from existing Objects</a>.</p></div>
</div>
<div class="sect3">
<h4 id="_create_instant_app_from_template">3.1.2. Create Instant App from Template</h4>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
On your browser, connect to the OpenShift web console at <code>https://master.example.com:8443[https://master.example.com:8443]</code>:
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
If prompted, accept the untrusted certificate.
</p>
</li>
<li>
<p>
Log in as <code>andrew</code> with the password <code>r3dh4t1!</code>.
</p>
</li>
</ol></div>
</li>
<li>
<p>
Click the blue <strong>New Project</strong> button in the top right corner.
</p>
</li>
<li>
<p>
Specify the project name, display name, and description:
</p>
<div class="ulist"><ul>
<li>
<p>
<strong>Name</strong>: <code>instant-app</code>
</p>
</li>
<li>
<p>
<strong>Display Name</strong>: <code>instant app example project</code>
</p>
</li>
<li>
<p>
<strong>Description</strong>: <code>A demonstration of an instant app or template</code>.
</p>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/tip.png" alt="Tip" />
</td>
<td class="content">
<div class="paragraph"><p>Alternatively, perform this step from the command line:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oadm new-project instant-app --display-name="instant app example project" --description='A demonstration of an instant-app/template' --node-selector='region=primary' --admin=andrew</code></pre>
</div></div>
</td>
</tr></table>
</div>
</li>
</ul></div>
</li>
<li>
<p>
From the <code>instant-app</code> project&#8217;s <strong>Overview</strong> screen, click <strong>Add to project</strong>.
</p>
</li>
<li>
<p>
Click the <code>ruby</code> tile to display ruby based applications and builder images
</p>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">Here you find the instant application, a special kind of template with the <code>instant-app</code> tag. The idea behind an instant application is that, when you create a template instance, you already have a fully functional application. In this example, your instant application is just a simple web page for key-value storage and retrieval.</td>
</tr></table>
</div>
</li>
<li>
<p>
Select <strong>a-quickstart-keyvalue-application</strong>.
</p>
<div class="paragraph"><p>The template configuration screen is displayed. Here, you can specify certain options for instantiating the application components:</p></div>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Set the <code>ADMIN_PASSWORD</code> parameter to your favorite password.
</p>
</li>
<li>
<p>
Add a label named <code>version</code> with the value <code>1</code>.
</p>
</li>
</ol></div>
</li>
<li>
<p>
Click <strong>Create</strong> to instantiate the services, pods, replication controllers, etc.
</p>
<div class="ulist"><ul>
<li>
<p>
The build starts immediately.
</p>
</li>
</ul></div>
</li>
<li>
<p>
Wait for the build to finish. You can browse the build logs to follow the progress.
</p>
</li>
</ol></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">Our Application is currently still missing heath checks for all containers. You will deal with health checks later in this lab. If you are an experienced OpenShift User feel free to build a template with health checks included.</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="_use_application">3.1.3. Use Application</h4>
<div class="paragraph"><p>After the build is complete, visit your application at <code>http://example-route-instant-app.cloudapps.example.com/[http://example-route-instant-app.cloudapps.example.com/]</code>.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">Be sure to use HTTP and <em>not</em> HTTPS. HTTPS does not work for this example because the form submission was coded with HTTP links.</td>
</tr></table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_use_templates_and_template_parameters">3.2. Use Templates and Template Parameters</h3>
<div class="paragraph"><p>Quick starts are nice and handy. But you will not be writing them from scratch. Developers are usualld stating with the components themselvs and will put the whole app together step by step first. This is what we will walk you through next. You treat the quick-start example as two separate applications to be wired together.</p></div>
<div class="sect3">
<h4 id="_deploy_ephemeral_database_back_end">3.2.1. Deploy Ephemeral Database Back End</h4>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Create a project new project for this work to live in:
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Use your browser to connect to the OpenShift web console at <code>https://master.example.com:8443</code>.
</p>
</li>
<li>
<p>
If prompted, accept the untrusted certificate.
</p>
</li>
<li>
<p>
Log in as <code>marina</code> with the password <code>r3dh4t1!</code>.
</p>
</li>
<li>
<p>
Click the blue <strong>New Project</strong> button in the top right corner.
</p>
</li>
<li>
<p>
Specify the project name, display name, and description:
</p>
<div class="ulist"><ul>
<li>
<p>
<strong>Name</strong>: <code>templates</code>
</p>
</li>
<li>
<p>
<strong>Display Name</strong>: <code>Templates Testing Project</code>
</p>
</li>
<li>
<p>
<strong>Description</strong>: <code>Project for testing templates</code>
</p>
</li>
</ul></div>
</li>
</ol></div>
</li>
</ol></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/tip.png" alt="Tip" />
</td>
<td class="content">Alternatively, perform this step from the command line:</td>
</tr></table>
</div>
<div class="paragraph"><p>+</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oadm new-project templates --display-name="Templates Testing Project" --description='Project used to test templates' --admin=marina</code></pre>
</div></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Deploy an ephemeral MySQL database:
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
From the <code>templates</code> project&#8217;s <strong>Overview</strong> screen, click <strong>Add to project</strong>.
</p>
</li>
<li>
<p>
Search for <code>mysql-ephemeral</code> in the <code>Browse Catalog</code>
</p>
</li>
<li>
<p>
Select the <code>mysql-ephemeral</code> database template.
</p>
</li>
<li>
<p>
Set the template parameters:
</p>
<div class="ulist"><ul>
<li>
<p>
<strong>Database Service Name</strong>: <code>database</code>
</p>
</li>
<li>
<p>
<strong>MySQL Connection Username</strong>: <code>mysqluser</code>
</p>
</li>
<li>
<p>
<strong>MySQL Connection Password</strong>: <code>redhat</code>
</p>
</li>
<li>
<p>
<strong>MySQL Database Name</strong>: <code>mydb</code>
</p>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/caution.png" alt="Caution" />
</td>
<td class="content">Make sure you set these values correctly, otherwise the application
 would not connect to the database backend.</td>
</tr></table>
</div>
</li>
</ul></div>
</li>
<li>
<p>
Click <strong>Create</strong> and then click <strong>Continue to overview</strong>.
</p>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/tip.png" alt="Tip" />
</td>
<td class="content">Alternatively, create the template instance from the command line:</td>
</tr></table>
</div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# su - marina</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc project templates</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc new-app --template=mysql-ephemeral --param MYSQL_USER=mysqluser --param MYSQL_PASSWORD=redhat --param MYSQL_DATABASE=mydb --param DATABASE_SERVICE_NAME=database</code></pre>
</div></div>
</li>
<li>
<p>
As <code>marina</code>, switch to the "templates" project (if you are not in that project already) and examine the objects that
 were created as part of the <code>mysql-ephemeral</code> template.
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc get projects

NAME                DISPLAY NAME                STATUS
custom-s2i-script   Custom S2I Build Script     Active
templates           Templates Testing Project   Active</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc project templates

Now using project "templates" on server "https://master.example.com:8443".</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc get dc

NAME       REVISION   DESIRED   CURRENT   TRIGGERED BY
database   1          1         1         config,image(mysql:5.7)</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$oc get service -o wide

NAME       CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE       SELECTOR
database   172.30.142.93   &lt;none&gt;        3306/TCP   3m        name=database</code></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">A deployment configuration is available for your instance. The service name is the same as that of your <code>DATABASE_SERVICE_NAME</code> parameter.</td>
</tr></table>
</div>
</li>
<li>
<p>
Verify that the values of the environment variables in the deployment configuration (<code>dc</code>) are correct:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc env dc database --list

# deploymentconfigs database, container mysql
# MYSQL_USER from secret database, key database-user
# MYSQL_PASSWORD from secret database, key database-password
# MYSQL_ROOT_PASSWORD from secret database, key database-root-password
MYSQL_DATABASE=mydb</code></pre>
</div></div>
</li>
</ol></div>
</li>
</ol></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">
<div class="title">Security relevant Environment Settings</div>Notic that the security releavant settings in environment vars are not displayed by commented out in the above output.</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="_deploy_application_8217_s_ruby_front_end">3.2.2. Deploy Application&#8217;s Ruby Front End</h4>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
As <code>marina</code>, create an application with the <code>https://github.com/openshift/ruby-hello-world</code> Git repository:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc new-app openshift/ruby~https://github.com/openshift/ruby-hello-world MYSQL_USER=mysqluser MYSQL_PASSWORD=redhat MYSQL_DATABASE=mydb</code></pre>
</div></div>
</li>
<li>
<p>
Verify that your service is in place:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc get service -o wide

NAME               CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE       SELECTOR
database           172.30.142.93   &lt;none&gt;        3306/TCP   20m       name=database
ruby-hello-world   172.30.37.49    &lt;none&gt;        8080/TCP   2m        app=ruby-hello-world,deploymentconfig=ruby-hello-world</code></pre>
</div></div>
</li>
<li>
<p>
Create an external route to your front-end application.
</p>
<div class="ulist"><ul>
<li>
<p>
If you do not specify a host name, the default subdomain route creates the route.
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc expose service ruby-hello-world

route "ruby-hello-world" exposed</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc get route

NAME               HOST/PORT                                                     PATH      SERVICE            LABELS
ruby-hello-world   ruby-hello-world-templates.cloudapps.example.com             ruby-hello-world   app=ruby-hello-world</code></pre>
</div></div>
</li>
</ul></div>
</li>
<li>
<p>
Wait for the build to complete. Then test your environment:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc logs -f builds/ruby-hello-world-1</code></pre>
</div></div>
</li>
<li>
<p>
Wait for the pods to start and verify that your application is running and connecting to the database:
</p>
<div class="listingblock">
<div class="content">
<pre><code>http://ruby-hello-world-templates.cloudapps.example.com</code></pre>
</div></div>
</li>
</ol></div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_deployment_life_cycle_lab">4. Deployment Life Cycle Lab</h2>
<div class="sectionbody">
<div class="paragraph"><p>This lab includes the following sections:</p></div>
<div class="ulist"><ul>
<li>
<p>
<strong>Roll Back, Activate, and Code Life Cycle</strong>
</p>
<div class="paragraph"><p>In this section, you manage the various phases of the deployment&#8217;s life cycle.</p></div>
</li>
<li>
<p>
<strong>Create and Use Web Hooks</strong>
</p>
<div class="paragraph"><p>In this section, you create a Git webhook and start a new build and a new
 deployment automatically by pushing a code change in your Git repository.</p></div>
</li>
</ul></div>
<div class="sect2">
<h3 id="_roll_back_activate_and_code_life_cycle">4.1. Roll Back, Activate, and Code Life Cycle</h3>
<div class="paragraph"><p>Going back and forth between different versions of your application and its configuration is helpful not only for developers but for operators as well. You can switch back a deployment version that you know is a working state with OpenShift.</p></div>
<div class="paragraph"><p>In this section, you take user <code>marina's hello-ruby</code> application, modify its
 front end, and then rebuild. Afterwards, you revert to the original version and
  then go on to your rebuilt version.</p></div>
<div class="paragraph"><p>The next sections require a GitHub account.</p></div>
<div class="sect3">
<h4 id="_fork_repository">4.1.1. Fork Repository</h4>
<div class="paragraph"><p>If you have not done so already: from the Git web interface, click <strong>Fork</strong> in the
 upper right corner of the GitHub web UI to fork the Git repository
  <code>https://github.com/openshift/ruby-hello-world</code> into your own account.</p></div>
</div>
<div class="sect3">
<h4 id="_create_your_application">4.1.2. Create Your Application</h4>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">Remember that <code>buildconfig</code> (the build-configuration file) instructs OpenShift Container Platform on how to perform a build.</td>
</tr></table>
</div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
As <code>root</code>, create a project for user <code>marina</code>:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oadm new-project lifecycle --display-name="Lifecycle Lab" \
    --description="This is the project we use to learn about Lifecycle management" \
    --admin=marina --node-selector='region=primary'</code></pre>
</div></div>
</li>
<li>
<p>
Switch to user <code>marina</code> and use the <code>lifecycle</code> project:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# su - marina</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc project lifecycle</code></pre>
</div></div>
</li>
<li>
<p>
Create an application from the <code>https://github.com/openshift/ruby-hello-world</code> repository:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc new-app https://github.com/openshift/ruby-hello-world</code></pre>
</div></div>
</li>
<li>
<p>
Run <code>oc env</code> to add the environment variables for a database to be used later:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc env dc/ruby-hello-world MYSQL_USER=mysqluser MYSQL_PASSWORD=redhat MYSQL_DATABASE=mydb</code></pre>
</div></div>
</li>
<li>
<p>
Waiting for the build to finish. Meanwhile, expose your service to the world so that you can test it from your local browser:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc expose service ruby-hello-world</code></pre>
</div></div>
</li>
<li>
<p>
View the current <code>buildconfig</code> for your application:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc get buildconfig ruby-hello-world -o yaml</code></pre>
</div></div>
</li>
<li>
<p>
This should look like the following text:
</p>
<div class="listingblock">
<div class="content">
<pre><code>apiVersion: v1
kind: BuildConfig
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftNewApp
  creationTimestamp: 2016-11-15T11:28:51Z
  labels:
    app: ruby-hello-world
  name: ruby-hello-world
  namespace: lifecycle
  resourceVersion: "207409"
  selfLink: /oapi/v1/namespaces/lifecycle/buildconfigs/ruby-hello-world
  uid: af4f7bf4-ab26-11e6-8733-2cc2602a6dc8
spec:
  output:
    to:
      kind: ImageStreamTag
      name: ruby-hello-world:latest
  postCommit: {}
  resources: {}
  source:
    git:
pass:quotes[      *uri: https://github.com/openshift/ruby-hello-world*]
    secrets: []
    type: Git
  strategy:
    sourceStrategy:
      from:
        kind: ImageStreamTag
        name: ruby:2.3
        namespace: openshift
    type: Source
  triggers:
  - github:
      secret: yx3JIc_qegmYlwF4dVnT
    type: GitHub
  - generic:
      secret: GH4lDKWvCeLfBh0-O2u6
    type: Generic
  - type: ConfigChange
  - imageChange:
      lastTriggeredImageID: registry.access.redhat.com/rhscl/ruby-23-rhel7@sha256:3601dd48c3ee5def47fd641188bcf676f7447346296d4607c40862261b522d93
    type: ImageChange
status:
  lastVersion: 1</code></pre>
</div></div>
</li>
<li>
<p>
Observe that the current configuration points to the <code>openshift/ruby-hello-world</code> github repository.
</p>
<div class="ulist"><ul>
<li>
<p>
Because you forked this repository earlier, you can now point to your git location.
</p>
</li>
</ul></div>
</li>
</ol></div>
</div>
<div class="sect3">
<h4 id="_point_to_your_git_location">4.1.3. Point to your git location</h4>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Run <code>oc edit</code> to change the configuration.
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc edit bc ruby-hello-world</code></pre>
</div></div>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Change the <code>uri</code> reference to match the name of your GitHub repository, which is based in part on your GitHub username: <code>https://github.com/GitHubUsername/ruby-hello-world</code>.
</p>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/important.png" alt="Important" />
</td>
<td class="content">Replace <code>GitHubUsername</code> with your actual GitHub username. For example, if your GitHub username is <code>jeandeaux</code>, the name of your GitHub repository is <code>'https://github.com/jeandeaux/ruby-hello-world</code>.</td>
</tr></table>
</div>
</li>
<li>
<p>
Save and exit <code>vi</code> by typing <strong>:wq</strong>.
</p>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">There are other ways to achieve this outcome, this way is used to cover
 the <code>oc edit</code> and the <code>oc start-build</code> commands.</td>
</tr></table>
</div>
</li>
</ol></div>
</li>
<li>
<p>
Run <code>oc get buildconfig ruby-hello-world -o yaml | grep uri</code>. Notice that <code>uri</code> has been updated.
</p>
</li>
<li>
<p>
Run <code>oc get builds</code> to check if the new build has started:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc get builds</code></pre>
</div></div>
<div class="paragraph"><p>No build was started, the change we made does not count as a config change. We essentially changed the application source. You will need to start a build manually :</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc get bc

NAME               TYPE      SOURCE
ruby-hello-world   Docker    https://github.com/YOURUSERNAME/ruby-hello-world</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc start-build ruby-hello-world

ruby-hello-world-2</code></pre>
</div></div>
<div class="paragraph"><p>This has started a new build :</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc get builds -w

NAME                 TYPE      FROM      STATUS     STARTED              DURATION
ruby-hello-world-1   Source    Git       Complete   16 minutes ago       4m25s
ruby-hello-world-2   Source    Git       Complete   About a minute ago   1m46s</code></pre>
</div></div>
<div class="paragraph"><p>Follow the build logs with :</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc logs -f bc/ruby-hello-world

I0709 23:41:08.493756       1 docker.go:69] Starting Docker build from justanother1/ruby-hello-world-7 BuildConfig ...
I0709 23:41:08.508448       1 tar.go:133] Adding to tar: /tmp/docker-build062004796/.gitignore as .gitignore
I0709 23:41:08.509588       1 tar.go:133] Adding to tar: /tmp/docker-build062004796/.sti/bin/README as .sti/bin/README
I0709 23:41:08.509953       1 tar.go:133] Adding to tar: /tmp/docker-build062004796/.sti/environment as .sti/environment
I0709 23:41:08.510183       1 tar.go:133] Adding to tar: /tmp/docker-build062004796/Dockerfile as Dockerfile
I0709 23:41:08.510548       1 tar.go:133] Adding to tar: /tmp/docker-build062004796/Gemfile as Gemfile
.......
Cropped Output
.......</code></pre>
</div></div>
</li>
<li>
<p>
Search for the available <code>mysql</code> applications (templates):
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc new-app --search mysql
...</code></pre>
</div></div>
<div class="paragraph"><p>The above command outputs quite a lot of info in current clusters. Lets run this again but reduce detail with grep. I just want the templates or images that start with mysql. As the next two lines are descriptions for these templates or images, lets display them as well.</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc new-app --search mysql | grep ^mysql -A 2

mysql-persistent
  Project: openshift
  MySQL database service, with persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mysql-container/blob/master/5.7/README.md.
--
mysql-ephemeral
  Project: openshift
  MySQL database service, without persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mysql-container/blob/master/5.7/README.md.
--
mysql
  Project: openshift
  Tags:    5.6, 5.7, latest
--
mysql
  Registry: Docker Hub
  Tags:     latest</code></pre>
</div></div>
</li>
<li>
<p>
Create the <code>database</code> application by running <code>oc new-app</code>:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc new-app --template=mysql-ephemeral --param MYSQL_USER=mysqluser --param MYSQL_PASSWORD=redhat --param MYSQL_DATABASE=mydb --param DATABASE_SERVICE_NAME=database</code></pre>
</div></div>
</li>
<li>
<p>
Verify that your values were processed correctly:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc env dc/database --list</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code># deploymentconfigs database, container mysql
# MYSQL_USER from secret database, key database-user
# MYSQL_PASSWORD from secret database, key database-password
# MYSQL_ROOT_PASSWORD from secret database, key database-root-password
MYSQL_DATABASE=mydb</code></pre>
</div></div>
<div class="paragraph"><p>Notice that you can net see the values of MYSQL_USER, MYSQL_PASSWORD and MYSQL_ROOT_PASSWORD as they are marked as secret keys in the template definition. The values are only shown in the output ofthe <code>oc new-app</code> command above.</p></div>
</li>
<li>
<p>
You must redeploy your front end so that it checks for the database again. You
 can either delete just the pod, or you can redeploy the application with :
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc deploy ruby-hello-world --latest</code></pre>
</div></div>
</li>
<li>
<p>
You can see the logs for your latest deployment if you use the <code>oc logs</code> command this way:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$  oc logs -f dc/ruby-hello-world

I1222 01:54:45.485814       1 deployer.go:198] Deploying from lifecycle/ruby-hello-world-3 to lifecycle/ruby-hello-world-4 (replicas: 1)
I1222 01:54:46.913895       1 rolling.go:232] RollingUpdater: Continuing update with existing controller ruby-hello-world-4.
I1222 01:54:47.019320       1 rolling.go:232] RollingUpdater: Scaling up ruby-hello-world-4 from 0 to 1, scaling down ruby-hello-world-3 from 1 to 0 (keep 0 pods available, don't exceed 2 pods)
I1222 01:54:47.020399       1 rolling.go:232] RollingUpdater: Scaling ruby-hello-world-4 up to 1
I1222 01:54:51.372703       1 rolling.go:232] RollingUpdater: Scaling ruby-hello-world-3 down to 0</code></pre>
</div></div>
</li>
<li>
<p>
Check that your Application is working now and that you can put and get keys.
</p>
</li>
</ol></div>
</div>
</div>
<div class="sect2">
<h3 id="_optional_create_and_use_webhooks">4.2. [ Optional ] Create and Use Webhooks</h3>
<div class="paragraph"><p>You can integrate external systems into your OpenShift Container Platform
environment so that they can start OpenShift Container Platform builds. This allows for use cases where making a change in the source code triggers a build process in OpenShift. This process is triggerd by web hooks. This is a special URL that e.g. your code repository can make a call to, when new code is available.</p></div>
<div class="sect3">
<h4 id="_find_the_webhook_url">4.2.1. Find the Webhook URL</h4>
<div class="paragraph"><p>Your GitHub account can configure a webhook whenever you push a commit to a specific branch.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Find the webhook URL:
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Go to the openshift web console, log in as Marina.
</p>
</li>
<li>
<p>
Navigate to your project.
</p>
</li>
<li>
<p>
Click <strong>Builds</strong> and then click the sub menu entry <strong>Builds</strong>.
</p>
</li>
<li>
<p>
Select your application build config <code>ruby-hello-world</code> from the list
</p>
</li>
<li>
<p>
Select the "Configuration" Tab.
</p>
<div class="ulist"><ul>
<li>
<p>
Two webhook URLs are displayed.
</p>
</li>
</ul></div>
</li>
</ol></div>
</li>
<li>
<p>
Copy the github URL, which looks like this:
</p>
<div class="listingblock">
<div class="content">
<pre><code>https://&lt;Put External master name here&gt;:8443/oapi/v1/namespaces/lifecycle/buildconfigs/ruby-hello-world/webhooks/_hoMePVjqAPrLKk526hP/github</code></pre>
</div></div>
<div class="paragraph"><p>You can get the External Master name from the <a href="https://lab.rhpet.de">Red Hat Partner Enablement Lab Portal</a>.</p></div>
<div class="listingblock">
<div class="title">You can also see the webhook on the command line</div>
<div class="content">
<pre><code>[marina@master ~]$ oc describe bc ruby-hello-world
Name:           ruby-hello-world
Namespace:      lifecycle
Created:        13 minutes ago
Labels:         app=ruby-hello-world
Annotations:    openshift.io/generated-by=OpenShiftNewApp
Latest Version: 1

Strategy:       Docker
URL:            https://github.com/LutzLange/ruby-hello-world
From Image:     ImageStreamTag openshift/ruby:2.3
Output to:      ImageStreamTag ruby-hello-world:latest

Build Run Policy:       Serial
Triggered by:           Config, ImageChange
Webhook Generic:
        URL:            https://master.example.com:8443/oapi/v1/namespaces/lifecycle/buildconfigs/ruby-hello-world/webhooks/a9ui0d5DbzH6_IGOttGO/generic
        AllowEnv:       false
pass:quotes[*Webhook GitHub:
        URL:    https://master.example.com:8443/oapi/v1/namespaces/lifecycle/buildconfigs/ruby-hello-world/webhooks/_hoMePVjqAPrLKk526hP/github*]

Build                   Status          Duration        Creation Time
ruby-hello-world-1      complete        3m34s           2017-06-16 02:56:23 -0400 EDT

No events.</code></pre>
</div></div>
</li>
<li>
<p>
In the GitHub repository, which you forked earlier, go to <strong>Settings &#8594; Webhooks</strong>.
</p>
</li>
<li>
<p>
Click "Add webhook"
</p>
</li>
<li>
<p>
Paste the URL (with the external Master Host Name in it) into the <strong>Payload URL</strong> field.
</p>
</li>
<li>
<p>
Set "Content type" to <code>application/json</code>
</p>
</li>
<li>
<p>
Disable SSL verification.
</p>
</li>
<li>
<p>
Click <strong>Add Webhook</strong>.
</p>
</li>
</ol></div>
</div>
<div class="sect3">
<h4 id="_test_your_webhook">4.2.2. Test Your Webhook</h4>
<div class="paragraph"><p>To test your webhook by changing and commiting / pushing some code in the Git repository. Do the following:</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">Alternatively, you can test the webhook the usual way by cloning your repository locally, making the required changes, and pushing them to the repository.</td>
</tr></table>
</div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Go to your forked repository (<code>https://github.com/GitHubUsername/ruby-hello-world</code>) and find the <code>main.erb</code> file in the <code>views</code> folder.
</p>
<div class="ulist"><ul>
<li>
<p>
You can edit files in the GitHub web UI.
</p>
</li>
</ul></div>
</li>
<li>
<p>
Change this HTML code
</p>
<div class="listingblock">
<div class="content">
<pre><code>    &lt;div class="page-header" align=center&gt;
      &lt;h1&gt; Welcome to an OpenShift v3 Demo App! &lt;/h1&gt;
    &lt;/div&gt;</code></pre>
</div></div>
<div class="paragraph"><p>to read as follows (including the deliberately misspelled <code>crustom</code>):</p></div>
<div class="listingblock">
<div class="content">
<pre><code>    &lt;div class="page-header" align=center&gt;
      &lt;h1&gt; This is my crustom demo! &lt;/h1&gt;
    &lt;/div&gt;</code></pre>
</div></div>
</li>
<li>
<p>
Commit the change to the repository.
</p>
</li>
<li>
<p>
Check if a build has started.
</p>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/caution.png" alt="Caution" />
</td>
<td class="content">If another build is already running, this latest build may fail because both builds are pushing to the registry. Either run <code>oc delete build</code> to stop the earlier build or <code>oc start-build</code> to restart the failed build.</td>
</tr></table>
</div>
</li>
<li>
<p>
Log in as <code>marina</code> and check the web UI to verify that the build is running.
</p>
</li>
<li>
<p>
Wait for the build to complete. It can take a minute for your service endpoint to update.
</p>
</li>
<li>
<p>
Use your browser to go to the application at <code>http://ruby-hello-world.lifecycle.cloudapps.example.com/</code>.
</p>
<div class="ulist"><ul>
<li>
<p>
The output includes the deliberately misspelled <code>crustom</code>.
</p>
</li>
<li>
<p>
If you try to access the application before the update is complete, you may see a <code>503</code> error.
</p>
</li>
</ul></div>
</li>
</ol></div>
</div>
<div class="sect3">
<h4 id="_roll_back_your_application">4.2.3. Roll Back Your Application</h4>
<div class="paragraph"><p>Because you failed to properly test your application and your typo made it into production, you must revert to the previous version of your application.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Log in to the web console as <code>marina</code>.
</p>
</li>
<li>
<p>
Go to your <code>Lifecycle Lab</code>
</p>
</li>
<li>
<p>
Open the <strong>Application</strong> Menu
</p>
</li>
<li>
<p>
Select <strong>Deployments</strong>
</p>
</li>
<li>
<p>
Select your frontend <code>ruby-hello-world</code> from the list.
</p>
<div class="ulist"><ul>
<li>
<p>
You should see at least 2 deployments versions for your frontend. The lastest version is marked active.
</p>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/tip.png" alt="Tip" />
</td>
<td class="content">
<div class="paragraph"><p>Alternatively, view this information from the CLI:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc describe dc/ruby-hello-world
...</code></pre>
</div></div>
</td>
</tr></table>
</div>
</li>
</ul></div>
</li>
<li>
<p>
From the CLI, roll back the deployment:
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Determine which rollouts are available:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~] oc rollout history dc/ruby-hello-world

deploymentconfigs "ruby-hello-world"
REVISION        STATUS          CAUSE
1               Complete        image change
2               Complete        config change</code></pre>
</div></div>
</li>
<li>
<p>
Choose a deployment and see what a rollback to <code>ruby-hello-world-X</code> would
 look like:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc rollback ruby-hello-world --to-version=X --dry-run # X is your desired deployment
Name:           ruby-hello-world
Namespace:      lifecycle
Created:        About an hour ago
Labels:         app=ruby-hello-world
Annotations:    openshift.io/generated-by=OpenShiftNewApp
Latest Version: 4
Selector:       app=ruby-hello-world,deploymentconfig=ruby-hello-world
Replicas:       1
Triggers:       Config, Image(ruby-hello-world@latest, auto=false)
Strategy:       Rolling
Template:
  Labels:       app=ruby-hello-world
                deploymentconfig=ruby-hello-world
  Annotations:  openshift.io/generated-by=OpenShiftNewApp
  Containers:
   ruby-hello-world:
    Image:                      172.30.120.134:5000/lifecycle/ruby-hello-world@sha256:20c8bf8238467e3343e3302ac36fc5f7fe3bbb9b5f48ff65a37dcc790339e48e
    Port:                       8080/TCP
    Volume Mounts:              &lt;none&gt;
    Environment Variables:      &lt;none&gt;
  No volumes.

Latest Deployment:      &lt;none&gt;</code></pre>
</div></div>
<div class="ulist"><ul>
<li>
<p>
From the above output, you can see that you can go ahead with the rollback.
</p>
</li>
</ul></div>
</li>
<li>
<p>
Roll back the deployment:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc rollback ruby-hello-world --to-version=1 # X is your desired deployment

#2 rolled back to ruby-hello-world-1
Warning: the following images triggers were disabled: ruby-hello-world:latest
  You can re-enable them with: oc set triggers dc/ruby-hello-world --auto</code></pre>
</div></div>
</li>
</ol></div>
</li>
<li>
<p>
Click the <strong>Overview</strong> tab for your project and note that you have a new deployment is happening.
</p>
</li>
</ol></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">Rolling back to an old deployment version creates the old state as a new deployment. You just reference an old state when rolling back.</td>
</tr></table>
</div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
After a few minutes, go back to the application in your browser.
</p>
<div class="ulist"><ul>
<li>
<p>
The old "Welcome . . ." message is displayed.
</p>
</li>
</ul></div>
</li>
</ol></div>
</div>
<div class="sect3">
<h4 id="_roll_your_application_forward">4.2.4. Roll Your Application Forward</h4>
<div class="paragraph"><p>To roll forward (activate) the typo-enabled application:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[marina@master ~]$ oc rollback ruby-hello-world-X # X is your desired deployment

#11 rolled back to ruby-hello-world-X
Warning: the following images triggers were disabled: ruby-hello-world
  You can re-enable them with: oc deploy ruby-hello-world --enable-triggers</code></pre>
</div></div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_cluster_metrics_introduction">5. Cluster Metrics Introduction</h2>
<div class="sectionbody">
<div class="paragraph"><p>Metrics are an important part of every monitoring solution. Metrics allow you to watch certain counters in your environment. OpenShift Container Platform comes with a metric stack that you can use for multiple purposes. The default use case collects information about CPU usage, memory usage, network troughput and a few other measurements.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">
<div class="title">Self-Signed-Certificates</div>
<div class="paragraph"><p>If you are using self signed certificates, you will need to direct your browser to the metrics URL once and accept the certificate in order to see metrics in the WebUI of OCP.</p></div>
</td>
</tr></table>
</div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/Open-Metrics-URL-Error-in-WebUI.png" alt="Metrics Error in WebUI" />
</div>
</div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Click on "Open Metrics URL" - this will open a new tab in your browser.
</p>
</li>
<li>
<p>
In Firefox, click the "Advanced" button. Choose "Add Exeception&#8230;" and click "Confirm Security Exception".
</p>
</li>
<li>
<p>
The page will look like this, if everything is working.
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/Hawkular-Metrics-working.png" alt="http://people.redhat.com/~llange/labimg/Hawkular-Metrics-working.png" />
</div>
</div>
</li>
<li>
<p>
Reload the WebUI tab that showed the initial error. OCP 3.5 will show CPU, Memory and Network metrics in the WebUI Overview section of the projects and in the metrics tab of each pod.
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/Metrics-Default-Project-Overview-Page-Registy-Console.png" alt="http://people.redhat.com/~llange/labimg/Metrics-Default-Project-Overview-Page-Registy-Console.png" />
</div>
</div>
</li>
<li>
<p>
In the default project, click the pod of the registry-console, you get to the pod details :
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/Registry-Console-Pod-Details.png" alt="http://people.redhat.com/~llange/labimg/Registry-Console-Pod-Details.png" />
</div>
</div>
</li>
<li>
<p>
Go to the metrics tab will display more details and allow you to select what time range of metrics you want to display.
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/Metrics-Details-Registy-Console.png" alt="http://people.redhat.com/~llange/labimg/Metrics-Details-Registy-Console.png" />
</div>
</div>
</li>
</ol></div>
<div class="paragraph"><p>The metrics stack is usually deployed in the openshift-infra project. You can check status there as well. This can be done from the command line or in the Web UI. This is how you do this on the command line :</p></div>
<div class="listingblock">
<div class="content">
<pre><code>$ oc project openshift-infra
$ oc get pods
NAME                         READY     STATUS    RESTARTS   AGE
hawkular-cassandra-1-6tx6k   1/1       Running   0          36m
hawkular-metrics-zp23l       1/1       Running   1          36m
heapster-8fjdc               1/1       Running   0          36m</code></pre>
</div></div>
<div class="paragraph"><p>There are 3 pods in the output above. Notice that all pods are marked as running and all containers that are supposed to run in each pod are up (1/1). Heapster collects the metrics, hawkular provides it for retrieval and stores the metrics in the cassandra backend. There are 4 services created for internal communication and one route that allows access the the hawkular-metrics service from the WebUI.</p></div>
<div class="listingblock">
<div class="content">
<pre><code>$ oc get services
NAME                       CLUSTER-IP       EXTERNAL-IP   PORT(S)                               AGE
hawkular-cassandra         172.30.181.23    &lt;none&gt;        9042/TCP,9160/TCP,7000/TCP,7001/TCP   2h
hawkular-cassandra-nodes   None             &lt;none&gt;        9042/TCP,9160/TCP,7000/TCP,7001/TCP   2h
hawkular-metrics           172.30.78.27     &lt;none&gt;        443/TCP                               2h
heapster                   172.30.203.233   &lt;none&gt;        80/TCP                                2h</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>$ oc get route
NAME               HOST/PORT                                SERVICES           PORT
hawkular-metrics   hawkular-metrics.cloudapps.example.com   hawkular-metrics   &lt;all&gt;</code></pre>
</div></div>
<div class="paragraph"><p>The next diagram illustrates what you have just seen in the Web UI and on the Command line:</p></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/OpenShift-Hawkular-Stack-Overview.png" alt="Metric Stack Overview" />
</div>
</div>
<div class="paragraph"><p>Verify the metric stack status as well in your WebUI :</p></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/Metrics-Status-Overview-WebUI.png" alt="http://people.redhat.com/~llange/labimg/Metrics-Status-Overview-WebUI.png" />
</div>
</div>
<div class="paragraph"><p>You can also check the oadm diagnostics output to look for health information of the metric stack. Note that oadm offers a rich set ouf diagnostics to choose from.</p></div>
<div class="listingblock">
<div class="content">
<pre><code># oadm diagnostics --help
...

  oadm diagnostics &lt;DiagnosticName&gt;

The available diagnostic names are: AggregatedLogging, AnalyzeLogs, ClusterRegistry, ClusterRoleBindings, ClusterRoles,
ClusterRouter, ConfigContexts, DiagnosticPod, MasterConfigCheck, MasterNode, MetricsApiProxy, NetworkCheck,
NodeConfigCheck, NodeDefinitions, ServiceExternalIPs, UnitStatus.

Usage:
  oadm diagnostics [options]

...</code></pre>
</div></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Run the oadm diagnostics for the MetricsApiProxy :
</p>
</li>
</ol></div>
<div class="listingblock">
<div class="content">
<pre><code># oadm diagnostics MetricsApiProxy
[Note] Determining if client configuration exists for client/cluster diagnostics
Info:  Successfully read a client config file at '/root/.kube/config'
Info:  Using context for cluster-admin access: 'openshift-infra/master-example-com:8443/system:admin'

[Note] Running diagnostic: MetricsApiProxy
       Description: Check the integrated heapster metrics can be reached via the API proxy

[Note] Summary of diagnostics execution (version v3.5.5.8):
[Note] Completed with no errors or warnings seen.</code></pre>
</div></div>
</div>
</div>
<div class="sect1">
<h2 id="_cloudforms_introduction">6. CloudForms Introduction</h2>
<div class="sectionbody">
<div class="paragraph"><p>CloudForms is the designated Operations Tool for the Openshift Container Platform. But CloudForms is much more than just a tool to look at and manage OpenShift. It originally found it&#8217;s way into the Red Hat portfolio though the acquisition of the company ManageIQ. It was primarily a virtualisation management tool in the beginning. The big differentiator to other existing tools was the main focus on <strong>Operational Visibility</strong> or <strong>Insight</strong> as it is called back in the day.</p></div>
<div class="paragraph"><p>CloudForms is a manager of managers. It talks to the APIs of other management infrastructures. These are called providers.</p></div>
<div class="ulist"><div class="title">CloudForms can be the central manager for all these infrastructures</div><ul>
<li>
<p>
AWS
</p>
</li>
<li>
<p>
Google Cloud
</p>
</li>
<li>
<p>
Azure
</p>
</li>
<li>
<p>
Red Hat OpenStack
</p>
</li>
<li>
<p>
Microsoft System Center VMM
</p>
</li>
<li>
<p>
Red Hat Virtualization
</p>
</li>
<li>
<p>
VmWare vCenter
</p>
</li>
<li>
<p>
Ansible Tower
</p>
</li>
<li>
<p>
<strong>Red Hat OpenShift Container Platform</strong>
</p>
</li>
</ul></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/CloudForms-Overview1.png" alt="http://people.redhat.com/~llange/labimg/CloudForms-Overview1.png" />
</div>
</div>
<div class="paragraph"><p>CloudForms gathers information about objects first and puts these in the internal (PostgreSQL) database. You can then take action based on the found information and influence the managed infrastructures. This was called <strong>Control</strong>. Provisioning instances or virtual machines or containers is an example for control.</p></div>
<div class="paragraph"><p>Control is the basis for <strong>Service Automation</strong>. This is where the Service Catalog features of CloudForms come into play. You can build your own Services by designing your own or reusing existing Forms to collect all the details you need to provision and manage your workloads. The Service Lifecycle goes through several stages. You can order a service from a catalog. This will create a request that is then approved or denied. Approval can be manual or automatic. An approved request is then scheduled an run, thus creating a service. Another important part of a services life is retirement. That process can be planned and implemented with CloudForms.</p></div>
<div class="paragraph"><p>You can think of CloudForms as a framework that allows for implementation of <strong>your processes</strong> across all the supported infrastrutures.</p></div>
<div class="paragraph"><p>CloudForms can even take a look into the managed workloads. This is called <strong>Smart State Analysis</strong> and works storage based. CloudForms extracts a certain level of detail from the analysed workloads. These are things like installed Software with versions and Users e.g. You can even extend this mechanism to autodetect and label certain workloads. This tagging is another key aspect in CloudForms that helps you to structure your environment in a sensible way. Some people use Tags to designate Service Level Agreements.</p></div>
<div class="paragraph"><p><strong>Policy &amp; Compliance</strong> give me mechanisms to define what states I want to see in my environment and how to get there. I could for example postulate that every Microsoft Windows system needs to have a current virus scanner. CloudForms can see into the workloads and can see what software is installed. I could than declare / implement what should happen if there is no or an out of date virus scanner found. That action is highly depending on your environment and can range from inform someone, to do not allow this workload to run.</p></div>
<div class="paragraph"><p>The following image illustrates the level of detail that CloudForms collects for workloads. This is a virtual machine called ansible-tower. It is running a RHEL instance, the same level of detail is available for Microsoft Windows Workloads.</p></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/Virtual-Machine-Details-CloudForms.png" alt="Virtual Machine Details" />
</div>
</div>
<div class="paragraph"><p>:</p></div>
<div class="sect2">
<h3 id="_connect_to_cloudforms">6.1. Connect to CloudForms</h3>
<div class="paragraph"><p>We did deploy a CloudForms 4.5 for you as part of this Lab. Open your Browser and connect to it via <a href="http://cf.example.com">http://cf.example.com</a>.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Log into the CloudForms Interface using the User "admin" and the password r3dh4t1!.
</p>
<div class="paragraph"><p>You will find the main navigation panel on the right hand side. Hover over Compute, move to Containers and Click on "Overview" in the 3rd side panel. This will bring you to the Container Dashboard. This is an Overview over all configured OpenShift environments.</p></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/CloudForms-Container-Provider-Dashboardv2.png" alt="Container Dashboard" />
</div>
</div>
<div class="paragraph"><p>The container dashboard give a quick overview of the known / configured OpenShift Cluster Environments. The section at the top of the board lists the number of known Objects. Below this are several usage statistics. These are filled only if the hawkular metric stack is set up in your OpenShift Container Platform. Note that it will take up to 24h after configuring the Hawkular part of the provider setup in CloudForms until the usage information is displayed.</p></div>
<div class="paragraph"><p>CloudForms offers another tool called <strong>Topology</strong>. This view might be familiar to you if you know OpenStack Horizon. The Topology view can be quite full and overwhelming if your cluster is bigger or has many applications.</p></div>
</li>
<li>
<p>
Go to Compute &#8594; Containers &#8594; Click on Topology.
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/CloudForms-Container-Topology.png" alt="http://people.redhat.com/~llange/labimg/CloudForms-Container-Topology.png" />
</div>
</div>
<div class="paragraph"><p>If this view is too full use the service icons to toggle visibilty of the respective objects. You could also use the search field to grey out every object not matching your search.</p></div>
<div class="paragraph"><p>The nice thing about the topology view is that every object is displayed with a status indicator. In our case every object has a green border. It an object has a failed state, you will see it with a red boarder instead. You could chose to display object names, or hover over the object with the mouse cursor to see name, type and status of that object. A double click on the object will bring you to the details page of that object in CloudForms.</p></div>
</li>
</ol></div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_containers_and_security">7. Containers and Security</h2>
<div class="sectionbody">
<div class="paragraph"><p>In this Lab you will:</p></div>
<div class="ulist"><ul>
<li>
<p>
Start a Ruby based example application
</p>
</li>
<li>
<p>
Add a Heath Check
</p>
</li>
<li>
<p>
Scan the container image for known vulnerabilities
</p>
<div class="ulist"><ul>
<li>
<p>
[Optional : prevent execution of images with know vulnerabilities]
</p>
</li>
</ul></div>
</li>
<li>
<p>
Patch issues found if possible
</p>
</li>
<li>
<p>
Create a schedule for scanning container images
</p>
</li>
<li>
<p>
Configure OpenShift to always get the latest ruby builder image
</p>
</li>
</ul></div>
<div class="sect2">
<h3 id="_application_setup">7.1. Application Setup</h3>
<div class="paragraph"><p>For more background information on Application setup consult the official <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-new-app">OpenShift Documentation here</a>.</p></div>
<div class="paragraph"><p>There are multiple ways to start or create your application in OpenShift. You can use the oc tool from the command line, or you can use the WebUI. You could even do it with the RestAPI. We will document how to use the command line to create a test application here. You are free to use the Web UI as well. The command line offers a powerful oc sub command called new-app. <strong>oc new-app</strong> is the swiss army knife for application creation as it will create all the objects you need to run your application in the OpenShift Container Platform.</p></div>
<div class="paragraph"><p>You will first create a project for you application to live in. Projects are used to separate Application Management. There can be multiple apps with the same name on the same OpenShift cluster, as long as they live in different projects.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Create a project called "testproject" as user andrew now :
</p>
<div class="listingblock">
<div class="content">
<pre><code>$ oc new-project testproject --description="My Test Project" --display-name="Test Project"
now using project "testproject" on server "https://master.example.com:8443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.</code></pre>
</div></div>
</li>
<li>
<p>
take a look at the --help output from the oc new-app command :
</p>
<div class="listingblock">
<div class="content">
<pre><code>$ oc new-app --help
Create a new application by specifying source code, templates, and/or images
...</code></pre>
</div></div>
<div class="paragraph"><p>You can use ImageStreams, Templates and Docker Images to create an Application.</p></div>
<div class="paragraph"><p>An <strong>image stream</strong> comprises any number of Docker-formatted container images identified by tags. It presents a single virtual view of related images, similar to an image repository. Image streams can be used to automatically perform an action when new images are created. Builds and deployments can watch an image stream to receive notifications when new images are added and react by performing a build or deployment, respectively.</p></div>
<div class="paragraph"><p>A <strong>template</strong> describes a set of objects that can be parameterized and processed to produce a list of objects for creation by OpenShift Container Platform. The objects to create can include anything that users have permission to create within a project, for example services, build configurations, and deployment configurations. A template may also define a set of labels to apply to every object defined in the template.</p></div>
<div class="paragraph"><p>Be aware that the <strong>Docker Container images</strong> need to be compatible with the OpenShift security restrictions to run in the platform. Most images found on Docker Hub do not adhere to security best practices and run as root. This is not allowed on a default OpenShift installation. Take a good look at the <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/creating_images/#creating-images-guidelines">OpenShift Guidelines for Container Images</a>. Be sure to look through the <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/creating_images/#openshift-container-platform-specific-guidelines">OpenShift Specific Guidelines</a> as well.</p></div>
</li>
<li>
<p>
Create a Test Application using the ruby:2.3 builder image and the ruby-ex example application. You can do this in the WebUI or on the command line. This is how to do it on the command line :
</p>
<div class="listingblock">
<div class="content">
<pre><code>$ oc new-app openshift/ruby:2.3~https://github.com/openshift/ruby-ex --name=rtest</code></pre>
</div></div>
</li>
<li>
<p>
Go to the Overview Page of your testproject in the Web UI. If you are quick enough, you will be able to see the following screen. Notice that there is a build in progress.
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/TestProject-rtest1-Deploy-1.png" alt="http://people.redhat.com/~llange/labimg/TestProject-rtest1-Deploy-1.png" />
</div>
</div>
</li>
<li>
<p>
Click "View Log" and go to the build page. You can view the logs of the build process here. Notice that the last line should read "Push successful". This tells us that the resulting image is saved in internal Registry.
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/TestProject-rtest1-buildlog.png" alt="http://people.redhat.com/~llange/labimg/TestProject-rtest1-buildlog.png" />
</div>
</div>
</li>
<li>
<p>
After successful deployemnt your Overview page of the Test Project should look like this :
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/TestProject-rtest1-no-route.png" alt="http://people.redhat.com/~llange/labimg/TestProject-rtest1-no-route.png" />
</div>
</div>
<div class="paragraph"><p>You will need to create a route object to expose you application to access from the outside. Note that there will be a Route for your application already if you created it using the Web UI. If there is no route for you application you will find the "Create Route" button in top right corner like in the screen shot above. You could use this button to create a route in the Web UI. Or you could expose you application on the command line with :</p></div>
<div class="listingblock">
<div class="content"></div></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]# oc get service
NAME      CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
rtest     172.30.111.54   &lt;none&gt;        8080/TCP   20h</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]# oc expose service rtest
route "rtest" exposed</code></pre>
</div></div>
<div class="listingblock">
<div class="content"></div></div>
<div class="paragraph"><p>You Overview page of the Test Project should now display the URL <a href="http://rtest-testproject.cloudapps.example.com">http://rtest-testproject.cloudapps.example.com</a> instead of the "Create Route" button. Click the link to go see if your application is working. The result should look like this :</p></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/TestProject-rtest1-the-app.png" alt="http://people.redhat.com/~llange/labimg/TestProject-rtest1-the-app.png" />
</div>
</div>
<div class="paragraph"><p>If something went wrong and you want to delete your application, you can do this with the oc tool using the label app=truby. <strong>Do not delete the app</strong> if it runs without problems, we will use it in the next section of this lab.</p></div>
</li>
<li>
<p>
Delete the app, if you want to start over and try the oc new-app command again in step 2.
</p>
<div class="listingblock">
<div class="content">
<pre><code>[andrew@master ~]$ oc delete all -l app=rtest</code></pre>
</div></div>
</li>
</ol></div>
</div>
<div class="sect2">
<h3 id="_application_health_checks">7.2. Application Health Checks</h3>
<div class="paragraph"><p>Read more about Readiness and Liveness Check in the <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-application-health">OpenShift Developer Guide about health checks</a>.</p></div>
<div class="paragraph"><p>It is good style for your application to provide health information for the platform to consume. A best practice pattern is to offer a web page that provides a good return code if your app is healthy. The ruby example that we used provides this information here : <a href="http://rtest-testproject.cloudapps.example.com/health">http://rtest-testproject.cloudapps.example.com/health</a>.</p></div>
<div class="paragraph"><p>You can take a look at the application source on Github.com : <a href="https://github.com/openshift/ruby-ex">https://github.com/openshift/ruby-ex</a> for more details.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Got to the Web UI and open the Overview page for your testproject. Notic that you are displayed a warning about missing health checks.
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-1.png" alt="http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-1.png" />
</div>
</div>
</li>
<li>
<p>
Add the health check to your deployement by clicking "Add Health Checks" and then "Add Liveliness Probe".
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-2-App-Probe.png" alt="http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-2-App-Probe.png" />
</div>
</div>
<div class="paragraph"><p>There are three possible types of checks that you can chose. HTTP Get is suitable for our application here. Don&#8217;t forget to add the path "/health" before you hit the "save" button.</p></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-3-add-probe-details.png" alt="http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-3-add-probe-details.png" />
</div>
</div>
<div class="paragraph"><p>Notice that saving your changes changes your deployement settings, thus a new deployement is triggerd by the configuration change.</p></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-4-new-deployment-list.png" alt="http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-4-new-deployment-list.png" />
</div>
</div>
<div class="paragraph"><p>If you are quick enough, you can see the new deployment happening live.</p></div>
</li>
<li>
<p>
Go to the Overview Page of your "Test Project"
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-5-new-deployement-overview.png" alt="http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-5-new-deployement-overview.png" />
</div>
</div>
<div class="paragraph"><p>The platform is now able to even detect internal application failure situations, but is of cause depending on the nature of the the faults in the application and the quality of the health checks implemented. If you do not provide any health checks, OpenShift falls back onto checking if the docker container is listed as up and running.</p></div>
</li>
</ol></div>
</div>
<div class="sect2">
<h3 id="_container_image_security_scans_with_cloudforms">7.3. Container Image Security Scans with CloudForms</h3>
<div class="paragraph"><p>Container Scans help you to determine known vulnerabilities in your container images. You can run Container Images scans from CloudForms. Initiate a Container Scan for your the container image of the "rtest" application.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Sign in to your CloudForms instance <a href="https://cf.example.com">https://cf.example.com</a>.
</p>
</li>
<li>
<p>
Log in as User "admin" with the password "r3dh4t1!".
</p>
</li>
<li>
<p>
Go to : Compute &#8594; Containers &#8594; Container Images
</p>
</li>
<li>
<p>
Fill out the Search box in the upper right corner search for "rtest".
</p>
<div class="exampleblock">
<div class="title">Example 1. Can&#8217;t find the image &#8594; Initiate a Container Provider Refresh</div>
<div class="content">
<div class="paragraph"><p>If you can&#8217;t see your image, you can wait and try again later or you can initiate a Provider Refresh in CloudForms.</p></div>
<div class="paragraph"><p>+
. Go to "Compute" &#8594; "Containers" &#8594; "Providers"
. Select the OpenShift Cluster 1.
. Select the Configuration Menu and click "Refresh Items and Relationships".</p></div>
<div class="paragraph"><p>+
It might still take ~15 min for the Refresh to run.</p></div>
<div class="paragraph"><p>+
<span class="image">
<img src=":http://people.redhat.com/~llange/labimg/CloudForms-Container-Provider-Refresh.png" alt=":http://people.redhat.com/~llange/labimg/CloudForms-Container-Provider-Refresh.png" />
</span></p></div>
</div></div>
</li>
<li>
<p>
In the search results page, click on the name field of the rtest container image to take a loot at the details.
</p>
</li>
<li>
<p>
Notice that there is no information in the Configuration Box about the RPM <strong>packages</strong>.
</p>
</li>
<li>
<p>
There is no OpenSCAP Results as well. And the Compliance Box tells us that there is no status available.
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/rtest-Image-details-before-scan.png" alt="http://people.redhat.com/~llange/labimg/rtest-Image-details-before-scan.png" />
</div>
</div>
<div class="paragraph"><p>We want to change this.</p></div>
</li>
<li>
<p>
Assign a Policy Profile first.
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Open the Policy Menu
</p>
</li>
<li>
<p>
Click "Manage Policies"
</p>
</li>
<li>
<p>
Select the Policy Profile "OpenSCAP profile"
</p>
</li>
<li>
<p>
Click "save"
</p>
</li>
</ol></div>
</li>
<li>
<p>
You should see a box that reads "Policy Provile assigned successfully".
</p>
</li>
<li>
<p>
Request an analysis of image content, or in other words start a Container Image Scan.
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Open the Configuration Menu
</p>
</li>
<li>
<p>
Select "Perform SmartState Analysis"
</p>
</li>
<li>
<p>
Confirm "Perform SmartState Analysis on the selected items"
</p>
<div class="paragraph"><p>Look for the scan in the Web UI</p></div>
</li>
<li>
<p>
Go back to the OpenShift Web UI
</p>
</li>
<li>
<p>
Make sure you are loged in as <code>admin</code>
</p>
</li>
<li>
<p>
Select the "management-infra" project
</p>
</li>
<li>
<p>
You should see a workload starting for the container scan. Notice the openshift3/image-inspector image.
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/Image-Scan-in-progress.png" alt="http://people.redhat.com/~llange/labimg/Image-Scan-in-progress.png" />
</div>
</div>
<div class="paragraph"><p>The scan and the transfer of the information takes a few minutes in our demo environment. After the scan you should be able to see the scan details after a reload of the page in the CloudForms Interface.</p></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/rtest-scan-results-overview.png" alt="http://people.redhat.com/~llange/labimg/rtest-scan-results-overview.png" />
</div>
</div>
<div class="paragraph"><p>You should now see values like these :</p></div>
<div class="exampleblock">
<div class="content">
<div class="ulist"><ul>
<li>
<p>
430 Packages
</p>
</li>
<li>
<p>
458 OpenSCAP Results
</p>
</li>
<li>
<p>
An OpenSCAP html listed as available
</p>
</li>
<li>
<p>
The OpenSCAP Failed Rules Summary list 3 High severities results.
</p>
</li>
<li>
<p>
The Image was marked a <strong>Non-Compliant</strong>
</p>
</li>
<li>
<p>
We do have a Compliance History available now.
</p>
</li>
</ul></div>
</div></div>
</li>
</ol></div>
</li>
<li>
<p>
Click on the line "OpenScap Scan Results"
</p>
</li>
<li>
<p>
Click the String "Result" to sort for fails
</p>
</li>
<li>
<p>
You will notice that the name of the rule that failed is no helpful information for us
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/OpenSCAP-scan-results-in-cf-rtest.png" alt="http://people.redhat.com/~llange/labimg/OpenSCAP-scan-results-in-cf-rtest.png" />
</div>
</div>
</li>
<li>
<p>
Click on the OpenSCAP html line. Use your browser to display this html page.
</p>
</li>
<li>
<p>
Deselect the box "pass" to see the failed rules quickly. This list has the Red Hat Security Advisory Numbers and short text in the Rule Overview section table under tiles.
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/rtest-scan-results-scap-html.png" alt="http://people.redhat.com/~llange/labimg/rtest-scan-results-scap-html.png" />
</div>
</div>
<div class="paragraph"><p>Every Red Hat Security Advisory is explained in fine detail on <a href="https://rhn.redhat.com/errata">https://rhn.redhat.com/errata</a> and <a href="https://access.redhat.com/errata">https://access.redhat.com/errata</a>. The first page is the older incarnation that is still around and has more detail than the Customer Portal equivalent. You can go directly to <a href="https://access.redhat.com/errata/RHSA-2017:0372">https://access.redhat.com/errata/RHSA-2017:0372</a> .</p></div>
<div class="exampleblock">
<div class="content">
<div class="olist arabic"><div class="title">Links to the Customer Portal Advisory Pages for the issues found</div><ol class="arabic">
<li>
<p>
<a href="https://access.redhat.com/errata/RHSA-2017:0372">RHSA-2017:0372: kernel-aarch64 security and bug fix update (Important)</a>
</p>
</li>
<li>
<p>
<a href="https://access.redhat.com/errata/RHSA-2017:1308">RHSA-2017:1308: kernel security, bug fix, and enhancement update (Important)</a>
</p>
</li>
<li>
<p>
<a href="https://access.redhat.com/errata/RHSA-2017:1365">RHSA-2017:1365: nss security and bug fix update (Important)</a>
</p>
</li>
</ol></div>
</div></div>
<div class="paragraph"><p>Lets go through the list to see and evaluate what this scan found in our image.</p></div>
</li>
</ol></div>
<div class="sect3">
<h4 id="_rhsa_2017_0372">7.3.1. RHSA-2017:0372</h4>
<div class="paragraph"><p>This is an issue that is effecting kernels on Arm Architectures. Red Hat provides updated packages for aarch64 to fix this. The scans that we are doing are based on package numbers. And our container image holds a kernel specific package as well.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Check the package list of the Container Image for kernel packages.
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
You could loCloudForms, the WebUI or on the command line.
</p>
</li>
</ol></div>
</li>
</ol></div>
<div class="listingblock">
<div class="content">
<pre><code># oc project testproject
Now using project "testproject" on server "https://master.example.com:8443".
# oc get pods
NAME            READY     STATUS      RESTARTS   AGE
rtest-1-build   0/1       Completed   0          17h
rtest-1-rfs06   1/1       Running     0          17h
# oc rsh rtest-1-rfs06
sh-4.2$ rpm -qa | grep kernel
kernel-headers-3.10.0-514.16.1.el7.x86_64</code></pre>
</div></div>
<div class="paragraph"><p>As we are clearly not on aarch64. This is x86_64 so we can ignore this result and file a bugzilla against CloudForms and the Container Scanner. This is a false finding.</p></div>
</div>
<div class="sect3">
<h4 id="_rhsa_2017_1308">7.3.2. RHSA-2017:1308</h4>
<div class="paragraph"><p>This advisory relates to multiple issues with only one issue marked as important. A local attacker could possibly use a flaw in the packet_set_ring() function of the kernel to produce a buffer overflow and crash a system if that application ran with CAP_NET_RAW. It might be possible to use this buffer overflow to gain additional privileges.</p></div>
<div class="paragraph"><p>The question to ask here is, if this is / might be a problem for the container that you are running. As the affected package here is only the kernel-headers package, we can assume that this needs to be checked and fixed on the container host side as well.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">This issue will only impact you, if you did build an application on top of this image that used these kernel headers. This is why we have this advisory included.</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="_rhsa_2017_1365">7.3.3. RHSA-2017:1365</h4>
<div class="paragraph"><p>The third issue listed here is found in the Name Service Switch. This issue effects the name service switch that is genuine installed as rpms in the image.</p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>A null pointer dereference flaw was found in the way NSS handled empty SSLv2 messages. An attacker could use this flaw to crash a server application compiled against the NSS library. (CVE-2017-7502)</p></div>
</div>
<div class="attribution">
&#8212; RHSA-2017:1365 and CVE-2017-7502
</div></div>
</div>
<div class="sect3">
<h4 id="_image_details">7.3.4. Image Details</h4>
<div class="paragraph"><p>To be able to evaluate how to fix issues, you need to know where an Image came from and who created it. You should turn to the image creator first to fix issues found. There many different places that you can turn to for details of this image.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
You can look at the detailed page for the rtest Container Image in CloudForms
</p>
</li>
<li>
<p>
You can open the <a href="https://registry-console-default.cloudapps.example.com/registry#/images/testproject/rtest:latest">OpenShift Registry Console</a> in OpenShift ( login as admin or andrew )
</p>
</li>
<li>
<p>
You can got to the <code>oc</code> command line in OpenShift and look at Images and ImageStreams.
</p>
</li>
</ol></div>
<div class="paragraph"><p>We are using the <code>oc</code> command here. Note the bold printed parts below :</p></div>
<div class="listingblock">
<div class="content">
<pre><code>$ oc get images | grep rtest
sha256:b925ddb55063d5f26526ca09e2f55aec5a8c4e95a7e4e4b644dd6ba08e3733c4   172.30.120.134:5000/testproject/rtest@sha256:b925ddb55063d5f26526ca09e2f55aec5a8c4e95a7e4e4b644dd6ba08e3733c4</code></pre>
</div></div>
<div class="paragraph"><p>OpenShift identifies images with sha256 values. So we have to use this sha256 value to take a closer look.</p></div>
<div class="listingblock">
<div class="content">
<pre><code>$ oc describe image sha256:b925ddb55063d5f26526ca09e2f55aec5a8c4e95a7e4e4b644dd6ba08e3733c4
Name:           sha256:b925ddb55063d5f26526ca09e2f55aec5a8c4e95a7e4e4b644dd6ba08e3733c4
Namespace:      &lt;none&gt;
Created:        23 hours ago
Labels:         &lt;none&gt;
Annotations:    images.openshift.io/deny-execution=true <img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" />
                openshift.io/image.managed=true
                security.manageiq.org/failed-policy=openscap policy <img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" />
Docker Image:   172.30.120.134:5000/testproject/rtest@sha256:b925ddb55063d5f26526ca09e2f55aec5a8c4e95a7e4e4b644dd6ba08e3733c4
Image Size:     170.1 MB (first layer 73.86 MB, last binary layer 912.3 kB)
Image Created:  23 hours ago
Author:         &lt;none&gt;
Arch:           amd64
Entrypoint:     container-entrypoint
Command:        /usr/libexec/s2i/run
Working Dir:    /opt/app-root/src
User:           1001
Exposes Ports:  8080/tcp
Docker Labels:  architecture=x86_64
                authoritative-source-url=registry.access.redhat.com <img src="/etc/asciidoc/images/icons/callouts/3.png" alt="3" />
                build-date=2017-04-21T09:41:29.844044
                com.redhat.build-host=ip-10-29-120-102.ec2.internal
                com.redhat.component=rh-ruby23-docker
                description=The Red Hat Enterprise Linux Base image is designed to be a fully supported foundation for your containerized applications.  This base image provides your operations and application teams with the packages, language runtimes and tools necessary to run, maintain, and troubleshoot all of your applications. This image is maintained by Red Hat and updated regularly. It is designed and engineered to be the base layer for all of your containerized applications, middleware and utilites. When used as the source for all of your containers, only one copy will ever be downloaded and cached in your production environment. Use this image just like you would a regular Red Hat Enterprise Linux distribution. Tools like yum, gzip, and bash are provided by default. For further information on how this image was built look at the /root/anacanda-ks.cfg file.
                distribution-scope=public
                io.k8s.description=Platform for building and running Ruby 2.3 applications
                io.k8s.display-name=testproject/rtest-1:00972bc1
                io.openshift.build.commit.author=Ionut Palade &lt;PI-Victor@users.noreply.github.com&gt;
                io.openshift.build.commit.date=Mon Dec 12 14:37:32 2016 +0100
                io.openshift.build.commit.id=855ab2de53ff897a19e1055f7554c64d19e02c50
                io.openshift.build.commit.message=Merge pull request #6 from aj07/typo
                io.openshift.build.commit.ref=master
                io.openshift.build.image=registry.access.redhat.com/rhscl/ruby-23-rhel7@sha256:4b496b8b4d306badbea387f790004f867ca774526c17fb0fffdc88d58384c495 <img src="/etc/asciidoc/images/icons/callouts/4.png" alt="4" />
                io.openshift.build.source-location=https://github.com/openshift/ruby-ex.git
                io.openshift.expose-services=8080:http
                io.openshift.s2i.scripts-url=image:///usr/libexec/s2i
                io.openshift.tags=builder,ruby,ruby23,rh-ruby23
                io.s2i.scripts-url=image:///usr/libexec/s2i
                name=rhscl/ruby-23-rhel7 <img src="/etc/asciidoc/images/icons/callouts/5.png" alt="5" />
                release=6.7 <img src="/etc/asciidoc/images/icons/callouts/6.png" alt="6" />
                summary=Platform for building and running Ruby 2.3 applications
                vcs-ref=368e1c5301205f920e5a1ad00b075878d6cd3d54
                vcs-type=git
                vendor=Red Hat, Inc.
                version=2.3
Environment:    OPENSHIFT_BUILD_NAME=rtest-1
                OPENSHIFT_BUILD_NAMESPACE=testproject
                OPENSHIFT_BUILD_SOURCE=https://github.com/openshift/ruby-ex.git
                OPENSHIFT_BUILD_REFERENCE=master
                OPENSHIFT_BUILD_COMMIT=855ab2de53ff897a19e1055f7554c64d19e02c50
                PATH=/opt/app-root/src/bin:/opt/app-root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
                container=oci
                STI_SCRIPTS_URL=image:///usr/libexec/s2i
                STI_SCRIPTS_PATH=/usr/libexec/s2i
                HOME=/opt/app-root/src
                BASH_ENV=/opt/app-root/etc/scl_enable
                ENV=/opt/app-root/etc/scl_enable
                PROMPT_COMMAND=. /opt/app-root/etc/scl_enable
                RUBY_VERSION=2.3</code></pre>
</div></div>
<div class="colist arabic"><table>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" /></td><td>
This is a special annotation that was put in place by the container scan
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" /></td><td>
This denotes which policy was used for the failed scan
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/3.png" alt="3" /></td><td>
Where did the image come from? <a href="https://github.com/projectatomic/ContainerApplicationGenericLabels">This and more labels are explained here</a>
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/4.png" alt="4" /></td><td>
What builder image was used to create this image?
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/5.png" alt="5" /></td><td>
The docker name of the builder image
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/6.png" alt="6" /></td><td>
The release version of the used builder image
</td></tr>
</table></div>
<div class="ulist"><ul>
<li>
<p>
<em>Labels</em>: There are no OpenShift Labels on this object. If there were, you could use the -l "label=value" Option with the oc command line tool to select this object. As it is here, you can only use the long id that is found in the name field.
</p>
</li>
<li>
<p>
<em>Annotations</em>: There are special annotations on this image. The highlighted annotations <strong>images.openshift.io/deny-execution=true</strong> and
<strong>security.manageiq.org/failed-policy=openscap policy</strong> are put into the image metadata by the security scan that we triggered in CloudForms. These annotations to document that there were "important" security issues found. You can the annotations on the OpenShift side as we will later.
</p>
</li>
<li>
<p>
<em>Docker Labels</em>: There are labels on the image that speak for it&#8217;s origin. Look at vendor, name and release. The special OpenShift Label io.openshift.build.image notes the parent image. The <strong>io.openshift.build.image</strong> tells us that the rthest container image was built using the <strong>rhscl/ruby-23-rhel7</strong> builder image from the Red Hat Registry. Further down you find the <strong>release</strong> label that tells us that we used a builder image that was tagged with the docker label release=6.7.
</p>
</li>
</ul></div>
<div class="paragraph"><p>Lets find the image in the <a href="https://access.redhat.com/containers">Red Hat Container Catalog</a>. Our Red Hat Container Catalog provides detailed information about the images that we as Red Hat provide. We do document known issues and the fixes once they become available. We recently added a "Health Index" to deliver an easy first impression about the freshness of an image.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Got to <a href="https://access.redhat.com/containers">https://access.redhat.com/containers</a>.
</p>
</li>
<li>
<p>
Fill out the search field, put in <strong>rhscl/ruby-23</strong> and hit return.
</p>
<div class="paragraph"><p>You can see that we do have a newer release available than 6.7. Back when I did this, it was 6.8 as you can see in the screen shot below. You will need to click on "Tags" to get to that same view.</p></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/RHCC-ruby-23-with-tags.png" alt="Red Hat Container Catalog Ruby 2.3 Image Tags Tab" />
</div>
</div>
</li>
<li>
<p>
Click the tag "<a href="https://access.redhat.com/containers/#/registry.access.redhat.com/rhscl/ruby-23-rhel7/images/2.3-6.7">2.3-6.7</a>" in your list to get to the details about the builder image that we were using.
</p>
</li>
</ol></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/RHCC-ruby-23-67-details.png" alt="Red Hat Container Calalog Ruby 2.3-6.7 Details" />
</div>
</div>
<div class="paragraph"><p>Lets take a closer look at the details in the 2.3-6.7 release of the rhscl/ruby-23-rhel7 builder image. The screen shot above lists the health index as B. It has an explanation on the side what this means.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">
<div class="title">Health Index Level B</div>
<div class="paragraph"><p>This image is affected by Critical ( no older than 7 days ) or Important ( no older than 30 days ) security updates</p></div>
</td>
</tr></table>
</div>
<div class="paragraph"><p>Also note that there is an update builder image available that fixes issues found in the release 6.7. It might be the case that not all issues are fixed in the latest available image. Red Hat is building new container images in a scheduled fashion. That is why we might not have a certain fix in the latest image.  We do divert from our scheduled build and do async updates for critical updates only.</p></div>
</div>
</div>
<div class="sect2">
<h3 id="_repair_known_security_issues">7.4. Repair known Security Issues</h3>
<div class="paragraph"><p>We just found out that there is a newer version of the ruby builder image available in the Red Hat Registry. Lets update the s2i builder Image to the latest available version to fix security issues. Images are managed in ImageStreams in OpenShift. So lets take a look at the ruby ImageStream before we go and let OpenShift get the newest version of the ruby builder image.</p></div>
<div class="listingblock">
<div class="content">
<pre><code>$ oc get is ruby -n openshift
NAME      DOCKER REPO                          TAGS                         UPDATED
ruby      172.30.120.134:5000/openshift/ruby   2.2,2.0,latest + 1 more...   3 weeks ago</code></pre>
</div></div>
<div class="paragraph"><p>Or with a lot more detail :</p></div>
<div class="listingblock">
<div class="content">
<pre><code>$ oc describe is ruby -n openshift
Name:                   ruby
Namespace:              openshift
Created:                3 weeks ago
Labels:                 &lt;none&gt;
Annotations:            openshift.io/display-name=Ruby
                        openshift.io/image.dockerRepositoryCheck=2017-05-16T13:09:06Z
Docker Pull Spec:       172.30.120.134:5000/openshift/ruby
pass:quotes[*Unique Images:             3*]
Tags:                   4

2.3 (latest)
  tagged from registry.access.redhat.com/rhscl/ruby-23-rhel7:latest

  Build and run Ruby 2.3 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/blob/master/2.3/README.md.
  Tags: builder, ruby
  Supports: ruby:2.3, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

pass:quotes[  * *registry.access.redhat.com/rhscl/ruby-23-rhel7@sha256:4b496b8b4d306badbea387f790004f867ca774526c17fb0fffdc88d58384c495*]
      3 weeks ago

2.2
  tagged from registry.access.redhat.com/rhscl/ruby-22-rhel7:latest

  Build and run Ruby 2.2 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/tree/master/2.2/README.md.
  Tags: builder, ruby
  Supports: ruby:2.2, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

  * registry.access.redhat.com/rhscl/ruby-22-rhel7@sha256:f8b0adc1bdb409e0cfbaa39870077c4944eb52b8e222551ef3146eddf1c9e6cb
      3 weeks ago

2.0
  tagged from registry.access.redhat.com/openshift3/ruby-20-rhel7:latest

  Build and run Ruby 2.0 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/tree/master/2.0/README.md.
  Tags: hidden, builder, ruby
  Supports: ruby:2.0, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

  * registry.access.redhat.com/openshift3/ruby-20-rhel7@sha256:9cfdf4b811ace13d4c555335b249ab831832a384113035512abc9d4d5cc59716
      3 weeks ago
-</code></pre>
</div></div>
<div class="paragraph"><p>Notice the bold lines above. We do have <strong>3 unique images</strong> referenced by the ruby ImageStream currently. There is only one image listed for the tag 2.3.</p></div>
<div class="sect3">
<h4 id="_update_the_builder_image">7.4.1. Update the Builder Image</h4>
<div class="ulist"><ul>
<li>
<p>
The command below will update the ruby ImageStream and load the latest container image tagged ruby:2.3 from the Red Hat Registry.
</p>
</li>
<li>
<p>
An update to the ruby ImageStream will have an effect for you rtest application. Your BuildConfig for the rtest application is setup to watch the openshift/ruby:2.3 image stream for new Images. The update will trigger a new s2i build.
</p>
</li>
<li>
<p>
The result of that build will be a new version of your rtest application in container image format.
</p>
</li>
<li>
<p>
This will be pushed into the internal OpenShift registry.
</p>
</li>
<li>
<p>
The DeployementConfig of your rtest application watches the rtest ImageStream for new versions and will trigger a new deployment in turn.
</p>
</li>
</ul></div>
<div class="paragraph"><p><strong>You can watch all of this happening in your environment if you are quick enough</strong></p></div>
<div class="listingblock">
<div class="content">
<pre><code>$ oc import-image ruby:2.3 -n openshift
The import completed successfully.

Name:                   ruby
Namespace:              openshift
Created:                3 weeks ago
Labels:                 &lt;none&gt;
Annotations:            openshift.io/display-name=Ruby
                        openshift.io/image.dockerRepositoryCheck=2017-06-13T11:19:06Z
Docker Pull Spec:       172.30.120.134:5000/openshift/ruby
pass:quotes[*Unique Images:             4*]
Tags:                   4

2.3 (latest)
  tagged from registry.access.redhat.com/rhscl/ruby-23-rhel7:latest

  Build and run Ruby 2.3 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/blob/master/2.3/README.md.
  Tags: builder, ruby
  Supports: ruby:2.3, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

pass:quotes[  * *registry.access.redhat.com/rhscl/ruby-23-rhel7@sha256:3539e468222542cbea0c127927db191c2bd823e134ab241de971c2f14fed5fc7
      Less than a second ago*]
    registry.access.redhat.com/rhscl/ruby-23-rhel7@sha256:4b496b8b4d306badbea387f790004f867ca774526c17fb0fffdc88d58384c495
      3 weeks ago

2.2
  tagged from registry.access.redhat.com/rhscl/ruby-22-rhel7:latest

  Build and run Ruby 2.2 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/tree/master/2.2/README.md.
  Tags: builder, ruby
  Supports: ruby:2.2, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

  * registry.access.redhat.com/rhscl/ruby-22-rhel7@sha256:f8b0adc1bdb409e0cfbaa39870077c4944eb52b8e222551ef3146eddf1c9e6cb
      3 weeks ago

2.0
  tagged from registry.access.redhat.com/openshift3/ruby-20-rhel7:latest

  Build and run Ruby 2.0 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/tree/master/2.0/README.md.
  Tags: hidden, builder, ruby
  Supports: ruby:2.0, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

  * registry.access.redhat.com/openshift3/ruby-20-rhel7@sha256:9cfdf4b811ace13d4c555335b249ab831832a384113035512abc9d4d5cc59716
      3 weeks ago</code></pre>
</div></div>
<div class="paragraph"><p>Notice that we now have 4 unique images and that there is a new image for the 2.3 tag that was synced less than a second ago.</p></div>
</div>
<div class="sect3">
<h4 id="_watch_the_new_build">7.4.2. Watch the new Build</h4>
<div class="exampleblock">
<div class="title">Example 2. See the build in OpenShift</div>
<div class="content">
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Go to the Overview Page of the "Test Project" and see the new build running. ( you need to be quick )
</p>
</li>
</ol></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/Image-Update-rtest-build-triggered.png" alt="A new build of the rtest container image was triggerd" />
</div>
</div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Watch the build on the the command line :
</p>
</li>
</ol></div>
<div class="listingblock">
<div class="content">
<pre><code># oc get bc rtest
NAME      TYPE      FROM      LATEST
rtest     Source    Git       2

[root@master ~]# oc describe bc
Name:           rtest
Namespace:      testproject
Created:        About an hour ago
Labels:         app=rtest
Annotations:    openshift.io/generated-by=OpenShiftNewApp
Latest Version: 2

Strategy:       Source
URL:            https://github.com/openshift/ruby-ex
From Image:     ImageStreamTag openshift/ruby:2.3 <img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" />
Output to:      ImageStreamTag rtest:latest

Build Run Policy:       Serial
Triggered by:           Config, ImageChange <img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" />
Webhook GitHub:
        URL:    https://master.example.com:8443/oapi/v1/namespaces/testproject/buildconfigs/rtest/webhooks/bIqI1y4ETX7uADNm-PMo/github
Webhook Generic:
        URL:            https://master.example.com:8443/oapi/v1/namespaces/testproject/buildconfigs/rtest/webhooks/aeS1J1OTCInI4Fv4kQMh/generic
        AllowEnv:       false

Build           Status          Duration        Creation Time
rtest-2         complete        2m29s           2017-06-13 07:19:06 -0400 EDT
rtest-1         complete        2m17s           2017-06-13 06:24:44 -0400 EDT</code></pre>
</div></div>
<div class="colist arabic"><table>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" /></td><td>
This is the reference to the <strong>openshift/ruby:2.3</strong> ImageStream
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" /></td><td>
Here you see what kind of triggers are configured for this buildconfig
</td></tr>
</table></div>
<div class="paragraph"><p>This is the BuildConfig that is watching the builder image ImageStream and start a new build automatically if an image change is detected.</p></div>
</div></div>
</div>
<div class="sect3">
<h4 id="_inspect_the_new_deployment">7.4.3. Inspect the new Deployment</h4>
<div class="paragraph"><p>The new build did put a new image into the internal registry. There is an rtest ImageStream in the testproject that is used by the rrtest DeploymentConfig to watch for ImageChanges and trigger new deployements.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Look up the rtest deployment in the WebUI :
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Select andrews "Test Project"
</p>
</li>
<li>
<p>
Select "Applications"
</p>
</li>
<li>
<p>
Click "Deployements" in the sub menu
</p>
</li>
<li>
<p>
Click "rtest" in the list of deployments
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/Image-Update-rtest-deployment-3.png" alt="rtest deployment" />
</div>
</div>
</li>
</ol></div>
</li>
<li>
<p>
Look at your rtest DeploymentConfig on the cmd :
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc describe dc
Name:           rtest
Namespace:      testproject
Created:        2 hours ago
Labels:         app=rtest
Annotations:    openshift.io/generated-by=OpenShiftNewApp
Latest Version: 3
Selector:       app=rtest,deploymentconfig=rtest
Replicas:       1
Triggers:       Config, Image(rtest@latest, auto=true)
Strategy:       Rolling
Template:
  Labels:       app=rtest
                deploymentconfig=rtest
  Annotations:  openshift.io/generated-by=OpenShiftNewApp
  Containers:
   rtest:
    Image:                      172.30.120.134:5000/testproject/rtest@sha256:828d41ae8c7044026732d2092734b312a27044241c23238f3a01525ad5a606c2
    Port:                       8080/TCP
    Liveness:                   http-get http://:8080/health delay=0s timeout=1s period=10s #success=1 #failure=3
    Volume Mounts:              &lt;none&gt;
    Environment Variables:      &lt;none&gt;
  No volumes.

Deployment #3 (latest):
        Name:           rtest-3
        Created:        45 minutes ago
        Status:         Complete
        Replicas:       1 current / 1 desired
        Selector:       app=rtest,deployment=rtest-3,deploymentconfig=rtest
        Labels:         app=rtest,openshift.io/deployment-config.name=rtest
        Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Deployment #2:
        Created:        about an hour ago
        Status:         Complete
        Replicas:       0 current / 0 desired
Deployment #1:
        Created:        2 hours ago
        Status:         Complete
        Replicas:       0 current / 0 desired
...</code></pre>
</div></div>
<div class="paragraph"><p>Both outputs above display that you are running on deployment No. 3 now. You can see in the Web UI that the last deployment was triggered by an image change. To see the same information on the command line you would need to take a look at the oc rollout command :</p></div>
<div class="listingblock">
<div class="content">
<pre><code># oc rollout --help
...

# oc rollout history dc/rtest
deploymentconfigs "rtest"
REVISION        STATUS          CAUSE
1               Complete        image change
2               Complete        config change
3               Complete        image change</code></pre>
</div></div>
<div class="paragraph"><p>For more information about Deployments take a look at the <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#deployments">OpenShift Developer Guide Deployements Chapter</a>.</p></div>
</li>
</ol></div>
<div class="paragraph"><p>If you know recent Kubernetes versions, you will have come across the Deployment Object. The OpenShift DeploymentConfig Objects are far more advanced than the Kubernetes Deployements. Read about the limitations <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-kubernetes-deployments-support">here</a>. You can expect the Kubernetes Deployments to get more and more features as we are upstreaming the work done in OpenShift.</p></div>
</div>
<div class="sect3">
<h4 id="_image_details_again">7.4.4. Image Details (again)</h4>
<div class="paragraph"><p>Which release of the ruby builder image was used to build the latest version of the rtest container image? You can check this in 3 different places, look for the release label in one of them :</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
In CloudForms - Compute &#8594; Containers &#8594; Conainer Images - search rtest
</p>
</li>
<li>
<p>
In the <a href="https://registry-console-default.cloudapps.example.com/registry">Registry Console</a>
</p>
</li>
<li>
<p>
on the command line through oc describe is rtest and oc describe images $IMID
</p>
</li>
</ol></div>
<div class="listingblock">
<div class="content">
<pre><code># oc describe is rtest -n testproject | grep sha | head -1
  * 172.30.120.134:5000/testproject/rtest@sha256:828d41ae8c7044026732d2092734b312a27044241c23238f3a01525ad5a606c2
# oc describe images sha256:828d41ae8c7044026732d2092734b312a27044241c23238f3a01525ad5a606c2 | grep release
                release=6.8</code></pre>
</div></div>
</div>
<div class="sect3">
<h4 id="_optional_rescan_the_new_container_images">7.4.5. [Optional] Rescan the new container images</h4>
<div class="paragraph"><p>Go to the CloudForms interface and schedule another image scam for the newly created rtest image. Notice that you will need to know your sha256 ID to identify the correct images quickly. In my case this sha256:828d&#8230;</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Find the new rtest Image
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Compute &#8594; Containers &#8594; Container Images &#8594; Search "rtest"
</p>
</li>
<li>
<p>
Click the correct Container Image identified by the sha256:&#8230;
</p>
</li>
</ol></div>
</li>
<li>
<p>
Assign the OpenSCAP Policy Profile
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Open the Menu "Policy"
</p>
</li>
<li>
<p>
Click "Manage Policies"
</p>
</li>
<li>
<p>
Select check box "OpenSCAP profile"
</p>
</li>
<li>
<p>
Click save
</p>
</li>
</ol></div>
</li>
<li>
<p>
Schedule Container Scan
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Click "Configuration" Menu
</p>
</li>
<li>
<p>
Select "Perform SmartState Analysis"
</p>
</li>
<li>
<p>
Confirm "Perform SmartState Analysis on this item"
.Wait a few minutes and reload the page. If the scan does not start at all, close your browser completely, start it again and schedlue the smart state analysis again.
.Load the OpenSCAP html information by clicking the "OpenSCAP HTML" line
</p>
</li>
<li>
<p>
select to open in your browser
</p>
</li>
<li>
<p>
scoll down and deselect the check box "pass"
</p>
</li>
</ol></div>
</li>
</ol></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/OpenSCAP-scan-results-in-cf-rtest-2.png" alt="http://people.redhat.com/~llange/labimg/OpenSCAP-scan-results-in-cf-rtest-2.png" />
</div>
</div>
<div class="paragraph"><p>Notice that we have fewer issues then before. The false finding with the aarch64 issue is still in and you can continue to ignore it. The <a href="https://access.redhat.com/errata/RHSA-2017:1308">RHSA-2017:1308</a> issue is gone. The nss security bug <a href="https://access.redhat.com/errata/RHSA-2017:1365">RHSA-2017-1365</a> is still present as was to be expected. We saw that this issue was not fixed with the <a href="https://access.redhat.com/containers/#/registry.access.redhat.com/rhscl/ruby-23-rhel7/images/2.3-6.8">6.8 release of our rhscl/ruby-23-rhel7</a> builder image when we looked it up in the <a href="https://access.redhat.com/containers">Red Hat Container Catalog</a>.</p></div>
</div>
<div class="sect3">
<h4 id="_optional_get_the_latest_ruby_builder_image_automatically">7.4.6. [Optional] Get the latest ruby builder image automatically</h4>
<div class="paragraph"><p>Could OpenShift get the latest builder image as soon as it becomes available? Yes it can!</p></div>
<div class="listingblock">
<div class="content">
<pre><code># oc tag is ruby:2.3 -n openshift --scheduled=true
Tag ruby:2.3 set to import is periodically.</code></pre>
</div></div>
<div class="paragraph"><p>This will set the ImportPolicy on the Image in the ImageStream Ruby that is tagged with 2.3. OpenShift will fetch new versions of this builder image every 15 min if the are available. The interval can be set in the master-config.yaml. The keyword is scheduledImageImportMinimumIntervalSeconds and defaults to 900 if it is not specified.</p></div>
</div>
</div>
<div class="sect2">
<h3 id="_optional_continued_security_in_container_environments">7.5. [Optional] Continued Security in Container Environments</h3>
<div class="paragraph"><p>This section will show you how you can examine your Container Images on a regular basis, and how you OpenShift can consume this information to prevent execution of additional workloads when they are found to be vulnerable.</p></div>
<div class="sect3">
<h4 id="_optional_create_a_schedule_for_regular_security_checks">7.5.1. [Optional] Create a Schedule for regular Security Checks</h4>
<div class="paragraph"><p>It is usually not sufficient to trigger container scans manually. If you build and check an image today and it is found to be good, this can change tomorrow or in a week or a year as new vulnerabilities are discovered. To address this, you want to schedule security scans at regular interval. You can do this with CloudForms.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
First assign the OpenSCAP Policy not to a single image, but on the Container Provider Level.
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Go to Compute &#8594; Container &#8594; Providers
</p>
</li>
<li>
<p>
Select the "OpenShift Cluster 1 Provider" checkbox
</p>
</li>
<li>
<p>
Click the Policy Button
</p>
</li>
<li>
<p>
Click "Manage Policies"
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/CloudForms-Manage-Policies-for-Providers.png" alt="Manage Provider Policies" />
</div>
</div>
</li>
<li>
<p>
Select the "OpenSCAP profile" checkbox
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/CloudForms-Manage-Policies-for-Providers-OpenSCAP.png" alt="Manage Provider Policies assign OpenSCAP Profile" />
</div>
</div>
</li>
<li>
<p>
Click the "Save" button.
</p>
</li>
</ol></div>
</li>
<li>
<p>
Create a Schedule to scan all images in your internal registry.
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Open the EVM Menu on the upper right corner of the CloudForms Interfach
</p>
</li>
<li>
<p>
Click "Configuration"
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/CloudForms-Add-Schedule-1.png" alt="http://people.redhat.com/~llange/labimg/CloudForms-Add-Schedule-1.png" />
</div>
</div>
</li>
<li>
<p>
Make sure to select "Schedules" in the Setting Part of the Accordion.
</p>
</li>
<li>
<p>
Open the "Configuration" Menu
</p>
</li>
<li>
<p>
Click "Add Schedule"
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/CloudForms-Add-Schedule-2.png" alt="http://people.redhat.com/~llange/labimg/CloudForms-Add-Schedule-2.png" />
</div>
</div>
</li>
<li>
<p>
Fill in the Details for you Schedule as in the screen shot below. But select a time ~10 min from now if you want to see CloudForms scheduling scans.
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/CloudForms-Create-Scanning-Schedule-3.png" alt="http://people.redhat.com/~llange/labimg/CloudForms-Create-Scanning-Schedule-3.png" />
</div>
</div>
</li>
<li>
<p>
Click the "Add" button to add the schedule.
</p>
</li>
</ol></div>
</li>
</ol></div>
<div class="paragraph"><p>Now wait for that schedule to start scanning all your container images.</p></div>
</div>
<div class="sect3">
<h4 id="_optional_prevent_the_starting_of_vulnerable_workloads">7.5.2. [Optional] Prevent the starting of vulnerable Workloads</h4>
<div class="paragraph"><p>It is possible to use the annotations that the Container Scan puts into the Container Image Metadata on the OpenShift side to prevent starting more vulnerable workloads. You can consult the documentation for more background :</p></div>
<div class="ulist"><ul>
<li>
<p>
<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/container_security_guide/#controlling-pod-execution">Documentation on Controlling Pod Executon</a>
</p>
</li>
<li>
<p>
<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/cluster_administration/#admin-guide-image-policy">ImagePolicy Documentation</a>
</p>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Before we implement this safe guard, verify that you can scale up your rtest application. This can be done easily from the Web UI by increasing the number of pods in the Project Overview, or in the DeploymentConfig.
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/OpenShift-Scaling-rtest-Project-Overview-level.png" alt="http://people.redhat.com/~llange/labimg/OpenShift-Scaling-rtest-Project-Overview-level.png" />
</div>
<div class="title">Figure 1. Scale rtest in the Web UI Project Overview</div>
</div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/OpenShift-Scaling-rtest-dc-level.png" alt="http://people.redhat.com/~llange/labimg/OpenShift-Scaling-rtest-dc-level.png" />
</div>
<div class="title">Figure 2. Scale rtest in the Deployment View</div>
</div>
</li>
<li>
<p>
You can do the same on the command line with :
</p>
<div class="listingblock">
<div class="title">Scale rtest in the command line</div>
<div class="content">
<pre><code># oc get rc
NAME      DESIRED   CURRENT   READY     AGE
rtest-1   0         0         0         19h
rtest-2   0         0         0         19h
rtest-3   0         0         0         19h
rtest-4   2         2         2         5m</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code># oc scale dc/rtest --replicas=3
deploymentconfig "rtest" scaled</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code># oc get rc
NAME      DESIRED   CURRENT   READY     AGE
rtest-1   0         0         0         20h
rtest-2   0         0         0         19h
rtest-3   0         0         0         19h
rtest-4   3         3         3         7m</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code># oc get pods
[root@master ~]# oc get pods
NAME            READY     STATUS      RESTARTS   AGE
rtest-1-build   0/1       Completed   0          20h
rtest-2-build   0/1       Completed   0          19h
rtest-4-1mzg0   1/1       Running     0          2m
rtest-4-crlfp   1/1       Running     0          5m
rtest-4-s6bmz   1/1       Running     0          7m</code></pre>
</div></div>
<div class="paragraph"><p>You can see above that the rtest-4 deployment was scaled to 3 pods successfully.</p></div>
<div class="paragraph"><p>We can use the ImagePolicy settings in the <code>master-config.yaml</code> of the OpenShift Master to instruct OpenShift not to start any workloads that have a certain annotation. Already running workloads would not be affected by this unless you scale them down. Scaling up counts as starting new workloads in this case.</p></div>
</li>
<li>
<p>
Make sure the following settings are found in your master-config.yaml. Be sure to get the indentation right.
</p>
<div class="listingblock">
<div class="title">ImagePolicy in /etc/origin/master/master-config.yaml</div>
<div class="content">
<pre><code>admissionConfig:
  pluginConfig:
pass:quotes[    *openshift.io/ImagePolicy:
      configuration:
        kind: ImagePolicyConfig
        apiVersion: v1
        resolveImages: AttemptRewrite
        executionRules:
        - name: execution-denied
          onResources:
          - resource: pods
          - resource: builds
          reject: true
          matchImageAnnotations:
          - key: images.openshift.io/deny-execution
            value: "true"
          skipOnResolutionFailure: true
        - name: allow-images-from-internal-registry
          # allows images from the internal registry and tries to resolve them
          onResources:
          - resource: pods
          - resource: builds
          matchIntegratedRegistry: true
        - name: allow-images-from-dockerhub
          onResources:
          - resource: pods
          - resource: builds
          matchRegistries:
          - docker.io*]
    BuildDefaults:
...</code></pre>
</div></div>
</li>
<li>
<p>
Restart the atomic-openshift-master service
</p>
<div class="listingblock">
<div class="content">
<pre><code>$ systemctl restart atomic-openshift-master</code></pre>
</div></div>
</li>
<li>
<p>
try scaling up your rtest application now.
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/OpenShift-Scaling-rtest-Project-Overview-level-not-working.png" alt="http://people.redhat.com/~llange/labimg/OpenShift-Scaling-rtest-Project-Overview-level-not-working.png" />
</div>
<div class="title">Figure 3. Note that you fail and scaling seems to be stuck</div>
</div>
</li>
<li>
<p>
Lets us dig for an error message about what is going on
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Go to Monitoring in the Web UI
</p>
</li>
<li>
<p>
Find the Events section on the top left
</p>
</li>
<li>
<p>
Click "View Details"
</p>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/OpenShift-Scaling-rtest-prevented-event-view.png" alt="http://people.redhat.com/~llange/labimg/OpenShift-Scaling-rtest-prevented-event-view.png" />
</div>
<div class="title">Figure 4. This tells why you can&#8217;t scale up any more</div>
</div>
</li>
</ol></div>
</li>
</ol></div>
</li>
</ul></div>
<div class="paragraph"><p>The message reads : "Forbidden: this image is prohibited by policy".</p></div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_optional_container_native_storage_lab">8. [Optional] Container Native Storage Lab</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_overview_2">8.1. Overview</h3>
<div class="paragraph"><p>In this section you will set up container-native storage (CNS) in your environment. You will use this to dynamically provision storage for containerized applications. It is provided by GlusterFS running in containers.<br />
GlusterFS in turn is backed by local storage available to the OpenShift nodes.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">All of the following tasks are carried out as root from the master node. All files created can be stored in root&#8217;s home directory unless a particular path is specified. At the end of this section you will have 3 GlusterFS pods running together with the heketi API frontend properly integrated into OpenShift.</td>
</tr></table>
</div>
</div>
<div class="sect2">
<h3 id="_deploying_container_native_storage">8.2. Deploying Container-native Storage</h3>
<div class="paragraph"><p>Make sure you are logged on to the master node.</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# hostname
master.example.com</code></pre>
</div></div>
<div class="paragraph"><p>First, as the root user, install the CNS deployment tool.<br />
We will also install ansible. Though not needed for CNS in this lab it will help us simplify an otherwise tedious manual configuration step.</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# yum -y install cns-deploy ansible</code></pre>
</div></div>
<hr />
<div class="sect3">
<h4 id="_configure_openshift_node_firewall_with_ansible">8.2.1. Configure OpenShift Node firewall with Ansible</h4>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">In the following section we will configure Ansible. We will use it&#8217;s configuration management capabilities in order to make sure all the OpenShift nodes have the right firewall settings.</td>
</tr></table>
</div>
<div class="exampleblock">
<div class="title">Example 3. Ansible setup</div>
<div class="content">
<div class="paragraph"><p>Replace the content of the Ansible inventory in <code>/etc/ansible/hosts</code> with the following</p></div>
<div class="listingblock">
<div class="title">/etc/ansible/hosts</div>
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000000">[master]</span></span>
master<span style="color: #990000">.</span>example<span style="color: #990000">.</span>com

<span style="font-weight: bold"><span style="color: #000000">[nodes]</span></span>
node1<span style="color: #990000">.</span>example<span style="color: #990000">.</span>com
node2<span style="color: #990000">.</span>example<span style="color: #990000">.</span>com
node3<span style="color: #990000">.</span>example<span style="color: #990000">.</span>com</tt></pre></div></div>
<div class="paragraph"><p>You should now be able to ping all hosts using Ansible</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# ansible nodes -m ping

node3.example.com | SUCCESS =&gt; {
    "changed": false,
    "ping": "pong"
}
node2.example.com | SUCCESS =&gt; {
    "changed": false,
    "ping": "pong"
}
node1.example.com | SUCCESS =&gt; {
    "changed": false,
    "ping": "pong"
}</code></pre>
</div></div>
<div class="paragraph"><p>Create a file called <code>configure-firewall.yml</code> and copy&amp;paste the following contents:</p></div>
<div class="listingblock">
<div class="title">configure-firewall.yml</div>
<div class="content"></div></div>
<div class="paragraph"><p>Done. This little helper construct will save us some work in configuring the firewall. Run it with the following command:</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# ansible-playbook configure-firewall.yml</code></pre>
</div></div>
<div class="paragraph"><p>Your output should look like this.</p></div>
<div class="literalblock">
<div class="content">
<pre><code>PLAY [nodes] *******************************************************************

TASK [setup] *******************************************************************
ok: [node2.example.com]
ok: [node1.example.com]
ok: [node3.example.com]

TASK [insert iptables rules required for GlusterFS] ****************************
changed: [node3.example.com]
changed: [node2.example.com]
changed: [node1.example.com]

TASK [reload iptables] *********************************************************
changed: [node2.example.com]
changed: [node1.example.com]
changed: [node3.example.com]

PLAY RECAP *********************************************************************
node1.example.com          : ok=3    changed=2    unreachable=0    failed=0
node2.example.com          : ok=3    changed=2    unreachable=0    failed=0
node3.example.com          : ok=3    changed=2    unreachable=0    failed=0</code></pre>
</div></div>
</div></div>
<hr />
<div class="paragraph"><p>With this we checked the requirement for additional firewall ports to be opened on the OpenShift app nodes.</p></div>
</div>
<div class="sect3">
<h4 id="_prepare_openshift_for_cns">8.2.2. Prepare OpenShift for CNS</h4>
<div class="paragraph"><p>Next we will create a namespace (also referred to as a <em>Project</em>) in OpenShift. It will be used to group the GlusterFS pods.
For this you need to be logged as an admin user in OpenShift.</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# oc whoami
system:admin</code></pre>
</div></div>
<div class="paragraph"><p>If you are for some reason not an admin, login as system admin like this:</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# oc login -u system:admin -n default</code></pre>
</div></div>
<div class="paragraph"><p>Create a namespace with a designation of your choice. In this example we will use <code>container-native-storage</code>.</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# oc new-project container-native-storage</code></pre>
</div></div>
<div class="paragraph"><p>GlusterFS pods need access to the physical block devices on the host. Hence they need elevated permissions. Enable containers to run in privileged mode.</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# oadm policy add-scc-to-user privileged -z default</code></pre>
</div></div>
</div>
<div class="sect3">
<h4 id="_describe_container_native_storage_topology">8.2.3. Describe Container-native Storage Topology</h4>
<div class="paragraph"><p>CNS will virtualize locally attached block storage on the OpenShift App nodes. In order to deploy you will need to supply the installer with information about where to find these nodes and what network and which block devices to use.<br />
This is done using JSON file describing the topology of your OpenShift deployment.</p></div>
<div class="paragraph"><p>For this purpose, create the file topology.json with the following content:</p></div>
<div class="listingblock">
<div class="title">topology.yml</div>
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="color: #990000">{</span>
    "clusters": <span style="color: #990000">[</span>
        <span style="color: #990000">{</span>
            "nodes": <span style="color: #990000">[</span>
                <span style="color: #990000">{</span>
                    "node": <span style="color: #990000">{</span>
                        "hostnames": <span style="color: #990000">{</span>
                            "manage": <span style="color: #990000">[</span>
                                "node1.example.com"
                            <span style="color: #990000">],</span>
                            "storage": <span style="color: #990000">[</span>
                                "192.168.0.102"
                            <span style="color: #990000">]</span>
                        <span style="color: #990000">},</span>
                        "zone": <span style="color: #993399">1</span>
                    <span style="color: #990000">},</span>
                    "devices": <span style="color: #990000">[</span>
                        "/dev/vdc"
                    <span style="color: #990000">]</span>
                <span style="color: #990000">},</span>
                <span style="color: #990000">{</span>
                    "node": <span style="color: #990000">{</span>
                        "hostnames": <span style="color: #990000">{</span>
                            "manage": <span style="color: #990000">[</span>
                                "node2.example.com"
                            <span style="color: #990000">],</span>
                            "storage": <span style="color: #990000">[</span>
                                "192.168.0.103"
                            <span style="color: #990000">]</span>
                        <span style="color: #990000">},</span>
                        "zone": <span style="color: #993399">2</span>
                    <span style="color: #990000">},</span>
                    "devices": <span style="color: #990000">[</span>
                        "/dev/vdc"
                    <span style="color: #990000">]</span>
                <span style="color: #990000">},</span>
                <span style="color: #990000">{</span>
                    "node": <span style="color: #990000">{</span>
                        "hostnames": <span style="color: #990000">{</span>
                            "manage": <span style="color: #990000">[</span>
                                "node3.example.com"
                            <span style="color: #990000">],</span>
                            "storage": <span style="color: #990000">[</span>
                                "192.168.0.104"
                            <span style="color: #990000">]</span>
                        <span style="color: #990000">},</span>
                        "zone": <span style="color: #993399">3</span>
                    <span style="color: #990000">},</span>
                    "devices": <span style="color: #990000">[</span>
                        "/dev/vdc"
                    <span style="color: #990000">]</span>
                <span style="color: #990000">}</span>
            <span style="color: #990000">]</span>
        <span style="color: #990000">}</span>
    <span style="color: #990000">]</span>
<span style="color: #990000">}</span></tt></pre></div></div>
<div class="paragraph"><p>This file contains an additional property called <code>zone</code> per node. This identifies the failure domain. In CNS data is always replicated 3 times. Failure domains make sure that two copies are never stored on nodes in the same failure domain.</p></div>
</div>
<div class="sect3">
<h4 id="_deploy_container_native_storage">8.2.4. Deploy Container-native Storage</h4>
<div class="paragraph"><p>You are now ready to deploy CNS. Alongside GlusterFS pods the API front-end known as <strong>heketi</strong> is deployed. This protects the API from unauthorized access we will define passwords for the <code>admin</code> and <code>user</code> role in heketi like below.</p></div>
<div class="tableblock">
<table rules="all"
width="60%"
frame="border"
cellspacing="0" cellpadding="4">
<caption class="title">Table 1. CNS passwords</caption>
<col width="50%" />
<col width="50%" />
<thead>
<tr>
<th align="left" valign="top"> Heketi Role     </th>
<th align="left" valign="top"> Password</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p class="table">admin</p></td>
<td align="left" valign="top"><p class="table">myS3cr3tpassw0rd</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">user</p></td>
<td align="left" valign="top"><p class="table">mys3rs3cr3tpassw0rd</p></td>
</tr>
</tbody>
</table>
</div>
<div class="paragraph"><p>Next start the deployment routine with the following command:</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# cns-deploy -n container-native-storage -g topology.json --admin-key 'myS3cr3tpassw0rd' --user-key 'mys3rs3cr3tpassw0rd'</code></pre>
</div></div>
<div class="paragraph"><p>Answer the interactive prompt with <strong>Y</strong>.</p></div>
<div class="paragraph"><p>The deployment will take several minutes to complete (especially waiting for the GlusterFS pods will take 2-3 minutes).<br />
You may want to monitor the progress in parallel also in the OpenShift UI in the <code>container-native-storage</code> project.<br />
On the command line the output should look like this:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>Welcome to the deployment tool for GlusterFS on Kubernetes and OpenShift.

Before getting started, this script has some requirements of the execution
environment and of the container platform that you should verify.

The client machine that will run this script must have:
 * Administrative access to an existing Kubernetes or OpenShift cluster
 * Access to a python interpreter 'python'
 * Access to the heketi client 'heketi-cli'

Each of the nodes that will host GlusterFS must also have appropriate firewall
rules for the required GlusterFS ports:
 * 2222  - sshd (if running GlusterFS in a pod)
 * 24007 - GlusterFS Daemon
 * 24008 - GlusterFS Management
 * 49152 to 49251 - Each brick for every volume on the host requires its own
   port. For every new brick, one new port will be used starting at 49152. We
   recommend a default range of 49152-49251 on each host, though you can adjust
   this to fit your needs.

In addition, for an OpenShift deployment you must:
 * Have 'cluster_admin' role on the administrative account doing the deployment
 * Add the 'default' and 'router' Service Accounts to the 'privileged' SCC
 * Have a router deployed that is configured to allow apps to access services
   running in the cluster

Do you wish to proceed with deployment?

[Y]es, [N]o? [Default: Y]: <img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" />
Using OpenShift CLI.
NAME                       STATUS    AGE
container-native-storage   Active    28m
Using namespace "container-native-storage".
Checking that heketi pod is not running ... OK
template "deploy-heketi" created
serviceaccount "heketi-service-account" created
template "heketi" created
template "glusterfs" created
role "edit" added: "system:serviceaccount:container-native-storage:heketi-service-account"
node "node1.example.com" labeled <img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" />
node "node2.example.com" labeled <img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" />
node "node3.example.com" labeled <img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" />
daemonset "glusterfs" created
Waiting for GlusterFS pods to start ... OK <img src="/etc/asciidoc/images/icons/callouts/3.png" alt="3" />
service "deploy-heketi" created
route "deploy-heketi" created
deploymentconfig "deploy-heketi" created
Waiting for deploy-heketi pod to start ... OK
Creating cluster ... ID: 307f708621f4e0c9eda962b713272e81
Creating node node1.example.com ... ID: f60a225a16e8678d5ef69afb4815e417 <img src="/etc/asciidoc/images/icons/callouts/4.png" alt="4" />
Adding device /dev/vdc ... OK <img src="/etc/asciidoc/images/icons/callouts/5.png" alt="5" />
Creating node node2.example.com ... ID: 13b7c17c541069862d7e66d142ab789e <img src="/etc/asciidoc/images/icons/callouts/4.png" alt="4" />
Adding device /dev/vdc ... OK <img src="/etc/asciidoc/images/icons/callouts/5.png" alt="5" />
Creating node node3.example.com ... ID: 5a6fbe5eb1864e711f8bd9b0cb5946ea <img src="/etc/asciidoc/images/icons/callouts/4.png" alt="4" />
Adding device /dev/vdc ... OK <img src="/etc/asciidoc/images/icons/callouts/5.png" alt="5" />
heketi topology loaded.
Saving heketi-storage.json
secret "heketi-storage-secret" created
endpoints "heketi-storage-endpoints" created
service "heketi-storage-endpoints" created
job "heketi-storage-copy-job" created
deploymentconfig "deploy-heketi" deleted
route "deploy-heketi" deleted
service "deploy-heketi" deleted
job "heketi-storage-copy-job" deleted
pod "deploy-heketi-1-599rc" deleted
secret "heketi-storage-secret" deleted
service "heketi" created
route "heketi" created
deploymentconfig "heketi" created <img src="/etc/asciidoc/images/icons/callouts/6.png" alt="6" />
Waiting for heketi pod to start ... OK
heketi is now running.
Ready to create and provide GlusterFS volumes.</code></pre>
</div></div>
<div class="colist arabic"><table>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" /></td><td>
Enter <strong>Y</strong> and press Enter.
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" /></td><td>
OpenShift nodes are labeled. Label is referred to in a DaemonSet.
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/3.png" alt="3" /></td><td>
GlusterFS daemonset is started. DaemonSet means: start exactly <strong>one</strong> pod per node.
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/4.png" alt="4" /></td><td>
All nodes will be referenced in heketi&#8217;s database by a UUID.
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/5.png" alt="5" /></td><td>
Node block devices are formatted for mounting by GlusterFS.
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/6.png" alt="6" /></td><td>
heketi is deployed in a pod as well.
</td></tr>
</table></div>
</div>
<div class="sect3">
<h4 id="_verifying_the_deployment">8.2.5. Verifying the deployment</h4>
<div class="paragraph"><p>You now have deployed CNS. Let&#8217;s verify all components are in place. While still in the <code>container-native-storage</code> project on the CLI list all running pods.</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get pods -o wide
NAME              READY     STATUS    RESTARTS   AGE       IP              NODE
glusterfs-37vn8   1/1       Running   0          3m       192.168.0.102   node1.example.com <img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" />
glusterfs-cq68l   1/1       Running   0          3m       192.168.0.103   node2.example.com <img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" />
glusterfs-m9fvl   1/1       Running   0          3m       192.168.0.104   node3.example.com <img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" />
heketi-1-cd032    1/1       Running   0          1m       10.130.0.4      node3.example.com <img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" /></code></pre>
</div></div>
<div class="colist arabic"><table>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" /></td><td>
GlusterFS pods, notice how all designated nodes run exactly one pod.
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" /></td><td>
heketi API frontend pod
</td></tr>
</table></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">The exact pod names will be different in your environment, since they are auto-generated.</td>
</tr></table>
</div>
<div class="paragraph"><p>The GlusterFS pods use the hosts network and disk devices to run the software-defined storage system. Hence they attached to the host&#8217;s network. See schematic below for a visualization.</p></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/cns_diagram_pod.png" alt="http://people.redhat.com/~llange/labimg/cns_diagram_pod.png" />
</div>
<div class="title">Figure 5. GlusterFS pods in CNS in detail.</div>
</div>
<div class="paragraph"><p>heketi is a component that will expose an API for GlusterFS to OpenShift. This allows OpenShift to dynamically allocate storage from CNS in a programmatic fashion. See below for a visualization. Note that for simplicity, in our example heketi runs on the OpenShift App nodes, not on the Infra node.</p></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/cns_diagram_heketi.png" alt="http://people.redhat.com/~llange/labimg/cns_diagram_heketi.png" />
</div>
<div class="title">Figure 6. heketi pod running in CNS</div>
</div>
<div class="paragraph"><p>To expose heketi&#8217;s API a <code>service</code> named <em>heketi</em> has been generated in OpenShift.</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get service/heketi
NAME      CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
heketi    172.30.5.231   &lt;none&gt;        8080/TCP   31m</code></pre>
</div></div>
<div class="paragraph"><p>To also use heketi outside of OpenShift in addition to the service a route has been deployed:</p></div>
<div class="listingblock">
<div class="content"></div></div>
<div class="paragraph"><p>Hence, heketi will be available via:</p></div>
<div class="dlist"><dl>
<dt class="hdlist1">
Heketi Service URL
</dt>
<dd>
<p>
<a href="http://heketi-container-native-storage.cloudapps.example.com">http://heketi-container-native-storage.cloudapps.example.com</a>
</p>
</dd>
</dl></div>
<div class="paragraph"><p>You may verify this with a trivial health check:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# curl http://heketi-container-native-storage.cloudapps.example.com/hello
Hello from Heketi</code></pre>
</div></div>
</div>
</div>
<div class="sect2">
<h3 id="_using_container_native_storage_in_openshift">8.3. Using Container-native Storage in OpenShift</h3>
<div class="sect3">
<h4 id="_creating_a_storageclass">8.3.1. Creating a StorageClass</h4>
<div class="paragraph"><p>OpenShift uses Kubernetes' PersistentStorage facility to dynamically allocate storage for applications. This is a fairly simple framework in which only 3 components exists: the storage provider, the storage volume and the request for a storage volume.</p></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/cns_diagram_pvc.png" alt="http://people.redhat.com/~llange/labimg/cns_diagram_pvc.png" />
</div>
<div class="title">Figure 7. OpenShift Storage Lifecycle</div>
</div>
<div class="paragraph"><p>OpenShift knows non-ephemeral storage as "persistent" volumes. This is storage that is decoupled from pod lifecycles.
Users can request such storage by submitting a <strong>PersistentVolumeClaim</strong> to the system, which carries aspects like desired capacity or access mode (shared, single, read-only).</p></div>
<div class="paragraph"><p>A storage provider in the system is represented by a <strong>StorageClass</strong> and is referenced in the claim. Upon receiving the claim it talks to the API of the actual storage system to provision the storage.</p></div>
<div class="paragraph"><p>The storage is represented in OpenShift as a <strong>PersistentVolume</strong> which can directly be used by pods to mount it.</p></div>
<div class="paragraph"><p>With these basics defined we can configure our system for CNS. First we will set up the credentials for CNS in OpenShift.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Create an encoded value for the CNS admin user like below:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# echo -n "myS3cr3tpassw0rd" | base64
bXlTM2NyM3RwYXNzdzByZA==</code></pre>
</div></div>
<div class="paragraph"><p>We will store this encoded value in an OpenShift secret.</p></div>
</li>
<li>
<p>
Create a file called <code>cns-secret.yml</code> as per below:
</p>
<div class="listingblock">
<div class="title">cns-secret.yml</div>
<div class="content"></div></div>
<div class="colist arabic"><table>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" /></td><td>
<em>key</em> contains base64-encoded version of <em>myS3cr3tpassw0rd</em>
</td></tr>
</table></div>
</li>
<li>
<p>
Create the secret in OpenShift with the following command:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc create -f cns-secret.yml</code></pre>
</div></div>
<div class="paragraph"><p>To represent CNS as a storage provider in the system you first have to create a StorageClass.</p></div>
</li>
<li>
<p>
Define the Storage Class by creating a file called <code>cns-storageclass.yml</code> which references the secret and the heketi URL shown earlier with the contents as below:
</p>
<div class="listingblock">
<div class="title">cns-storageclass.yml</div>
<div class="content"></div></div>
</li>
<li>
<p>
Create the StorageClass in OpenShift with the following command:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc create -f cns-storageclass.yml</code></pre>
</div></div>
<div class="paragraph"><p>With these components in place the system is ready to dynamically provision storage capacity from Container-native Storage.</p></div>
</li>
</ol></div>
</div>
<div class="sect3">
<h4 id="_requesting_storage">8.3.2. Requesting Storage</h4>
<div class="paragraph"><p>To get storage provisioned as a user you have to "claim" storage. The <em>PersistentVolumeClaim</em> (PVC) basically acts a request to the system to provision storage with certain properties, like a specific capacity.<br />
Also the access mode is set here, where <em>ReadWriteOnce</em> allows one container at a time to mount this storage.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Create a claim by specifying a file called <code>cns-pvc.yml</code> with the following contents:
</p>
<div class="listingblock">
<div class="title">cns-pvc.yml</div>
<div class="content"></div></div>
<div class="paragraph"><p>With above PVC we are requesting 10 GiB of non-shared storage. Instead of <em>ReadWriteOnce</em> you could also have specified <em>ReadWriteOnly</em> (for read-only) and <em>ReadWriteMany</em> (for shared storage).</p></div>
</li>
<li>
<p>
Submit the PVC to the system like so:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc create -f cns-pvc.yml
persistentvolumeclaim "my-container-storage" created</code></pre>
</div></div>
</li>
<li>
<p>
Look at the requests state with the following command:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get pvc
NAME                   STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
my-container-storage   Bound     pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8   10Gi       RWO           16s</code></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">It may take up to 15 seconds for the claim to be in <strong>bound</strong>.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/caution.png" alt="Caution" />
</td>
<td class="content">If the PVC is stuck in <em>PENDING</em> state you will need to investigate. Run <code>oc describe pvc/my-container-storage</code> to see a more detailed explanation. Typically there are two root causes - the StorageClass is not properly setup (wrong name, wrong credentials, incorrect secret name, wrong heketi URL, heketi service not up, heketi pod not up&#8230;) or the PVC is malformed (wrong StorageClass, name already taken &#8230;)</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/tip.png" alt="Tip" />
</td>
<td class="content">You can also do this step with the UI. If you like you can switch to an arbitrary project you have access to and go to the "Storage" tab. Select "Create" storage and make selections accordingly to the PVC described before.</td>
</tr></table>
</div>
<div class="paragraph"><p>When the claim was fulfilled successfully it is in the <strong>Bound</strong> state. That means the system has successfully (via the StorageClass) reached out to the storage backend (in our case GlusterFS). The backend in turn provisioned the storage and provided a handle back OpenShift. In OpenShift the provisioned storage is then represented by a <em>PersistentVolume</em> (PV) which is <em>bound</em> to the PVC.<br /></p></div>
</li>
<li>
<p>
Look at the PVC for these details:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc describe pvc/my-container-storage
Name:           my-container-storage
Namespace:      container-native-storage
StorageClass:   container-native-storage <img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" />
Status:         Bound
Volume:         pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8 <img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" />
Labels:         &lt;none&gt;
Capacity:       10Gi
Access Modes:   RWO
No events.</code></pre>
</div></div>
<div class="colist arabic"><table>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" /></td><td>
The StorageClass against which the PVC was submitted.
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" /></td><td>
The name of PV that has been created.
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">The PV name will be different in your environment since it&#8217;s automatically generated.</td>
</tr></table>
</div>
</td></tr>
</table></div>
</li>
<li>
<p>
Look at the corresponding PV by it&#8217;s name:
</p>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc describe pv/pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8
Name:           pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8
Labels:         &lt;none&gt;
StorageClass:   container-native-storage <img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" />
Status:         Bound
Claim:          container-native-storage/my-container-storage <img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" />
Reclaim Policy: Delete <img src="/etc/asciidoc/images/icons/callouts/3.png" alt="3" />
Access Modes:   RWO <img src="/etc/asciidoc/images/icons/callouts/4.png" alt="4" />
Capacity:       10Gi <img src="/etc/asciidoc/images/icons/callouts/5.png" alt="5" />
Message:
Source:
    Type:               Glusterfs (a Glusterfs mount on the host that shares a pod's lifetime) <img src="/etc/asciidoc/images/icons/callouts/6.png" alt="6" />
    EndpointsName:      glusterfs-dynamic-my-container-storage
    Path:               vol_304670f0d50bf5aa4717a69652bd48ff
    ReadOnly:           false
No events.</code></pre>
</div></div>
<div class="colist arabic"><table>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" /></td><td>
The StorageClass which provisioned this PV.
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" /></td><td>
The claim that initiated the provisioning.
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/3.png" alt="3" /></td><td>
What happens to the storage when the PV object is deleted: here it&#8217;s deleted as well.
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/4.png" alt="4" /></td><td>
The desired access mode. RWO = ReadWriteOnce.
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/5.png" alt="5" /></td><td>
The capacity of the provisioned storage.
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/6.png" alt="6" /></td><td>
The type of storage: in our case GlusterFS as part of CNS.
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/tip.png" alt="Tip" />
</td>
<td class="content">Note that in earlier documentation you will find references to administrators  <strong>pre-provisioning</strong> PVs. Later PVCs would "pick up" a suitable PV by looking at it&#8217;s capacity. This was needed for storage like NFS that does not have an API and therefore does not support <strong>dynamic provisioning</strong>.<br />
This kind of storage should not be used anymore as it requires manual intervention, risky capacity planning and incurs inefficient storage utilization.</td>
</tr></table>
</div>
</td></tr>
</table></div>
</li>
<li>
<p>
Release this storage capacity again, since it&#8217;s in the wrong namespace anyway.
</p>
<div class="paragraph"><p>Storage is freed up by deleting the <strong>PVC</strong>. The PVC controls the lifecycle of the storage, not the PV.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/important.png" alt="Important" />
</td>
<td class="content">Never delete PVs that are dynamically provided. They are only handles for pods mounting the storage. Storage lifecycle is entirely controlled via PVCs.</td>
</tr></table>
</div>
</li>
<li>
<p>
Delete the storage by deleting the PVC like this:
</p>
<div class="listingblock">
<div class="content">
<pre><code> [root@master ~]# oc delete pvc/my-container-storage</code></pre>
</div></div>
</li>
</ol></div>
</div>
<div class="sect3">
<h4 id="_using_non_shared_storage_for_databases">8.3.3. Using non-shared storage for databases</h4>
<div class="paragraph"><p>Normally a user doesn&#8217;t request storage with a PVC directly. Rather the PVC is integrated in a larger template that describe the entire application. Such examples ship with OpenShift out of the box.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/tip.png" alt="Tip" />
</td>
<td class="content">The following steps can again also be done with the UI. For this purpose follow these steps:</td>
</tr></table>
</div>
<hr />
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Log on to a project you have access to and quota available
</p>
</li>
<li>
<p>
next to the project&#8217;s name select <em>Add to project</em>
</p>
</li>
<li>
<p>
In the <em>Browse Catalog</em> view select <em>Ruby</em> from the list of programming languages
</p>
</li>
<li>
<p>
Select the example app entitled <em>Rails + PostgreSQL (Persistent)</em>
</p>
</li>
<li>
<p>
Optionally change the <em>Volume Capacity</em> parameter to something greater than 1GiB, e.g. 15 GiB
</p>
</li>
<li>
<p>
Select <em>Create</em> to start deploying the app
</p>
</li>
<li>
<p>
Select <em>Continue to Overview</em> in the confirmation screen
</p>
</li>
<li>
<p>
Back on the overview page select the deploymentconfig <em>postgresql</em>
</p>
</li>
<li>
<p>
On the following page select <em>Actions</em> &gt; <em>Edit Health Checks</em>
</p>
</li>
<li>
<p>
In the settings menu change the <em>Initial Delay</em> values for both <em>Readiness Probe</em> and <em>Liveliness Probe</em> to 180 seconds
</p>
</li>
</ol></div>
<hr />
<div class="paragraph"><p>Log on to the system as <code>marina</code> und create a project with an arbitrary name.</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# oc login -u marina --insecure-skip-tls-verify --server=https://master.example.com:8443
[root@master ~]# oc new-project my-test-project</code></pre>
</div></div>
<div class="paragraph"><p>To use some of the examples that ship with OpenShift enter the following command to export the template for a sample Ruby on Rails with PostgreSQL application:</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# oc export template/rails-pgsql-persistent -n openshift -o yaml &gt; rails-app-template.yml</code></pre>
</div></div>
<div class="paragraph"><p>In the file <code>rails-app-template.yml</code> you can now review the template for this entire application stack in all it&#8217;s glory. In essence it creates Rails Application instance which mimics a very basic blogging application. The articles are saved in a PostgreSQL database which runs in another pod. In addition a PVC is issued (line 194) to supply this pod with persistent storage below the mount point /var/lib/pgsql/data (line 275).</p></div>
<div class="paragraph"><p>We need to modify this template now. Open it in your favorite editor and increase the values for <code>initialDelaySeconds</code> in both sections (<code>livenessProbe</code> and <code>readinessProbe</code>), around lines 255 - 270:</p></div>
<div class="listingblock">
<div class="title">rails-app-template.yml</div>
<div class="content"></div></div>
<div class="colist arabic"><table>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" /></td><td>
Set the <em>initialDelaySeconds</em> value to 180 in both the livenessProbe and readinessProbe section
</td></tr>
</table></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/important.png" alt="Important" />
</td>
<td class="content">In production you don&#8217;t have to change these values. Your test environment however is using nested virtualization and therefore has much lower performance than a production environment in the cloud or on-premise. Therefore the postgres container takes longer to initialize and would be declared unhealthy by OpenShift with the default delays when checking the container health.</td>
</tr></table>
</div>
<div class="paragraph"><p>Next we are going to create all the resources from the templates while passing in an additional parameter to override the default storage capacity requested from the PVC.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/tip.png" alt="Tip" />
</td>
<td class="content">To list all available parameters from this template run <code>oc process -f rails-app-template.yml --parameters</code></td>
</tr></table>
</div>
<div class="paragraph"><p>The parameter in the template is called <code>VOLUME_CAPACITY</code>. We will process the template with the CLI client and override this parameter with a value of <em>15Gi</em> as follows:</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# oc process -f rails-app-template.yml -o yaml -p VOLUME_CAPACITY=15Gi &gt; my-rails-app.yml</code></pre>
</div></div>
<div class="paragraph"><p>The <code>oc process</code> command parses the template and replaces any parameters with their default values if not supplied explicitly like we did for the volume capacity.</p></div>
<div class="paragraph"><p>The result <code>my-rails-app.yml</code> file contains all resources for this application ready to deploy, like so:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc create -f my-rails-app.yml
secret "rails-pgsql-persistent" created
service "rails-pgsql-persistent" created
route "rails-pgsql-persistent" created
imagestream "rails-pgsql-persistent" created
buildconfig "rails-pgsql-persistent" created
deploymentconfig "rails-pgsql-persistent" created
persistentvolumeclaim "postgresql" created
service "postgresql" created
deploymentconfig "postgresql" created</code></pre>
</div></div>
<div class="paragraph"><p>You can now use the OpenShift UI (while being logged in as <em>marina</em> in the newly created project) to follow the deployment process. Alternatively watch the containers deploy like this:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get pods -w
NAME                             READY     STATUS              RESTARTS   AGE
postgresql-1-deploy              0/1       ContainerCreating   0          11s
rails-pgsql-persistent-1-build   0/1       ContainerCreating   0          11s
NAME                  READY     STATUS    RESTARTS   AGE
postgresql-1-deploy   1/1       Running   0          14s
postgresql-1-81gnm   0/1       Pending   0         0s
postgresql-1-81gnm   0/1       Pending   0         0s
rails-pgsql-persistent-1-build   1/1       Running   0         19s
postgresql-1-81gnm   0/1       Pending   0         15s
postgresql-1-81gnm   0/1       ContainerCreating   0         16s
postgresql-1-81gnm   0/1       Running   0         47s
postgresql-1-81gnm   1/1       Running   0         4m
postgresql-1-deploy   0/1       Completed   0         4m
postgresql-1-deploy   0/1       Terminating   0         4m
postgresql-1-deploy   0/1       Terminating   0         4m
rails-pgsql-persistent-1-deploy   0/1       Pending   0         0s
rails-pgsql-persistent-1-deploy   0/1       Pending   0         0s
rails-pgsql-persistent-1-deploy   0/1       ContainerCreating   0         0s
rails-pgsql-persistent-1-build   0/1       Completed   0         11m
rails-pgsql-persistent-1-deploy   1/1       Running   0         6s
rails-pgsql-persistent-1-hook-pre   0/1       Pending   0         0s
rails-pgsql-persistent-1-hook-pre   0/1       Pending   0         0s
rails-pgsql-persistent-1-hook-pre   0/1       ContainerCreating   0         0s
rails-pgsql-persistent-1-hook-pre   1/1       Running   0         6s
rails-pgsql-persistent-1-hook-pre   0/1       Completed   0         15s
rails-pgsql-persistent-1-dkj7w   0/1       Pending   0         0s
rails-pgsql-persistent-1-dkj7w   0/1       Pending   0         0s
rails-pgsql-persistent-1-dkj7w   0/1       ContainerCreating   0         0s
rails-pgsql-persistent-1-dkj7w   0/1       Running   0         1m
rails-pgsql-persistent-1-dkj7w   1/1       Running   0         1m
rails-pgsql-persistent-1-deploy   0/1       Completed   0         1m
rails-pgsql-persistent-1-deploy   0/1       Terminating   0         1m
rails-pgsql-persistent-1-deploy   0/1       Terminating   0         1m
rails-pgsql-persistent-1-hook-pre   0/1       Terminating   0         1m
rails-pgsql-persistent-1-hook-pre   0/1       Terminating   0         1m</code></pre>
</div></div>
<div class="paragraph"><p>Exit out of the watch mode with kbd:[Ctrl + c]</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">It may take up to 10 minutes for the deployment to complete.</td>
</tr></table>
</div>
<div class="paragraph"><p>You should also see a PVC being issued and in the <em>Bound</em> state.</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get pvc
NAME         STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
postgresql   Bound     pvc-9bb84d88-4ac6-11e7-b56f-2cc2602a6dc8   15Gi       RWO           4m</code></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/tip.png" alt="Tip" />
</td>
<td class="content">Why did this even work? If you paid close attention you likely noticed that the PVC in the template does not specify a particular <em>StorageClass</em>. This still yields a PV deployed because our <em>StorageClass</em> has been defined as the system-wide default.</td>
</tr></table>
</div>
<div class="paragraph"><p>Now go ahead and try out the application. The overview page in the OpenShift UI will tell you the <code>route</code> which has been deployed as well. Otherwise get it on the CLI like this:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get route
NAME                     HOST/PORT                                                      PATH      SERVICES                 PORT      TERMINATION   WILDCARD
rails-pgsql-persistent   rails-pgsql-persistent-my-test-project.cloudapps.example.com             rails-pgsql-persistent   &lt;all&gt;                   None</code></pre>
</div></div>
<div class="paragraph"><p>Following this output, point your browser to <a href="http://rails-pgsql-persistent-my-test-project.cloudapps.example.com/articles">http://rails-pgsql-persistent-my-test-project.cloudapps.example.com/articles</a>.<br />
The username/password to create articles and comments is by default <em>openshift</em>/<em>secret</em>.</p></div>
<div class="paragraph"><p>You should be able to successfully create articles and comments. They are saved they in the PostgreSQL database which stores it&#8217;s table spaces on a GlusterFS volume provided by CNS.</p></div>
<div class="paragraph"><p>Now let&#8217;s take a look at how this was actually achieved. First you need to acquire necessary permissions:</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# oc login -u system:admin</code></pre>
</div></div>
<div class="paragraph"><p>Select the example project of the user <code>marina</code> if not already/still selected:</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# oc project my-test-project</code></pre>
</div></div>
<div class="paragraph"><p>Look at the PVC to determine the PV:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get pvc
NAME         STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
postgresql   Bound     pvc-9bb84d88-4ac6-11e7-b56f-2cc2602a6dc8   15Gi       RWO           17m</code></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">Your PV name will be different as it&#8217;s dynamically generated.</td>
</tr></table>
</div>
<div class="paragraph"><p>Look at the details of this PV:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc describe pv/pvc-9bb84d88-4ac6-11e7-b56f-2cc2602a6dc8
Name:           pvc-9bb84d88-4ac6-11e7-b56f-2cc2602a6dc8 <img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" />
Labels:         &lt;none&gt;
StorageClass:   container-native-storage
Status:         Bound
Claim:          my-test-project/postgresql
Reclaim Policy: Delete
Access Modes:   RWO
Capacity:       15Gi
Message:
Source:
    Type:               Glusterfs (a Glusterfs mount on the host that shares a pod's lifetime)
    EndpointsName:      glusterfs-dynamic-postgresql
    Path:               vol_e8fe7f46fedf7af7628feda0dcbf2f60 <img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" />
    ReadOnly:           false
No events.</code></pre>
</div></div>
<div class="colist arabic"><table>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" /></td><td>
The unique name of this PV in the system OpenShift refers to
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" /></td><td>
The unique volume name backing the PV known to GlusterFS
</td></tr>
</table></div>
<div class="paragraph"><p>Note the GlusterFS volume name, in this case <strong>vol_e8fe7f46fedf7af7628feda0dcbf2f60</strong>.</p></div>
<div class="paragraph"><p>Now let&#8217;s switch to the namespace we used for CNS deployment:</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# oc project container-native-storage</code></pre>
</div></div>
<div class="paragraph"><p>Look at the GlusterFS pods running and pick one (which one is not important):</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get pods -o wide
NAME              READY     STATUS    RESTARTS   AGE       IP              NODE
glusterfs-37vn8   1/1       Running   1          15m       192.168.0.102   node1.example.com
glusterfs-cq68l   1/1       Running   1          15m       192.168.0.103   node2.example.com
glusterfs-m9fvl   1/1       Running   1          15m       192.168.0.104   node3.example.com
heketi-1-cd032    1/1       Running   1          13m       10.130.0.5      node3.example.com</code></pre>
</div></div>
<div class="paragraph"><p>Remember the IP address of the pod you select. Log on to GlusterFS pod with a remote terminal session like so:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc rsh glusterfs-37vn8
sh-4.2#</code></pre>
</div></div>
<div class="paragraph"><p>You have now access to this container&#8217;s namespace which has the GlusterFS CLI utilities installed.<br />
Let&#8217;s list all known volumes:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2# gluster volume list
heketidbstorage <img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" />
vol_e8fe7f46fedf7af7628feda0dcbf2f60 <img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" /></code></pre>
</div></div>
<div class="colist arabic"><table>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" /></td><td>
A special volume dedicated to heketi&#8217;s internal database.
</td></tr>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/2.png" alt="2" /></td><td>
The volume backing the PV of the PostgreSQL database deployed earlier.
</td></tr>
</table></div>
<div class="paragraph"><p>Interrogate GlusterFS about the topology of this volume:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2# gluster volume info vol_e8fe7f46fedf7af7628feda0dcbf2f60

Volume Name: vol_e8fe7f46fedf7af7628feda0dcbf2f60
Type: Replicate
Volume ID: c2bedd16-8b0d-432c-b9eb-4ab1274826dd
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 3 = 3
Transport-type: tcp
Bricks:
Brick1: 192.168.0.103:/var/lib/heketi/mounts/vg_63b05bee6695ee5a63ad95bfbce43bf7/brick_aa28de668c8c21192df55956a822bd3c/brick
Brick2: 192.168.0.102:/var/lib/heketi/mounts/vg_0246fd563709384a3cbc3f3bbeeb87a9/brick_684a01f8993f241a92db02b117e0b912/brick <img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" />
Brick3: 192.168.0.104:/var/lib/heketi/mounts/vg_5a8c767e65feef7455b58d01c6936b83/brick_25972cf5ed7ea81c947c62443ccb308c/brick
Options Reconfigured:
transport.address-family: inet
performance.readdir-ahead: on
nfs.disable: on</code></pre>
</div></div>
<div class="colist arabic"><table>
<tr><td><img src="/etc/asciidoc/images/icons/callouts/1.png" alt="1" /></td><td>
According to the output of <code>oc get pods -o wide</code> this is the container we are logged on to.
</td></tr>
</table></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">Identify the right brick by looking at the host IP of the pod you have just logged on to. <code>oc get pods -o wide</code> will give you this information.</td>
</tr></table>
</div>
<div class="paragraph"><p>GlusterFS created this volume as a 3-way replica set across all GlusterFS pods, in therefore across all your OpenShift App nodes running CNS.<br />
Each pod/node exposes his local storage via the GlusterFS protocol. This local storage is known as a <strong>brick</strong> in GlusterFS and is usually backed by a local SAS disk or NVMe device. The brick is simply formatted with XFS and thus made available to GlusterFS.</p></div>
<div class="paragraph"><p>You can even look at this yourself:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2# ls -ahl /var/lib/heketi/mounts/vg_0246fd563709384a3cbc3f3bbeeb87a9/brick_684a01f8993f241a92db02b117e0b912/brick
total 16K
drwxrwsr-x.   5 root       2001   57 Jun  6 14:44 .
drwxr-xr-x.   3 root       root   19 Jun  6 14:44 ..
drw---S---. 263 root       2001 8.0K Jun  6 14:46 .glusterfs
drwxr-sr-x.   3 root       2001   25 Jun  6 14:44 .trashcan
drwx------.  20 1000080000 2001 8.0K Jun  6 14:46 userdata

sh-4.2# ls -ahl /var/lib/heketi/mounts/vg_0246fd563709384a3cbc3f3bbeeb87a9/brick_684a01f8993f241a92db02b117e0b912/brick/userdata

total 68K
drwx------. 20 1000080000 2001 8.0K Jun  6 14:46 .
drwxrwsr-x.  5 root       2001   57 Jun  6 14:44 ..
-rw-------.  2 1000080000 root    4 Jun  6 14:44 PG_VERSION
drwx------.  6 1000080000 root   54 Jun  6 14:46 base
drwx------.  2 1000080000 root 8.0K Jun  6 14:47 global
drwx------.  2 1000080000 root   18 Jun  6 14:44 pg_clog
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_commit_ts
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_dynshmem
-rw-------.  2 1000080000 root 4.6K Jun  6 14:46 pg_hba.conf
-rw-------.  2 1000080000 root 1.6K Jun  6 14:44 pg_ident.conf
drwx------.  2 1000080000 root   32 Jun  6 14:46 pg_log
drwx------.  4 1000080000 root   39 Jun  6 14:44 pg_logical
drwx------.  4 1000080000 root   36 Jun  6 14:44 pg_multixact
drwx------.  2 1000080000 root   18 Jun  6 14:46 pg_notify
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_replslot
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_serial
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_snapshots
drwx------.  2 1000080000 root    6 Jun  6 14:46 pg_stat
drwx------.  2 1000080000 root   84 Jun  6 15:16 pg_stat_tmp
drwx------.  2 1000080000 root   18 Jun  6 14:44 pg_subtrans
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_tblspc
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_twophase
drwx------.  3 1000080000 root   60 Jun  6 14:44 pg_xlog
-rw-------.  2 1000080000 root   88 Jun  6 14:44 postgresql.auto.conf
-rw-------.  2 1000080000 root  21K Jun  6 14:46 postgresql.conf
-rw-------.  2 1000080000 root   46 Jun  6 14:46 postmaster.opts
-rw-------.  2 1000080000 root   89 Jun  6 14:46 postmaster.pid</code></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">The exact path name will be different in your environment as it has been automatically generated.</td>
</tr></table>
</div>
<div class="paragraph"><p>You are looking at the PostgreSQL internal data file structure from the perspective of the GlusterFS server side. It&#8217;s a normal local filesystem here.</p></div>
<div class="paragraph"><p>Clients, like the OpenShift nodes and their application pods talk to this storage with the GlusterFS protocol. Which abstracts the 3-way replication behind a single FUSE mount point.<br />
When a pod starts that mounts storage from a PV backed by GlusterFS OpenShift will mount the GlusterFS volume on the App Node and then <em>bind-mount</em> this directory to the right pod.<br />
This is happen transparently to the application inside the pod and looks like a normal local filesystem.</p></div>
<div class="paragraph"><p>You may exit your remote session to the GlusterFS pod.</p></div>
<div class="literalblock">
<div class="content">
<pre><code>sh-4.2# exit</code></pre>
</div></div>
</div>
<div class="sect3">
<h4 id="_providing_shared_storage_to_multiple_application_instances">8.3.4. Providing shared storage to multiple application instances</h4>
<div class="paragraph"><p>So far only very few options, like the basic NFS support existed, to provide a PersistentVolume to more than one container at once. The access mode used for this is <strong>ReadWriteMany</strong>.</p></div>
<div class="paragraph"><p>With CNS this capabilities is now available to all OpenShift deployments, no matter where they are deployed. To demonstrate this capability with an application we will deploy a PHP file uploader that has multiple front-end instances sharing a common storage repository.</p></div>
<div class="paragraph"><p>First log back in as <code>marina</code></p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# oc login -u marina --insecure-skip-tls-verify --server=https://master.example.com:8443</code></pre>
</div></div>
<div class="paragraph"><p>Next deploy the example application:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc new-app openshift/php:7.0~https://github.com/christianh814/openshift-php-upload-demo --name=file-uploader
--&gt; Found image a1ebebb (6 weeks old) in image stream "openshift/php" under tag "7.0" for "openshift/php:7.0"

    Apache 2.4 with PHP 7.0
    -----------------------
    Platform for building and running PHP 7.0 applications

    Tags: builder, php, php70, rh-php70

    * A source build using source code from https://github.com/christianh814/openshift-php-upload-demo will be created
      * The resulting image will be pushed to image stream "file-uploader:latest"
      * Use 'start-build' to trigger a new build
    * This image will be deployed in deployment config "file-uploader"
    * Port 8080/tcp will be load balanced by service "file-uploader"
      * Other containers can access this service through the hostname "file-uploader"

--&gt; Creating resources ...
    imagestream "file-uploader" created
    buildconfig "file-uploader" created
    deploymentconfig "file-uploader" created
    service "file-uploader" created
--&gt; Success
    Build scheduled, use 'oc logs -f bc/file-uploader' to track its progress.
    Run 'oc status' to view your app.</code></pre>
</div></div>
<div class="paragraph"><p>Wait for the application to be deployed with the suggest command:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc logs -f bc/file-uploader
Cloning "https://github.com/christianh814/openshift-php-upload-demo" ...
        Commit: 7508da63d78b4abc8d03eac480ae930beec5d29d (Update index.html)
        Author: Christian Hernandez &lt;christianh814@users.noreply.github.com&gt;
        Date:   Thu Mar 23 09:59:38 2017 -0700
---&gt; Installing application source...
Pushing image 172.30.120.134:5000/my-test-project/file-uploader:latest ...
Pushed 0/5 layers, 2% complete
Pushed 1/5 layers, 20% complete
Pushed 2/5 layers, 40% complete
Push successful</code></pre>
</div></div>
<div class="paragraph"><p>Again kbd:[Ctrl + c] out of the tail mode.
When the build is completed ensure the pods are running:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get pods
NAME                             READY     STATUS      RESTARTS   AGE
file-uploader-1-build            0/1       Completed   0          2m
file-uploader-1-k2v0d            1/1       Running     0          1m
...</code></pre>
</div></div>
<div class="paragraph"><p>Note the name of the single pod currently running the app: <strong>file-uploader-1-k2v0d</strong>. The container called <code>file-uploader-1-build</code> is the builder container and is not relevant for us. A service has been created for our app but not exposed yet. Let&#8217;s fix this:</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# oc expose svc/file-uploader</code></pre>
</div></div>
<div class="paragraph"><p>Check the route that has been created:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get route
NAME                     HOST/PORT                                                      PATH      SERVICES                 PORT       TERMINATION   WILDCARD
file-uploader            file-uploader-my-test-project.cloudapps.example.com                      file-uploader            8080-tcp                 None
...</code></pre>
</div></div>
<div class="paragraph"><p>Point your browser the the URL advertised by the route (<a href="http://file-uploader-my-test-project.cloudapps.example.com">http://file-uploader-my-test-project.cloudapps.example.com</a>)</p></div>
<div class="paragraph"><p>The application simply lists all file previously uploaded and offers the ability to upload new ones as well as download the existing data. Right now there is nothing.</p></div>
<div class="paragraph"><p>Select an arbitrary from your local system and upload it to the app.</p></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/uploader_screen_upload.png" alt="http://people.redhat.com/~llange/labimg/uploader_screen_upload.png" />
</div>
<div class="title">Figure 8. A simple PHP-based file upload tool</div>
</div>
<div class="paragraph"><p>After uploading a file validate it has been stored locally in the container by following the link <em>List uploaded files</em> in the browser or logging into it via a remote session (using the name noted earlier):</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# oc rsh file-uploader-1-k2v0d</code></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><code>sh-4.2$ cd uploaded
sh-4.2$ pwd
/opt/app-root/src/uploaded
sh-4.2$ ls -lh
total 16K
-rw-r--r--. 1 1000080000 root 16K May 26 09:32 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz</code></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">The exact name of the pod will be different in your environment.</td>
</tr></table>
</div>
<div class="paragraph"><p>The app should also list the file in the overview:</p></div>
<div class="imageblock">
<div class="content">
<img src="http://people.redhat.com/~llange/labimg/uploader_screen_list.png" alt="uploader" />
</div>
<div class="title">Figure 9. The file has been uploaded and can be downloaded again</div>
</div>
<div class="paragraph"><p>This pod currently does not use any persistent storage. It stores the file locally.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/caution.png" alt="Caution" />
</td>
<td class="content">Never store data in a pod. It&#8217;s ephemeral by definition and will be lost as soon as the pod terminates.</td>
</tr></table>
</div>
<div class="paragraph"><p>Let&#8217;s see when this become a problem. Exit out of the container shell:</p></div>
<div class="literalblock">
<div class="content">
<pre><code>sh-4.2$ exit</code></pre>
</div></div>
<div class="paragraph"><p>Let&#8217;s scale the deployment to 3 instances of the app:</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# oc scale dc/file-uploader --replicas=3</code></pre>
</div></div>
<div class="paragraph"><p>Watch the additional pods getting spawned:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get pods
NAME                             READY     STATUS      RESTARTS   AGE
file-uploader-1-3cgh1            1/1       Running     0          20s
file-uploader-1-3hckj            1/1       Running     0          20s
file-uploader-1-build            0/1       Completed   0          4m
file-uploader-1-k2v0d            1/1       Running     0          3m
...</code></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="/etc/asciidoc/images/icons/note.png" alt="Note" />
</td>
<td class="content">The pod names will be different in your environment since they are automatically generated.</td>
</tr></table>
</div>
<div class="paragraph"><p>When you log on to one of the new instances you will see they have no data.</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc rsh file-uploader-1-3cgh1
sh-4.2$ cd uploaded
sh-4.2$ pwd
/opt/app-root/src/uploaded
sh-4.2$ ls -hl
total 0</code></pre>
</div></div>
<div class="paragraph"><p>Similarly, other users of the app will sometimes see your uploaded files and sometimes not - whenever the load balancing service in OpenShift points to the pod that has the file stored locally. You can simulate this with another instance of your browser in "Incognito mode" pointing to your app.</p></div>
<div class="paragraph"><p>The app is of course not usable like this. We can fix this by providing shared storage to this app.</p></div>
<div class="paragraph"><p>First create a PVC with the appropriate setting in a file called <code>cns-rwx-pvc.yml</code> with below contents:</p></div>
<div class="listingblock">
<div class="title">cns-rwx-pvc.yml</div>
<div class="content"></div></div>
<div class="paragraph"><p>Submit the request to the system:</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# oc create -f cns-rwx-pvc.yml</code></pre>
</div></div>
<div class="paragraph"><p>Let&#8217;s look at the result:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get pvc
NAME                STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
my-shared-storage   Bound     pvc-62aa4dfe-4ad2-11e7-b56f-2cc2602a6dc8   10Gi       RWX           22s
...</code></pre>
</div></div>
<div class="paragraph"><p>Notice the ACCESSMODE being set to <strong>RWX</strong> (short for <em>ReadWriteMany</em>, synonym for "shared storage").</p></div>
<div class="paragraph"><p>We can now update the <em>DeploymentConfig</em> of our application to use this PVC to provide the application with persistent, shared storage for uploads.</p></div>
<div class="literalblock">
<div class="content">
<pre><code>[root@master ~]# oc volume dc/file-uploader --add --name=shared-storage --type=persistentVolumeClaim --claim-name=my-shared-storage --mount-path=/opt/app-root/src/uploaded</code></pre>
</div></div>
<div class="paragraph"><p>Our app will now re-deploy (in a rolling fashion) with the new settings - all pods will mount the volume identified by the PVC under /opt/app-root/src/upload (the path is predictable so we can hard-code it here).</p></div>
<div class="paragraph"><p>You can watch it like this:</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc logs dc/file-uploader -f
--&gt; Scaling up file-uploader-2 from 0 to 3, scaling down file-uploader-1 from 3 to 0 (keep 3 pods available, don't exceed 4 pods)
    Scaling file-uploader-2 up to 1
    Scaling file-uploader-1 down to 2
    Scaling file-uploader-2 up to 2
    Scaling file-uploader-1 down to 1
    Scaling file-uploader-2 up to 3
    Scaling file-uploader-1 down to 0
--&gt; Success</code></pre>
</div></div>
<div class="paragraph"><p>The new config <code>file-uploader-2</code> will have 3 pods all sharing the same storage.</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc get pods
NAME                             READY     STATUS      RESTARTS   AGE
file-uploader-1-build            0/1       Completed   0          18m
file-uploader-2-jd22b            1/1       Running     0          1m
file-uploader-2-kw9lq            1/1       Running     0          2m
file-uploader-2-xbz24            1/1       Running     0          1m
...</code></pre>
</div></div>
<div class="paragraph"><p>Try it out in your application: upload new files and watch them being visible from within all application pods. In the browser the application behaves fluently as it circles through the pods between browser requests.</p></div>
<div class="listingblock">
<div class="content">
<pre><code>[root@master ~]# oc rsh file-uploader-2-jd22b
sh-4.2$ ls -lh uploaded
total 16K
-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz
sh-4.2$ exit
exit
[root@master ~]# oc rsh file-uploader-2-kw9lq
sh-4.2$ ls -lh uploaded
-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz
sh-4.2$ exit
exit
[root@master ~]# oc rsh file-uploader-2-xbz24
sh-4.2$ ls -lh uploaded
-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz
sh-4.2$ exit</code></pre>
</div></div>
<div class="paragraph"><p>That&#8217;s it. You have successfully provided shared storage to pods throughout the entire system, therefore avoiding the need for data to be replicated at the application level to each pod.</p></div>
<div class="paragraph"><p>With CNS this is available wherever OpenShift is deployed with no external dependency.</p></div>
</div>
</div>
</div>
</div>
</div>
<div id="footnotes"><hr /></div>
<div id="footer">
<div id="footer-text">
Last updated 2017-06-19 15:48:07 CEST
</div>
</div>
</body>
</html>
